{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:  /mnt/e/py_data/project_3_git\n"
     ]
    }
   ],
   "source": [
    "# 기본 작업 경로 설정\n",
    "\n",
    "import os\n",
    "notebook_path = os.path.abspath(\"../readme.md\")\n",
    "notebook_dir = os.path.dirname(notebook_path)\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# 현재 작업 디렉토리 출력\n",
    "print(\"Current working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# 데이터셋 경로와 형식 설정\n",
    "register_coco_instances(\"face_data_set\", {}, \"data/ssd_rcnn_face/annotations.json\", \"data/yolo_data/train/\")\n",
    "register_coco_instances(\"face_data_set_valid\", {}, \"data/ssd_rcnn_face/annotations_val.json\", \"data/yolo_data/val/\")\n",
    "\n",
    "# 메타데이터를 확인.\n",
    "metadata = MetadataCatalog.get(\"face_data_set\")\n",
    "dataset_dicts = DatasetCatalog.get(\"face_data_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/07 09:19:58 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[09/07 09:19:58 d2.data.datasets.coco]: \u001b[0mLoaded 5992 images in COCO format from data/ssd_rcnn_face/annotations.json\n",
      "\u001b[32m[09/07 09:19:58 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 5992 images left.\n",
      "\u001b[32m[09/07 09:19:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [RandomCrop(crop_type='relative_range', crop_size=[0.8, 0.8]), ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[09/07 09:19:58 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[09/07 09:19:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:19:58 d2.data.common]: \u001b[0mSerializing 5992 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:19:58 d2.data.common]: \u001b[0mSerialized dataset takes 1.79 MiB\n",
      "\u001b[32m[09/07 09:19:58 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=16\n",
      "\u001b[32m[09/07 09:19:58 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./models/faster_rcnn_R_50_FPN_3x/model_0023999.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/p3/lib/python3.11/site-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/07 09:20:00 d2.engine.hooks]: \u001b[0mLoading scheduler from state_dict ...\n",
      "\u001b[32m[09/07 09:20:00 d2.engine.train_loop]: \u001b[0mStarting training from iteration 24000\n",
      "\u001b[32m[09/07 09:20:49 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 24019  total_loss: 0.1038  loss_cls: 0.01578  loss_box_reg: 0.08612  loss_rpn_cls: 3.186e-05  loss_rpn_loc: 0.001119    time: 2.3142  last_time: 2.1611  data_time: 1.4605  last_data_time: 1.2784   lr: 1.7106e-11  max_mem: 7035M\n",
      "\u001b[32m[09/07 09:20:49 d2.engine.hooks]: \u001b[0mOverall training speed: 18 iterations in 0:00:41 (2.3142 s / it)\n",
      "\u001b[32m[09/07 09:20:49 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:43 (0:00:01 on hooks)\n",
      "\u001b[32m[09/07 09:20:49 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:20:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:20:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:20:49 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:20:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:20:49 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:20:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:20:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0290 s/iter. Inference: 0.0687 s/iter. Eval: 0.0004 s/iter. Total: 0.0982 s/iter. ETA=0:01:56\n",
      "\u001b[32m[09/07 09:20:56 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0600 s/iter. Inference: 0.0539 s/iter. Eval: 0.0004 s/iter. Total: 0.1145 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:21:01 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0830 s/iter. Inference: 0.0552 s/iter. Eval: 0.0004 s/iter. Total: 0.1388 s/iter. ETA=0:02:34\n",
      "\u001b[32m[09/07 09:21:06 d2.evaluation.evaluator]: \u001b[0mInference done 122/1199. Dataloading: 0.0824 s/iter. Inference: 0.0550 s/iter. Eval: 0.0004 s/iter. Total: 0.1379 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/07 09:21:11 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0790 s/iter. Inference: 0.0564 s/iter. Eval: 0.0004 s/iter. Total: 0.1359 s/iter. ETA=0:02:21\n",
      "\u001b[32m[09/07 09:21:16 d2.evaluation.evaluator]: \u001b[0mInference done 201/1199. Dataloading: 0.0773 s/iter. Inference: 0.0566 s/iter. Eval: 0.0004 s/iter. Total: 0.1344 s/iter. ETA=0:02:14\n",
      "\u001b[32m[09/07 09:21:22 d2.evaluation.evaluator]: \u001b[0mInference done 235/1199. Dataloading: 0.0783 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1364 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:21:27 d2.evaluation.evaluator]: \u001b[0mInference done 269/1199. Dataloading: 0.0791 s/iter. Inference: 0.0583 s/iter. Eval: 0.0004 s/iter. Total: 0.1379 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 09:21:32 d2.evaluation.evaluator]: \u001b[0mInference done 290/1199. Dataloading: 0.0861 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1453 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 09:21:37 d2.evaluation.evaluator]: \u001b[0mInference done 326/1199. Dataloading: 0.0850 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1447 s/iter. ETA=0:02:06\n",
      "\u001b[32m[09/07 09:21:42 d2.evaluation.evaluator]: \u001b[0mInference done 366/1199. Dataloading: 0.0838 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1431 s/iter. ETA=0:01:59\n",
      "\u001b[32m[09/07 09:21:47 d2.evaluation.evaluator]: \u001b[0mInference done 403/1199. Dataloading: 0.0835 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1426 s/iter. ETA=0:01:53\n",
      "\u001b[32m[09/07 09:21:52 d2.evaluation.evaluator]: \u001b[0mInference done 446/1199. Dataloading: 0.0816 s/iter. Inference: 0.0582 s/iter. Eval: 0.0004 s/iter. Total: 0.1403 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 09:21:57 d2.evaluation.evaluator]: \u001b[0mInference done 486/1199. Dataloading: 0.0807 s/iter. Inference: 0.0581 s/iter. Eval: 0.0004 s/iter. Total: 0.1393 s/iter. ETA=0:01:39\n",
      "\u001b[32m[09/07 09:22:02 d2.evaluation.evaluator]: \u001b[0mInference done 526/1199. Dataloading: 0.0799 s/iter. Inference: 0.0578 s/iter. Eval: 0.0004 s/iter. Total: 0.1382 s/iter. ETA=0:01:33\n",
      "\u001b[32m[09/07 09:22:07 d2.evaluation.evaluator]: \u001b[0mInference done 568/1199. Dataloading: 0.0790 s/iter. Inference: 0.0577 s/iter. Eval: 0.0004 s/iter. Total: 0.1371 s/iter. ETA=0:01:26\n",
      "\u001b[32m[09/07 09:22:12 d2.evaluation.evaluator]: \u001b[0mInference done 608/1199. Dataloading: 0.0784 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1364 s/iter. ETA=0:01:20\n",
      "\u001b[32m[09/07 09:22:18 d2.evaluation.evaluator]: \u001b[0mInference done 645/1199. Dataloading: 0.0797 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 09:22:23 d2.evaluation.evaluator]: \u001b[0mInference done 684/1199. Dataloading: 0.0792 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:01:10\n",
      "\u001b[32m[09/07 09:22:28 d2.evaluation.evaluator]: \u001b[0mInference done 725/1199. Dataloading: 0.0784 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1365 s/iter. ETA=0:01:04\n",
      "\u001b[32m[09/07 09:22:34 d2.evaluation.evaluator]: \u001b[0mInference done 752/1199. Dataloading: 0.0802 s/iter. Inference: 0.0577 s/iter. Eval: 0.0004 s/iter. Total: 0.1385 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/07 09:22:39 d2.evaluation.evaluator]: \u001b[0mInference done 790/1199. Dataloading: 0.0798 s/iter. Inference: 0.0579 s/iter. Eval: 0.0004 s/iter. Total: 0.1382 s/iter. ETA=0:00:56\n",
      "\u001b[32m[09/07 09:22:44 d2.evaluation.evaluator]: \u001b[0mInference done 833/1199. Dataloading: 0.0790 s/iter. Inference: 0.0577 s/iter. Eval: 0.0004 s/iter. Total: 0.1372 s/iter. ETA=0:00:50\n",
      "\u001b[32m[09/07 09:22:49 d2.evaluation.evaluator]: \u001b[0mInference done 877/1199. Dataloading: 0.0782 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1363 s/iter. ETA=0:00:43\n",
      "\u001b[32m[09/07 09:22:54 d2.evaluation.evaluator]: \u001b[0mInference done 913/1199. Dataloading: 0.0782 s/iter. Inference: 0.0577 s/iter. Eval: 0.0004 s/iter. Total: 0.1364 s/iter. ETA=0:00:39\n",
      "\u001b[32m[09/07 09:22:59 d2.evaluation.evaluator]: \u001b[0mInference done 959/1199. Dataloading: 0.0773 s/iter. Inference: 0.0574 s/iter. Eval: 0.0004 s/iter. Total: 0.1353 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 09:23:04 d2.evaluation.evaluator]: \u001b[0mInference done 992/1199. Dataloading: 0.0781 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1359 s/iter. ETA=0:00:28\n",
      "\u001b[32m[09/07 09:23:09 d2.evaluation.evaluator]: \u001b[0mInference done 1029/1199. Dataloading: 0.0783 s/iter. Inference: 0.0571 s/iter. Eval: 0.0004 s/iter. Total: 0.1359 s/iter. ETA=0:00:23\n",
      "\u001b[32m[09/07 09:23:14 d2.evaluation.evaluator]: \u001b[0mInference done 1067/1199. Dataloading: 0.0783 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1358 s/iter. ETA=0:00:17\n",
      "\u001b[32m[09/07 09:23:19 d2.evaluation.evaluator]: \u001b[0mInference done 1105/1199. Dataloading: 0.0782 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1357 s/iter. ETA=0:00:12\n",
      "\u001b[32m[09/07 09:23:25 d2.evaluation.evaluator]: \u001b[0mInference done 1140/1199. Dataloading: 0.0787 s/iter. Inference: 0.0569 s/iter. Eval: 0.0004 s/iter. Total: 0.1361 s/iter. ETA=0:00:08\n",
      "\u001b[32m[09/07 09:23:30 d2.evaluation.evaluator]: \u001b[0mInference done 1179/1199. Dataloading: 0.0784 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1359 s/iter. ETA=0:00:02\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:41.960195 (0.135645 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:08 (0.057010 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "\u001b[32m[09/07 09:23:32 d2.engine.defaults]: \u001b[0mEvaluation results for face_data_set_valid in csv format:\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.testing]: \u001b[0mcopypaste: 67.8147,79.8091,78.9786,nan,nan,67.8147\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:23:32 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:23:32 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:23:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:23:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:23:32 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:23:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:23:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:23:35 d2.evaluation.evaluator]: \u001b[0mInference done 18/1199. Dataloading: 0.0621 s/iter. Inference: 0.0559 s/iter. Eval: 0.0004 s/iter. Total: 0.1184 s/iter. ETA=0:02:19\n",
      "\u001b[32m[09/07 09:23:40 d2.evaluation.evaluator]: \u001b[0mInference done 62/1199. Dataloading: 0.0595 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1172 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 09:23:45 d2.evaluation.evaluator]: \u001b[0mInference done 95/1199. Dataloading: 0.0727 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1308 s/iter. ETA=0:02:24\n",
      "\u001b[32m[09/07 09:23:50 d2.evaluation.evaluator]: \u001b[0mInference done 127/1199. Dataloading: 0.0793 s/iter. Inference: 0.0578 s/iter. Eval: 0.0004 s/iter. Total: 0.1376 s/iter. ETA=0:02:27\n",
      "\u001b[32m[09/07 09:23:55 d2.evaluation.evaluator]: \u001b[0mInference done 171/1199. Dataloading: 0.0747 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1321 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 09:24:00 d2.evaluation.evaluator]: \u001b[0mInference done 212/1199. Dataloading: 0.0730 s/iter. Inference: 0.0569 s/iter. Eval: 0.0004 s/iter. Total: 0.1304 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 09:24:05 d2.evaluation.evaluator]: \u001b[0mInference done 245/1199. Dataloading: 0.0765 s/iter. Inference: 0.0565 s/iter. Eval: 0.0004 s/iter. Total: 0.1335 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:24:10 d2.evaluation.evaluator]: \u001b[0mInference done 274/1199. Dataloading: 0.0793 s/iter. Inference: 0.0583 s/iter. Eval: 0.0004 s/iter. Total: 0.1381 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:24:16 d2.evaluation.evaluator]: \u001b[0mInference done 298/1199. Dataloading: 0.0855 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1447 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 09:24:21 d2.evaluation.evaluator]: \u001b[0mInference done 340/1199. Dataloading: 0.0827 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1417 s/iter. ETA=0:02:01\n",
      "\u001b[32m[09/07 09:24:26 d2.evaluation.evaluator]: \u001b[0mInference done 376/1199. Dataloading: 0.0827 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1418 s/iter. ETA=0:01:56\n",
      "\u001b[32m[09/07 09:24:31 d2.evaluation.evaluator]: \u001b[0mInference done 414/1199. Dataloading: 0.0820 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1409 s/iter. ETA=0:01:50\n",
      "\u001b[32m[09/07 09:24:36 d2.evaluation.evaluator]: \u001b[0mInference done 458/1199. Dataloading: 0.0794 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1384 s/iter. ETA=0:01:42\n",
      "\u001b[32m[09/07 09:24:41 d2.evaluation.evaluator]: \u001b[0mInference done 498/1199. Dataloading: 0.0788 s/iter. Inference: 0.0581 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:01:36\n",
      "\u001b[32m[09/07 09:24:46 d2.evaluation.evaluator]: \u001b[0mInference done 537/1199. Dataloading: 0.0784 s/iter. Inference: 0.0578 s/iter. Eval: 0.0004 s/iter. Total: 0.1367 s/iter. ETA=0:01:30\n",
      "\u001b[32m[09/07 09:24:51 d2.evaluation.evaluator]: \u001b[0mInference done 578/1199. Dataloading: 0.0775 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1357 s/iter. ETA=0:01:24\n",
      "\u001b[32m[09/07 09:24:56 d2.evaluation.evaluator]: \u001b[0mInference done 618/1199. Dataloading: 0.0770 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1350 s/iter. ETA=0:01:18\n",
      "\u001b[32m[09/07 09:25:01 d2.evaluation.evaluator]: \u001b[0mInference done 647/1199. Dataloading: 0.0785 s/iter. Inference: 0.0577 s/iter. Eval: 0.0004 s/iter. Total: 0.1367 s/iter. ETA=0:01:15\n",
      "\u001b[32m[09/07 09:25:06 d2.evaluation.evaluator]: \u001b[0mInference done 688/1199. Dataloading: 0.0778 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1358 s/iter. ETA=0:01:09\n",
      "\u001b[32m[09/07 09:25:11 d2.evaluation.evaluator]: \u001b[0mInference done 728/1199. Dataloading: 0.0772 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1354 s/iter. ETA=0:01:03\n",
      "\u001b[32m[09/07 09:25:16 d2.evaluation.evaluator]: \u001b[0mInference done 757/1199. Dataloading: 0.0788 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1369 s/iter. ETA=0:01:00\n",
      "\u001b[32m[09/07 09:25:21 d2.evaluation.evaluator]: \u001b[0mInference done 795/1199. Dataloading: 0.0786 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1366 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/07 09:25:26 d2.evaluation.evaluator]: \u001b[0mInference done 839/1199. Dataloading: 0.0777 s/iter. Inference: 0.0573 s/iter. Eval: 0.0004 s/iter. Total: 0.1355 s/iter. ETA=0:00:48\n",
      "\u001b[32m[09/07 09:25:31 d2.evaluation.evaluator]: \u001b[0mInference done 879/1199. Dataloading: 0.0773 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1351 s/iter. ETA=0:00:43\n",
      "\u001b[32m[09/07 09:25:36 d2.evaluation.evaluator]: \u001b[0mInference done 915/1199. Dataloading: 0.0775 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1353 s/iter. ETA=0:00:38\n",
      "\u001b[32m[09/07 09:25:41 d2.evaluation.evaluator]: \u001b[0mInference done 959/1199. Dataloading: 0.0767 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1344 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 09:25:47 d2.evaluation.evaluator]: \u001b[0mInference done 991/1199. Dataloading: 0.0775 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1352 s/iter. ETA=0:00:28\n",
      "\u001b[32m[09/07 09:25:52 d2.evaluation.evaluator]: \u001b[0mInference done 1027/1199. Dataloading: 0.0777 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1354 s/iter. ETA=0:00:23\n",
      "\u001b[32m[09/07 09:25:57 d2.evaluation.evaluator]: \u001b[0mInference done 1063/1199. Dataloading: 0.0780 s/iter. Inference: 0.0571 s/iter. Eval: 0.0004 s/iter. Total: 0.1356 s/iter. ETA=0:00:18\n",
      "\u001b[32m[09/07 09:26:02 d2.evaluation.evaluator]: \u001b[0mInference done 1099/1199. Dataloading: 0.0781 s/iter. Inference: 0.0571 s/iter. Eval: 0.0004 s/iter. Total: 0.1358 s/iter. ETA=0:00:13\n",
      "\u001b[32m[09/07 09:26:07 d2.evaluation.evaluator]: \u001b[0mInference done 1134/1199. Dataloading: 0.0783 s/iter. Inference: 0.0571 s/iter. Eval: 0.0004 s/iter. Total: 0.1360 s/iter. ETA=0:00:08\n",
      "\u001b[32m[09/07 09:26:12 d2.evaluation.evaluator]: \u001b[0mInference done 1171/1199. Dataloading: 0.0783 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1360 s/iter. ETA=0:00:03\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:41.960687 (0.135645 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:08 (0.057129 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:26:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:26:17 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:26:17 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:26:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:26:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:26:17 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:26:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:26:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:26:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0378 s/iter. Inference: 0.0463 s/iter. Eval: 0.0004 s/iter. Total: 0.0844 s/iter. ETA=0:01:40\n",
      "\u001b[32m[09/07 09:26:24 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0604 s/iter. Inference: 0.0538 s/iter. Eval: 0.0004 s/iter. Total: 0.1148 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:26:29 d2.evaluation.evaluator]: \u001b[0mInference done 86/1199. Dataloading: 0.0747 s/iter. Inference: 0.0561 s/iter. Eval: 0.0004 s/iter. Total: 0.1313 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/07 09:26:34 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0768 s/iter. Inference: 0.0581 s/iter. Eval: 0.0005 s/iter. Total: 0.1354 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/07 09:26:39 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0746 s/iter. Inference: 0.0578 s/iter. Eval: 0.0005 s/iter. Total: 0.1329 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/07 09:26:44 d2.evaluation.evaluator]: \u001b[0mInference done 202/1199. Dataloading: 0.0723 s/iter. Inference: 0.0579 s/iter. Eval: 0.0005 s/iter. Total: 0.1307 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 09:26:49 d2.evaluation.evaluator]: \u001b[0mInference done 239/1199. Dataloading: 0.0730 s/iter. Inference: 0.0581 s/iter. Eval: 0.0005 s/iter. Total: 0.1316 s/iter. ETA=0:02:06\n",
      "\u001b[32m[09/07 09:26:54 d2.evaluation.evaluator]: \u001b[0mInference done 273/1199. Dataloading: 0.0749 s/iter. Inference: 0.0582 s/iter. Eval: 0.0005 s/iter. Total: 0.1336 s/iter. ETA=0:02:03\n",
      "\u001b[32m[09/07 09:26:59 d2.evaluation.evaluator]: \u001b[0mInference done 296/1199. Dataloading: 0.0815 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1408 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:27:04 d2.evaluation.evaluator]: \u001b[0mInference done 331/1199. Dataloading: 0.0810 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1410 s/iter. ETA=0:02:02\n",
      "\u001b[32m[09/07 09:27:09 d2.evaluation.evaluator]: \u001b[0mInference done 364/1199. Dataloading: 0.0807 s/iter. Inference: 0.0609 s/iter. Eval: 0.0005 s/iter. Total: 0.1422 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/07 09:27:15 d2.evaluation.evaluator]: \u001b[0mInference done 390/1199. Dataloading: 0.0839 s/iter. Inference: 0.0617 s/iter. Eval: 0.0005 s/iter. Total: 0.1463 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/07 09:27:20 d2.evaluation.evaluator]: \u001b[0mInference done 425/1199. Dataloading: 0.0832 s/iter. Inference: 0.0622 s/iter. Eval: 0.0005 s/iter. Total: 0.1460 s/iter. ETA=0:01:53\n",
      "\u001b[32m[09/07 09:27:25 d2.evaluation.evaluator]: \u001b[0mInference done 467/1199. Dataloading: 0.0815 s/iter. Inference: 0.0616 s/iter. Eval: 0.0005 s/iter. Total: 0.1437 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 09:27:30 d2.evaluation.evaluator]: \u001b[0mInference done 508/1199. Dataloading: 0.0802 s/iter. Inference: 0.0613 s/iter. Eval: 0.0005 s/iter. Total: 0.1421 s/iter. ETA=0:01:38\n",
      "\u001b[32m[09/07 09:27:35 d2.evaluation.evaluator]: \u001b[0mInference done 547/1199. Dataloading: 0.0796 s/iter. Inference: 0.0610 s/iter. Eval: 0.0005 s/iter. Total: 0.1412 s/iter. ETA=0:01:32\n",
      "\u001b[32m[09/07 09:27:40 d2.evaluation.evaluator]: \u001b[0mInference done 586/1199. Dataloading: 0.0792 s/iter. Inference: 0.0609 s/iter. Eval: 0.0005 s/iter. Total: 0.1406 s/iter. ETA=0:01:26\n",
      "\u001b[32m[09/07 09:27:45 d2.evaluation.evaluator]: \u001b[0mInference done 628/1199. Dataloading: 0.0786 s/iter. Inference: 0.0604 s/iter. Eval: 0.0005 s/iter. Total: 0.1396 s/iter. ETA=0:01:19\n",
      "\u001b[32m[09/07 09:27:51 d2.evaluation.evaluator]: \u001b[0mInference done 661/1199. Dataloading: 0.0797 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1404 s/iter. ETA=0:01:15\n",
      "\u001b[32m[09/07 09:27:56 d2.evaluation.evaluator]: \u001b[0mInference done 694/1199. Dataloading: 0.0802 s/iter. Inference: 0.0606 s/iter. Eval: 0.0005 s/iter. Total: 0.1413 s/iter. ETA=0:01:11\n",
      "\u001b[32m[09/07 09:28:01 d2.evaluation.evaluator]: \u001b[0mInference done 733/1199. Dataloading: 0.0795 s/iter. Inference: 0.0608 s/iter. Eval: 0.0005 s/iter. Total: 0.1409 s/iter. ETA=0:01:05\n",
      "\u001b[32m[09/07 09:28:06 d2.evaluation.evaluator]: \u001b[0mInference done 761/1199. Dataloading: 0.0809 s/iter. Inference: 0.0609 s/iter. Eval: 0.0005 s/iter. Total: 0.1424 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 09:28:11 d2.evaluation.evaluator]: \u001b[0mInference done 801/1199. Dataloading: 0.0804 s/iter. Inference: 0.0607 s/iter. Eval: 0.0005 s/iter. Total: 0.1416 s/iter. ETA=0:00:56\n",
      "\u001b[32m[09/07 09:28:16 d2.evaluation.evaluator]: \u001b[0mInference done 832/1199. Dataloading: 0.0804 s/iter. Inference: 0.0614 s/iter. Eval: 0.0005 s/iter. Total: 0.1424 s/iter. ETA=0:00:52\n",
      "\u001b[32m[09/07 09:28:21 d2.evaluation.evaluator]: \u001b[0mInference done 871/1199. Dataloading: 0.0801 s/iter. Inference: 0.0613 s/iter. Eval: 0.0005 s/iter. Total: 0.1420 s/iter. ETA=0:00:46\n",
      "\u001b[32m[09/07 09:28:26 d2.evaluation.evaluator]: \u001b[0mInference done 907/1199. Dataloading: 0.0801 s/iter. Inference: 0.0613 s/iter. Eval: 0.0005 s/iter. Total: 0.1419 s/iter. ETA=0:00:41\n",
      "\u001b[32m[09/07 09:28:31 d2.evaluation.evaluator]: \u001b[0mInference done 948/1199. Dataloading: 0.0794 s/iter. Inference: 0.0611 s/iter. Eval: 0.0005 s/iter. Total: 0.1410 s/iter. ETA=0:00:35\n",
      "\u001b[32m[09/07 09:28:37 d2.evaluation.evaluator]: \u001b[0mInference done 981/1199. Dataloading: 0.0799 s/iter. Inference: 0.0611 s/iter. Eval: 0.0005 s/iter. Total: 0.1415 s/iter. ETA=0:00:30\n",
      "\u001b[32m[09/07 09:28:42 d2.evaluation.evaluator]: \u001b[0mInference done 1019/1199. Dataloading: 0.0804 s/iter. Inference: 0.0610 s/iter. Eval: 0.0005 s/iter. Total: 0.1420 s/iter. ETA=0:00:25\n",
      "\u001b[32m[09/07 09:28:47 d2.evaluation.evaluator]: \u001b[0mInference done 1056/1199. Dataloading: 0.0800 s/iter. Inference: 0.0612 s/iter. Eval: 0.0005 s/iter. Total: 0.1418 s/iter. ETA=0:00:20\n",
      "\u001b[32m[09/07 09:28:52 d2.evaluation.evaluator]: \u001b[0mInference done 1091/1199. Dataloading: 0.0800 s/iter. Inference: 0.0613 s/iter. Eval: 0.0005 s/iter. Total: 0.1418 s/iter. ETA=0:00:15\n",
      "\u001b[32m[09/07 09:28:58 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.0803 s/iter. Inference: 0.0611 s/iter. Eval: 0.0005 s/iter. Total: 0.1420 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 09:29:03 d2.evaluation.evaluator]: \u001b[0mInference done 1163/1199. Dataloading: 0.0801 s/iter. Inference: 0.0611 s/iter. Eval: 0.0005 s/iter. Total: 0.1418 s/iter. ETA=0:00:05\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:48.761916 (0.141342 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:12 (0.061053 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:29:07 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:29:09 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:29:09 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:29:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:29:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:29:09 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:29:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:29:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:29:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0446 s/iter. Inference: 0.0481 s/iter. Eval: 0.0004 s/iter. Total: 0.0930 s/iter. ETA=0:01:50\n",
      "\u001b[32m[09/07 09:29:16 d2.evaluation.evaluator]: \u001b[0mInference done 52/1199. Dataloading: 0.0666 s/iter. Inference: 0.0520 s/iter. Eval: 0.0004 s/iter. Total: 0.1192 s/iter. ETA=0:02:16\n",
      "\u001b[32m[09/07 09:29:21 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0842 s/iter. Inference: 0.0531 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:02:33\n",
      "\u001b[32m[09/07 09:29:27 d2.evaluation.evaluator]: \u001b[0mInference done 119/1199. Dataloading: 0.0846 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1449 s/iter. ETA=0:02:36\n",
      "\u001b[32m[09/07 09:29:32 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0812 s/iter. Inference: 0.0578 s/iter. Eval: 0.0005 s/iter. Total: 0.1395 s/iter. ETA=0:02:24\n",
      "\u001b[32m[09/07 09:29:37 d2.evaluation.evaluator]: \u001b[0mInference done 200/1199. Dataloading: 0.0783 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1374 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/07 09:29:42 d2.evaluation.evaluator]: \u001b[0mInference done 235/1199. Dataloading: 0.0793 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1383 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 09:29:47 d2.evaluation.evaluator]: \u001b[0mInference done 270/1199. Dataloading: 0.0801 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 09:29:52 d2.evaluation.evaluator]: \u001b[0mInference done 293/1199. Dataloading: 0.0860 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1461 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 09:29:58 d2.evaluation.evaluator]: \u001b[0mInference done 327/1199. Dataloading: 0.0867 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1468 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 09:30:03 d2.evaluation.evaluator]: \u001b[0mInference done 368/1199. Dataloading: 0.0848 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1443 s/iter. ETA=0:01:59\n",
      "\u001b[32m[09/07 09:30:08 d2.evaluation.evaluator]: \u001b[0mInference done 404/1199. Dataloading: 0.0846 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1442 s/iter. ETA=0:01:54\n",
      "\u001b[32m[09/07 09:30:13 d2.evaluation.evaluator]: \u001b[0mInference done 446/1199. Dataloading: 0.0833 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1423 s/iter. ETA=0:01:47\n",
      "\u001b[32m[09/07 09:30:18 d2.evaluation.evaluator]: \u001b[0mInference done 484/1199. Dataloading: 0.0829 s/iter. Inference: 0.0583 s/iter. Eval: 0.0005 s/iter. Total: 0.1417 s/iter. ETA=0:01:41\n",
      "\u001b[32m[09/07 09:30:23 d2.evaluation.evaluator]: \u001b[0mInference done 524/1199. Dataloading: 0.0818 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1407 s/iter. ETA=0:01:34\n",
      "\u001b[32m[09/07 09:30:28 d2.evaluation.evaluator]: \u001b[0mInference done 566/1199. Dataloading: 0.0803 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:01:28\n",
      "\u001b[32m[09/07 09:30:34 d2.evaluation.evaluator]: \u001b[0mInference done 606/1199. Dataloading: 0.0798 s/iter. Inference: 0.0582 s/iter. Eval: 0.0005 s/iter. Total: 0.1385 s/iter. ETA=0:01:22\n",
      "\u001b[32m[09/07 09:30:39 d2.evaluation.evaluator]: \u001b[0mInference done 645/1199. Dataloading: 0.0807 s/iter. Inference: 0.0581 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:01:17\n",
      "\u001b[32m[09/07 09:30:44 d2.evaluation.evaluator]: \u001b[0mInference done 685/1199. Dataloading: 0.0800 s/iter. Inference: 0.0580 s/iter. Eval: 0.0005 s/iter. Total: 0.1385 s/iter. ETA=0:01:11\n",
      "\u001b[32m[09/07 09:30:50 d2.evaluation.evaluator]: \u001b[0mInference done 724/1199. Dataloading: 0.0797 s/iter. Inference: 0.0580 s/iter. Eval: 0.0005 s/iter. Total: 0.1383 s/iter. ETA=0:01:05\n",
      "\u001b[32m[09/07 09:30:55 d2.evaluation.evaluator]: \u001b[0mInference done 752/1199. Dataloading: 0.0814 s/iter. Inference: 0.0581 s/iter. Eval: 0.0005 s/iter. Total: 0.1401 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 09:31:00 d2.evaluation.evaluator]: \u001b[0mInference done 790/1199. Dataloading: 0.0809 s/iter. Inference: 0.0583 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:00:57\n",
      "\u001b[32m[09/07 09:31:05 d2.evaluation.evaluator]: \u001b[0mInference done 832/1199. Dataloading: 0.0799 s/iter. Inference: 0.0583 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:00:50\n",
      "\u001b[32m[09/07 09:31:10 d2.evaluation.evaluator]: \u001b[0mInference done 871/1199. Dataloading: 0.0795 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1383 s/iter. ETA=0:00:45\n",
      "\u001b[32m[09/07 09:31:15 d2.evaluation.evaluator]: \u001b[0mInference done 905/1199. Dataloading: 0.0796 s/iter. Inference: 0.0586 s/iter. Eval: 0.0005 s/iter. Total: 0.1388 s/iter. ETA=0:00:40\n",
      "\u001b[32m[09/07 09:31:20 d2.evaluation.evaluator]: \u001b[0mInference done 948/1199. Dataloading: 0.0787 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1380 s/iter. ETA=0:00:34\n",
      "\u001b[32m[09/07 09:31:26 d2.evaluation.evaluator]: \u001b[0mInference done 981/1199. Dataloading: 0.0791 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1386 s/iter. ETA=0:00:30\n",
      "\u001b[32m[09/07 09:31:31 d2.evaluation.evaluator]: \u001b[0mInference done 1019/1199. Dataloading: 0.0796 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:00:25\n",
      "\u001b[32m[09/07 09:31:36 d2.evaluation.evaluator]: \u001b[0mInference done 1059/1199. Dataloading: 0.0794 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:00:19\n",
      "\u001b[32m[09/07 09:31:42 d2.evaluation.evaluator]: \u001b[0mInference done 1095/1199. Dataloading: 0.0793 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1388 s/iter. ETA=0:00:14\n",
      "\u001b[32m[09/07 09:31:47 d2.evaluation.evaluator]: \u001b[0mInference done 1130/1199. Dataloading: 0.0796 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:00:09\n",
      "\u001b[32m[09/07 09:31:52 d2.evaluation.evaluator]: \u001b[0mInference done 1168/1199. Dataloading: 0.0794 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:00:04\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:45.316548 (0.138456 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.059039 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:31:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:31:58 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:31:58 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:31:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:31:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:31:58 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:31:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:31:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:32:00 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0738 s/iter. Inference: 0.0676 s/iter. Eval: 0.0004 s/iter. Total: 0.1419 s/iter. ETA=0:02:48\n",
      "\u001b[32m[09/07 09:32:05 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0665 s/iter. Inference: 0.0555 s/iter. Eval: 0.0004 s/iter. Total: 0.1225 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/07 09:32:10 d2.evaluation.evaluator]: \u001b[0mInference done 87/1199. Dataloading: 0.0792 s/iter. Inference: 0.0567 s/iter. Eval: 0.0004 s/iter. Total: 0.1365 s/iter. ETA=0:02:31\n",
      "\u001b[32m[09/07 09:32:16 d2.evaluation.evaluator]: \u001b[0mInference done 123/1199. Dataloading: 0.0792 s/iter. Inference: 0.0582 s/iter. Eval: 0.0004 s/iter. Total: 0.1379 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/07 09:32:21 d2.evaluation.evaluator]: \u001b[0mInference done 163/1199. Dataloading: 0.0771 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1356 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/07 09:32:26 d2.evaluation.evaluator]: \u001b[0mInference done 201/1199. Dataloading: 0.0773 s/iter. Inference: 0.0578 s/iter. Eval: 0.0004 s/iter. Total: 0.1356 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 09:32:31 d2.evaluation.evaluator]: \u001b[0mInference done 238/1199. Dataloading: 0.0772 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1358 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 09:32:36 d2.evaluation.evaluator]: \u001b[0mInference done 272/1199. Dataloading: 0.0796 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1382 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 09:32:41 d2.evaluation.evaluator]: \u001b[0mInference done 294/1199. Dataloading: 0.0860 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1453 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:32:46 d2.evaluation.evaluator]: \u001b[0mInference done 332/1199. Dataloading: 0.0845 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1438 s/iter. ETA=0:02:04\n",
      "\u001b[32m[09/07 09:32:51 d2.evaluation.evaluator]: \u001b[0mInference done 371/1199. Dataloading: 0.0832 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1423 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/07 09:32:56 d2.evaluation.evaluator]: \u001b[0mInference done 407/1199. Dataloading: 0.0826 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1422 s/iter. ETA=0:01:52\n",
      "\u001b[32m[09/07 09:33:02 d2.evaluation.evaluator]: \u001b[0mInference done 448/1199. Dataloading: 0.0812 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1409 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 09:33:07 d2.evaluation.evaluator]: \u001b[0mInference done 489/1199. Dataloading: 0.0798 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1394 s/iter. ETA=0:01:38\n",
      "\u001b[32m[09/07 09:33:12 d2.evaluation.evaluator]: \u001b[0mInference done 528/1199. Dataloading: 0.0791 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1386 s/iter. ETA=0:01:32\n",
      "\u001b[32m[09/07 09:33:17 d2.evaluation.evaluator]: \u001b[0mInference done 568/1199. Dataloading: 0.0784 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1376 s/iter. ETA=0:01:26\n",
      "\u001b[32m[09/07 09:33:22 d2.evaluation.evaluator]: \u001b[0mInference done 608/1199. Dataloading: 0.0776 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1370 s/iter. ETA=0:01:20\n",
      "\u001b[32m[09/07 09:33:28 d2.evaluation.evaluator]: \u001b[0mInference done 645/1199. Dataloading: 0.0788 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1381 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 09:33:33 d2.evaluation.evaluator]: \u001b[0mInference done 683/1199. Dataloading: 0.0784 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:01:11\n",
      "\u001b[32m[09/07 09:33:38 d2.evaluation.evaluator]: \u001b[0mInference done 726/1199. Dataloading: 0.0776 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1368 s/iter. ETA=0:01:04\n",
      "\u001b[32m[09/07 09:33:43 d2.evaluation.evaluator]: \u001b[0mInference done 754/1199. Dataloading: 0.0792 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1386 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/07 09:33:48 d2.evaluation.evaluator]: \u001b[0mInference done 792/1199. Dataloading: 0.0788 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1382 s/iter. ETA=0:00:56\n",
      "\u001b[32m[09/07 09:33:53 d2.evaluation.evaluator]: \u001b[0mInference done 837/1199. Dataloading: 0.0775 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1369 s/iter. ETA=0:00:49\n",
      "\u001b[32m[09/07 09:33:58 d2.evaluation.evaluator]: \u001b[0mInference done 877/1199. Dataloading: 0.0772 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1364 s/iter. ETA=0:00:43\n",
      "\u001b[32m[09/07 09:34:03 d2.evaluation.evaluator]: \u001b[0mInference done 914/1199. Dataloading: 0.0773 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1364 s/iter. ETA=0:00:38\n",
      "\u001b[32m[09/07 09:34:08 d2.evaluation.evaluator]: \u001b[0mInference done 959/1199. Dataloading: 0.0765 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1355 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 09:34:14 d2.evaluation.evaluator]: \u001b[0mInference done 991/1199. Dataloading: 0.0773 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1363 s/iter. ETA=0:00:28\n",
      "\u001b[32m[09/07 09:34:19 d2.evaluation.evaluator]: \u001b[0mInference done 1028/1199. Dataloading: 0.0773 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1362 s/iter. ETA=0:00:23\n",
      "\u001b[32m[09/07 09:34:24 d2.evaluation.evaluator]: \u001b[0mInference done 1066/1199. Dataloading: 0.0773 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1362 s/iter. ETA=0:00:18\n",
      "\u001b[32m[09/07 09:34:29 d2.evaluation.evaluator]: \u001b[0mInference done 1101/1199. Dataloading: 0.0775 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1364 s/iter. ETA=0:00:13\n",
      "\u001b[32m[09/07 09:34:34 d2.evaluation.evaluator]: \u001b[0mInference done 1136/1199. Dataloading: 0.0779 s/iter. Inference: 0.0583 s/iter. Eval: 0.0004 s/iter. Total: 0.1367 s/iter. ETA=0:00:08\n",
      "\u001b[32m[09/07 09:34:39 d2.evaluation.evaluator]: \u001b[0mInference done 1176/1199. Dataloading: 0.0777 s/iter. Inference: 0.0582 s/iter. Eval: 0.0004 s/iter. Total: 0.1364 s/iter. ETA=0:00:03\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:42.564815 (0.136151 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:09 (0.058127 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:34:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:34:44 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:34:44 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:34:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:34:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:34:44 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:34:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:34:44 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:34:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0343 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0830 s/iter. ETA=0:01:38\n",
      "\u001b[32m[09/07 09:34:51 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0632 s/iter. Inference: 0.0545 s/iter. Eval: 0.0004 s/iter. Total: 0.1182 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 09:34:56 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0819 s/iter. Inference: 0.0552 s/iter. Eval: 0.0004 s/iter. Total: 0.1377 s/iter. ETA=0:02:33\n",
      "\u001b[32m[09/07 09:35:01 d2.evaluation.evaluator]: \u001b[0mInference done 120/1199. Dataloading: 0.0822 s/iter. Inference: 0.0568 s/iter. Eval: 0.0004 s/iter. Total: 0.1395 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/07 09:35:06 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0783 s/iter. Inference: 0.0562 s/iter. Eval: 0.0004 s/iter. Total: 0.1351 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/07 09:35:11 d2.evaluation.evaluator]: \u001b[0mInference done 201/1199. Dataloading: 0.0763 s/iter. Inference: 0.0563 s/iter. Eval: 0.0004 s/iter. Total: 0.1332 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 09:35:16 d2.evaluation.evaluator]: \u001b[0mInference done 237/1199. Dataloading: 0.0773 s/iter. Inference: 0.0573 s/iter. Eval: 0.0004 s/iter. Total: 0.1351 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 09:35:21 d2.evaluation.evaluator]: \u001b[0mInference done 271/1199. Dataloading: 0.0782 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1368 s/iter. ETA=0:02:06\n",
      "\u001b[32m[09/07 09:35:27 d2.evaluation.evaluator]: \u001b[0mInference done 294/1199. Dataloading: 0.0853 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1442 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 09:35:32 d2.evaluation.evaluator]: \u001b[0mInference done 331/1199. Dataloading: 0.0842 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1432 s/iter. ETA=0:02:04\n",
      "\u001b[32m[09/07 09:35:37 d2.evaluation.evaluator]: \u001b[0mInference done 371/1199. Dataloading: 0.0828 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1413 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/07 09:35:42 d2.evaluation.evaluator]: \u001b[0mInference done 406/1199. Dataloading: 0.0831 s/iter. Inference: 0.0578 s/iter. Eval: 0.0004 s/iter. Total: 0.1415 s/iter. ETA=0:01:52\n",
      "\u001b[32m[09/07 09:35:47 d2.evaluation.evaluator]: \u001b[0mInference done 445/1199. Dataloading: 0.0817 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1405 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 09:35:52 d2.evaluation.evaluator]: \u001b[0mInference done 477/1199. Dataloading: 0.0818 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1417 s/iter. ETA=0:01:42\n",
      "\u001b[32m[09/07 09:35:57 d2.evaluation.evaluator]: \u001b[0mInference done 518/1199. Dataloading: 0.0806 s/iter. Inference: 0.0593 s/iter. Eval: 0.0004 s/iter. Total: 0.1404 s/iter. ETA=0:01:35\n",
      "\u001b[32m[09/07 09:36:02 d2.evaluation.evaluator]: \u001b[0mInference done 554/1199. Dataloading: 0.0805 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1404 s/iter. ETA=0:01:30\n",
      "\u001b[32m[09/07 09:36:07 d2.evaluation.evaluator]: \u001b[0mInference done 593/1199. Dataloading: 0.0798 s/iter. Inference: 0.0593 s/iter. Eval: 0.0004 s/iter. Total: 0.1396 s/iter. ETA=0:01:24\n",
      "\u001b[32m[09/07 09:36:12 d2.evaluation.evaluator]: \u001b[0mInference done 633/1199. Dataloading: 0.0792 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1388 s/iter. ETA=0:01:18\n",
      "\u001b[32m[09/07 09:36:17 d2.evaluation.evaluator]: \u001b[0mInference done 667/1199. Dataloading: 0.0799 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1395 s/iter. ETA=0:01:14\n",
      "\u001b[32m[09/07 09:36:22 d2.evaluation.evaluator]: \u001b[0mInference done 702/1199. Dataloading: 0.0802 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1397 s/iter. ETA=0:01:09\n",
      "\u001b[32m[09/07 09:36:27 d2.evaluation.evaluator]: \u001b[0mInference done 742/1199. Dataloading: 0.0795 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1391 s/iter. ETA=0:01:03\n",
      "\u001b[32m[09/07 09:36:32 d2.evaluation.evaluator]: \u001b[0mInference done 764/1199. Dataloading: 0.0819 s/iter. Inference: 0.0593 s/iter. Eval: 0.0004 s/iter. Total: 0.1417 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/07 09:36:37 d2.evaluation.evaluator]: \u001b[0mInference done 806/1199. Dataloading: 0.0808 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1406 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/07 09:36:43 d2.evaluation.evaluator]: \u001b[0mInference done 852/1199. Dataloading: 0.0795 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1390 s/iter. ETA=0:00:48\n",
      "\u001b[32m[09/07 09:36:48 d2.evaluation.evaluator]: \u001b[0mInference done 890/1199. Dataloading: 0.0792 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1387 s/iter. ETA=0:00:42\n",
      "\u001b[32m[09/07 09:36:53 d2.evaluation.evaluator]: \u001b[0mInference done 929/1199. Dataloading: 0.0791 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1383 s/iter. ETA=0:00:37\n",
      "\u001b[32m[09/07 09:36:58 d2.evaluation.evaluator]: \u001b[0mInference done 963/1199. Dataloading: 0.0794 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1386 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 09:37:03 d2.evaluation.evaluator]: \u001b[0mInference done 1003/1199. Dataloading: 0.0789 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1382 s/iter. ETA=0:00:27\n",
      "\u001b[32m[09/07 09:37:08 d2.evaluation.evaluator]: \u001b[0mInference done 1038/1199. Dataloading: 0.0792 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1385 s/iter. ETA=0:00:22\n",
      "\u001b[32m[09/07 09:37:13 d2.evaluation.evaluator]: \u001b[0mInference done 1076/1199. Dataloading: 0.0792 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1383 s/iter. ETA=0:00:17\n",
      "\u001b[32m[09/07 09:37:18 d2.evaluation.evaluator]: \u001b[0mInference done 1117/1199. Dataloading: 0.0788 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1377 s/iter. ETA=0:00:11\n",
      "\u001b[32m[09/07 09:37:23 d2.evaluation.evaluator]: \u001b[0mInference done 1148/1199. Dataloading: 0.0794 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1384 s/iter. ETA=0:00:07\n",
      "\u001b[32m[09/07 09:37:28 d2.evaluation.evaluator]: \u001b[0mInference done 1190/1199. Dataloading: 0.0788 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:00:01\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:44.703282 (0.137942 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:09 (0.058465 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:37:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:37:32 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:37:32 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:37:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:37:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:37:32 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:37:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:37:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:37:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0360 s/iter. Inference: 0.0508 s/iter. Eval: 0.0004 s/iter. Total: 0.0872 s/iter. ETA=0:01:43\n",
      "\u001b[32m[09/07 09:37:38 d2.evaluation.evaluator]: \u001b[0mInference done 53/1199. Dataloading: 0.0584 s/iter. Inference: 0.0571 s/iter. Eval: 0.0004 s/iter. Total: 0.1160 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 09:37:44 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0782 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1382 s/iter. ETA=0:02:33\n",
      "\u001b[32m[09/07 09:37:49 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0784 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/07 09:37:54 d2.evaluation.evaluator]: \u001b[0mInference done 159/1199. Dataloading: 0.0753 s/iter. Inference: 0.0626 s/iter. Eval: 0.0005 s/iter. Total: 0.1385 s/iter. ETA=0:02:23\n",
      "\u001b[32m[09/07 09:37:59 d2.evaluation.evaluator]: \u001b[0mInference done 199/1199. Dataloading: 0.0741 s/iter. Inference: 0.0615 s/iter. Eval: 0.0005 s/iter. Total: 0.1362 s/iter. ETA=0:02:16\n",
      "\u001b[32m[09/07 09:38:04 d2.evaluation.evaluator]: \u001b[0mInference done 237/1199. Dataloading: 0.0743 s/iter. Inference: 0.0616 s/iter. Eval: 0.0005 s/iter. Total: 0.1365 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:38:09 d2.evaluation.evaluator]: \u001b[0mInference done 271/1199. Dataloading: 0.0758 s/iter. Inference: 0.0616 s/iter. Eval: 0.0005 s/iter. Total: 0.1379 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:38:14 d2.evaluation.evaluator]: \u001b[0mInference done 293/1199. Dataloading: 0.0827 s/iter. Inference: 0.0618 s/iter. Eval: 0.0005 s/iter. Total: 0.1450 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:38:20 d2.evaluation.evaluator]: \u001b[0mInference done 328/1199. Dataloading: 0.0828 s/iter. Inference: 0.0615 s/iter. Eval: 0.0005 s/iter. Total: 0.1448 s/iter. ETA=0:02:06\n",
      "\u001b[32m[09/07 09:38:25 d2.evaluation.evaluator]: \u001b[0mInference done 369/1199. Dataloading: 0.0807 s/iter. Inference: 0.0610 s/iter. Eval: 0.0005 s/iter. Total: 0.1423 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/07 09:38:30 d2.evaluation.evaluator]: \u001b[0mInference done 404/1199. Dataloading: 0.0812 s/iter. Inference: 0.0607 s/iter. Eval: 0.0005 s/iter. Total: 0.1424 s/iter. ETA=0:01:53\n",
      "\u001b[32m[09/07 09:38:35 d2.evaluation.evaluator]: \u001b[0mInference done 446/1199. Dataloading: 0.0794 s/iter. Inference: 0.0607 s/iter. Eval: 0.0005 s/iter. Total: 0.1407 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 09:38:40 d2.evaluation.evaluator]: \u001b[0mInference done 485/1199. Dataloading: 0.0786 s/iter. Inference: 0.0605 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:01:39\n",
      "\u001b[32m[09/07 09:38:45 d2.evaluation.evaluator]: \u001b[0mInference done 524/1199. Dataloading: 0.0787 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1395 s/iter. ETA=0:01:34\n",
      "\u001b[32m[09/07 09:38:50 d2.evaluation.evaluator]: \u001b[0mInference done 566/1199. Dataloading: 0.0779 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1382 s/iter. ETA=0:01:27\n",
      "\u001b[32m[09/07 09:38:55 d2.evaluation.evaluator]: \u001b[0mInference done 605/1199. Dataloading: 0.0773 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1377 s/iter. ETA=0:01:21\n",
      "\u001b[32m[09/07 09:39:01 d2.evaluation.evaluator]: \u001b[0mInference done 645/1199. Dataloading: 0.0786 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1386 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 09:39:07 d2.evaluation.evaluator]: \u001b[0mInference done 684/1199. Dataloading: 0.0781 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1382 s/iter. ETA=0:01:11\n",
      "\u001b[32m[09/07 09:39:12 d2.evaluation.evaluator]: \u001b[0mInference done 724/1199. Dataloading: 0.0774 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1377 s/iter. ETA=0:01:05\n",
      "\u001b[32m[09/07 09:39:17 d2.evaluation.evaluator]: \u001b[0mInference done 752/1199. Dataloading: 0.0791 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1396 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 09:39:22 d2.evaluation.evaluator]: \u001b[0mInference done 791/1199. Dataloading: 0.0788 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:00:56\n",
      "\u001b[32m[09/07 09:39:27 d2.evaluation.evaluator]: \u001b[0mInference done 835/1199. Dataloading: 0.0777 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1381 s/iter. ETA=0:00:50\n",
      "\u001b[32m[09/07 09:39:32 d2.evaluation.evaluator]: \u001b[0mInference done 876/1199. Dataloading: 0.0771 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1374 s/iter. ETA=0:00:44\n",
      "\u001b[32m[09/07 09:39:38 d2.evaluation.evaluator]: \u001b[0mInference done 913/1199. Dataloading: 0.0771 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1374 s/iter. ETA=0:00:39\n",
      "\u001b[32m[09/07 09:39:43 d2.evaluation.evaluator]: \u001b[0mInference done 958/1199. Dataloading: 0.0760 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1362 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 09:39:48 d2.evaluation.evaluator]: \u001b[0mInference done 989/1199. Dataloading: 0.0769 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1372 s/iter. ETA=0:00:28\n",
      "\u001b[32m[09/07 09:39:53 d2.evaluation.evaluator]: \u001b[0mInference done 1025/1199. Dataloading: 0.0771 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1373 s/iter. ETA=0:00:23\n",
      "\u001b[32m[09/07 09:39:58 d2.evaluation.evaluator]: \u001b[0mInference done 1059/1199. Dataloading: 0.0772 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1376 s/iter. ETA=0:00:19\n",
      "\u001b[32m[09/07 09:40:03 d2.evaluation.evaluator]: \u001b[0mInference done 1092/1199. Dataloading: 0.0775 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1381 s/iter. ETA=0:00:14\n",
      "\u001b[32m[09/07 09:40:08 d2.evaluation.evaluator]: \u001b[0mInference done 1127/1199. Dataloading: 0.0778 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1383 s/iter. ETA=0:00:09\n",
      "\u001b[32m[09/07 09:40:13 d2.evaluation.evaluator]: \u001b[0mInference done 1164/1199. Dataloading: 0.0779 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1382 s/iter. ETA=0:00:04\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:44.485994 (0.137760 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:11 (0.059654 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:40:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:40:19 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:40:19 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:40:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:40:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:40:19 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:40:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:40:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:40:21 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0339 s/iter. Inference: 0.0559 s/iter. Eval: 0.0005 s/iter. Total: 0.0904 s/iter. ETA=0:01:47\n",
      "\u001b[32m[09/07 09:40:26 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0602 s/iter. Inference: 0.0552 s/iter. Eval: 0.0004 s/iter. Total: 0.1159 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 09:40:31 d2.evaluation.evaluator]: \u001b[0mInference done 86/1199. Dataloading: 0.0768 s/iter. Inference: 0.0550 s/iter. Eval: 0.0005 s/iter. Total: 0.1323 s/iter. ETA=0:02:27\n",
      "\u001b[32m[09/07 09:40:36 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0790 s/iter. Inference: 0.0562 s/iter. Eval: 0.0005 s/iter. Total: 0.1358 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/07 09:40:41 d2.evaluation.evaluator]: \u001b[0mInference done 162/1199. Dataloading: 0.0754 s/iter. Inference: 0.0564 s/iter. Eval: 0.0005 s/iter. Total: 0.1324 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/07 09:40:46 d2.evaluation.evaluator]: \u001b[0mInference done 201/1199. Dataloading: 0.0733 s/iter. Inference: 0.0579 s/iter. Eval: 0.0005 s/iter. Total: 0.1317 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:40:51 d2.evaluation.evaluator]: \u001b[0mInference done 237/1199. Dataloading: 0.0745 s/iter. Inference: 0.0578 s/iter. Eval: 0.0005 s/iter. Total: 0.1329 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:40:56 d2.evaluation.evaluator]: \u001b[0mInference done 270/1199. Dataloading: 0.0768 s/iter. Inference: 0.0580 s/iter. Eval: 0.0005 s/iter. Total: 0.1353 s/iter. ETA=0:02:05\n",
      "\u001b[32m[09/07 09:41:02 d2.evaluation.evaluator]: \u001b[0mInference done 289/1199. Dataloading: 0.0849 s/iter. Inference: 0.0586 s/iter. Eval: 0.0005 s/iter. Total: 0.1441 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:41:07 d2.evaluation.evaluator]: \u001b[0mInference done 321/1199. Dataloading: 0.0855 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1456 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:41:12 d2.evaluation.evaluator]: \u001b[0mInference done 356/1199. Dataloading: 0.0850 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1458 s/iter. ETA=0:02:02\n",
      "\u001b[32m[09/07 09:41:17 d2.evaluation.evaluator]: \u001b[0mInference done 387/1199. Dataloading: 0.0860 s/iter. Inference: 0.0607 s/iter. Eval: 0.0005 s/iter. Total: 0.1473 s/iter. ETA=0:01:59\n",
      "\u001b[32m[09/07 09:41:22 d2.evaluation.evaluator]: \u001b[0mInference done 422/1199. Dataloading: 0.0861 s/iter. Inference: 0.0609 s/iter. Eval: 0.0005 s/iter. Total: 0.1476 s/iter. ETA=0:01:54\n",
      "\u001b[32m[09/07 09:41:27 d2.evaluation.evaluator]: \u001b[0mInference done 464/1199. Dataloading: 0.0838 s/iter. Inference: 0.0607 s/iter. Eval: 0.0005 s/iter. Total: 0.1451 s/iter. ETA=0:01:46\n",
      "\u001b[32m[09/07 09:41:32 d2.evaluation.evaluator]: \u001b[0mInference done 492/1199. Dataloading: 0.0846 s/iter. Inference: 0.0620 s/iter. Eval: 0.0005 s/iter. Total: 0.1471 s/iter. ETA=0:01:43\n",
      "\u001b[32m[09/07 09:41:38 d2.evaluation.evaluator]: \u001b[0mInference done 524/1199. Dataloading: 0.0853 s/iter. Inference: 0.0624 s/iter. Eval: 0.0005 s/iter. Total: 0.1483 s/iter. ETA=0:01:40\n",
      "\u001b[32m[09/07 09:41:43 d2.evaluation.evaluator]: \u001b[0mInference done 554/1199. Dataloading: 0.0861 s/iter. Inference: 0.0630 s/iter. Eval: 0.0005 s/iter. Total: 0.1496 s/iter. ETA=0:01:36\n",
      "\u001b[32m[09/07 09:41:48 d2.evaluation.evaluator]: \u001b[0mInference done 589/1199. Dataloading: 0.0855 s/iter. Inference: 0.0633 s/iter. Eval: 0.0005 s/iter. Total: 0.1494 s/iter. ETA=0:01:31\n",
      "\u001b[32m[09/07 09:41:53 d2.evaluation.evaluator]: \u001b[0mInference done 628/1199. Dataloading: 0.0846 s/iter. Inference: 0.0630 s/iter. Eval: 0.0005 s/iter. Total: 0.1482 s/iter. ETA=0:01:24\n",
      "\u001b[32m[09/07 09:41:58 d2.evaluation.evaluator]: \u001b[0mInference done 660/1199. Dataloading: 0.0854 s/iter. Inference: 0.0627 s/iter. Eval: 0.0005 s/iter. Total: 0.1486 s/iter. ETA=0:01:20\n",
      "\u001b[32m[09/07 09:42:03 d2.evaluation.evaluator]: \u001b[0mInference done 696/1199. Dataloading: 0.0855 s/iter. Inference: 0.0622 s/iter. Eval: 0.0005 s/iter. Total: 0.1483 s/iter. ETA=0:01:14\n",
      "\u001b[32m[09/07 09:42:08 d2.evaluation.evaluator]: \u001b[0mInference done 736/1199. Dataloading: 0.0847 s/iter. Inference: 0.0621 s/iter. Eval: 0.0005 s/iter. Total: 0.1473 s/iter. ETA=0:01:08\n",
      "\u001b[32m[09/07 09:42:13 d2.evaluation.evaluator]: \u001b[0mInference done 763/1199. Dataloading: 0.0858 s/iter. Inference: 0.0623 s/iter. Eval: 0.0005 s/iter. Total: 0.1487 s/iter. ETA=0:01:04\n",
      "\u001b[32m[09/07 09:42:18 d2.evaluation.evaluator]: \u001b[0mInference done 804/1199. Dataloading: 0.0847 s/iter. Inference: 0.0621 s/iter. Eval: 0.0005 s/iter. Total: 0.1474 s/iter. ETA=0:00:58\n",
      "\u001b[32m[09/07 09:42:23 d2.evaluation.evaluator]: \u001b[0mInference done 849/1199. Dataloading: 0.0831 s/iter. Inference: 0.0619 s/iter. Eval: 0.0005 s/iter. Total: 0.1455 s/iter. ETA=0:00:50\n",
      "\u001b[32m[09/07 09:42:28 d2.evaluation.evaluator]: \u001b[0mInference done 886/1199. Dataloading: 0.0827 s/iter. Inference: 0.0619 s/iter. Eval: 0.0005 s/iter. Total: 0.1451 s/iter. ETA=0:00:45\n",
      "\u001b[32m[09/07 09:42:33 d2.evaluation.evaluator]: \u001b[0mInference done 921/1199. Dataloading: 0.0827 s/iter. Inference: 0.0618 s/iter. Eval: 0.0005 s/iter. Total: 0.1450 s/iter. ETA=0:00:40\n",
      "\u001b[32m[09/07 09:42:39 d2.evaluation.evaluator]: \u001b[0mInference done 963/1199. Dataloading: 0.0828 s/iter. Inference: 0.0615 s/iter. Eval: 0.0005 s/iter. Total: 0.1449 s/iter. ETA=0:00:34\n",
      "\u001b[32m[09/07 09:42:44 d2.evaluation.evaluator]: \u001b[0mInference done 1004/1199. Dataloading: 0.0822 s/iter. Inference: 0.0612 s/iter. Eval: 0.0005 s/iter. Total: 0.1440 s/iter. ETA=0:00:28\n",
      "\u001b[32m[09/07 09:42:50 d2.evaluation.evaluator]: \u001b[0mInference done 1038/1199. Dataloading: 0.0826 s/iter. Inference: 0.0611 s/iter. Eval: 0.0005 s/iter. Total: 0.1443 s/iter. ETA=0:00:23\n",
      "\u001b[32m[09/07 09:42:55 d2.evaluation.evaluator]: \u001b[0mInference done 1075/1199. Dataloading: 0.0825 s/iter. Inference: 0.0611 s/iter. Eval: 0.0005 s/iter. Total: 0.1441 s/iter. ETA=0:00:17\n",
      "\u001b[32m[09/07 09:43:00 d2.evaluation.evaluator]: \u001b[0mInference done 1113/1199. Dataloading: 0.0820 s/iter. Inference: 0.0612 s/iter. Eval: 0.0005 s/iter. Total: 0.1437 s/iter. ETA=0:00:12\n",
      "\u001b[32m[09/07 09:43:05 d2.evaluation.evaluator]: \u001b[0mInference done 1144/1199. Dataloading: 0.0824 s/iter. Inference: 0.0613 s/iter. Eval: 0.0005 s/iter. Total: 0.1442 s/iter. ETA=0:00:07\n",
      "\u001b[32m[09/07 09:43:10 d2.evaluation.evaluator]: \u001b[0mInference done 1187/1199. Dataloading: 0.0816 s/iter. Inference: 0.0611 s/iter. Eval: 0.0005 s/iter. Total: 0.1432 s/iter. ETA=0:00:01\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:51.083669 (0.143286 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:12 (0.061064 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:43:12 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:43:14 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:43:14 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:43:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:43:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:43:14 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:43:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:43:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:43:16 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0398 s/iter. Inference: 0.0521 s/iter. Eval: 0.0005 s/iter. Total: 0.0923 s/iter. ETA=0:01:49\n",
      "\u001b[32m[09/07 09:43:21 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0652 s/iter. Inference: 0.0528 s/iter. Eval: 0.0004 s/iter. Total: 0.1185 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 09:43:26 d2.evaluation.evaluator]: \u001b[0mInference done 86/1199. Dataloading: 0.0802 s/iter. Inference: 0.0535 s/iter. Eval: 0.0004 s/iter. Total: 0.1342 s/iter. ETA=0:02:29\n",
      "\u001b[32m[09/07 09:43:31 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0807 s/iter. Inference: 0.0557 s/iter. Eval: 0.0004 s/iter. Total: 0.1369 s/iter. ETA=0:02:27\n",
      "\u001b[32m[09/07 09:43:36 d2.evaluation.evaluator]: \u001b[0mInference done 162/1199. Dataloading: 0.0741 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1333 s/iter. ETA=0:02:18\n",
      "\u001b[32m[09/07 09:43:41 d2.evaluation.evaluator]: \u001b[0mInference done 197/1199. Dataloading: 0.0756 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1356 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 09:43:46 d2.evaluation.evaluator]: \u001b[0mInference done 233/1199. Dataloading: 0.0763 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1363 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:43:51 d2.evaluation.evaluator]: \u001b[0mInference done 268/1199. Dataloading: 0.0771 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1372 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:43:56 d2.evaluation.evaluator]: \u001b[0mInference done 290/1199. Dataloading: 0.0833 s/iter. Inference: 0.0605 s/iter. Eval: 0.0004 s/iter. Total: 0.1443 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 09:44:02 d2.evaluation.evaluator]: \u001b[0mInference done 327/1199. Dataloading: 0.0832 s/iter. Inference: 0.0604 s/iter. Eval: 0.0005 s/iter. Total: 0.1442 s/iter. ETA=0:02:05\n",
      "\u001b[32m[09/07 09:44:07 d2.evaluation.evaluator]: \u001b[0mInference done 366/1199. Dataloading: 0.0819 s/iter. Inference: 0.0603 s/iter. Eval: 0.0005 s/iter. Total: 0.1427 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/07 09:44:12 d2.evaluation.evaluator]: \u001b[0mInference done 401/1199. Dataloading: 0.0821 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1428 s/iter. ETA=0:01:53\n",
      "\u001b[32m[09/07 09:44:17 d2.evaluation.evaluator]: \u001b[0mInference done 442/1199. Dataloading: 0.0802 s/iter. Inference: 0.0601 s/iter. Eval: 0.0004 s/iter. Total: 0.1408 s/iter. ETA=0:01:46\n",
      "\u001b[32m[09/07 09:44:22 d2.evaluation.evaluator]: \u001b[0mInference done 480/1199. Dataloading: 0.0799 s/iter. Inference: 0.0598 s/iter. Eval: 0.0004 s/iter. Total: 0.1403 s/iter. ETA=0:01:40\n",
      "\u001b[32m[09/07 09:44:27 d2.evaluation.evaluator]: \u001b[0mInference done 521/1199. Dataloading: 0.0790 s/iter. Inference: 0.0593 s/iter. Eval: 0.0004 s/iter. Total: 0.1389 s/iter. ETA=0:01:34\n",
      "\u001b[32m[09/07 09:44:32 d2.evaluation.evaluator]: \u001b[0mInference done 560/1199. Dataloading: 0.0787 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1384 s/iter. ETA=0:01:28\n",
      "\u001b[32m[09/07 09:44:37 d2.evaluation.evaluator]: \u001b[0mInference done 600/1199. Dataloading: 0.0780 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1376 s/iter. ETA=0:01:22\n",
      "\u001b[32m[09/07 09:44:42 d2.evaluation.evaluator]: \u001b[0mInference done 641/1199. Dataloading: 0.0774 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1370 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 09:44:48 d2.evaluation.evaluator]: \u001b[0mInference done 675/1199. Dataloading: 0.0791 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1388 s/iter. ETA=0:01:12\n",
      "\u001b[32m[09/07 09:44:53 d2.evaluation.evaluator]: \u001b[0mInference done 720/1199. Dataloading: 0.0776 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1370 s/iter. ETA=0:01:05\n",
      "\u001b[32m[09/07 09:44:59 d2.evaluation.evaluator]: \u001b[0mInference done 752/1199. Dataloading: 0.0795 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1389 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 09:45:04 d2.evaluation.evaluator]: \u001b[0mInference done 788/1199. Dataloading: 0.0794 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1389 s/iter. ETA=0:00:57\n",
      "\u001b[32m[09/07 09:45:09 d2.evaluation.evaluator]: \u001b[0mInference done 828/1199. Dataloading: 0.0789 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1383 s/iter. ETA=0:00:51\n",
      "\u001b[32m[09/07 09:45:14 d2.evaluation.evaluator]: \u001b[0mInference done 869/1199. Dataloading: 0.0782 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1376 s/iter. ETA=0:00:45\n",
      "\u001b[32m[09/07 09:45:19 d2.evaluation.evaluator]: \u001b[0mInference done 903/1199. Dataloading: 0.0784 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1381 s/iter. ETA=0:00:40\n",
      "\u001b[32m[09/07 09:45:24 d2.evaluation.evaluator]: \u001b[0mInference done 945/1199. Dataloading: 0.0778 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:00:34\n",
      "\u001b[32m[09/07 09:45:29 d2.evaluation.evaluator]: \u001b[0mInference done 979/1199. Dataloading: 0.0783 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:00:30\n",
      "\u001b[32m[09/07 09:45:34 d2.evaluation.evaluator]: \u001b[0mInference done 1018/1199. Dataloading: 0.0782 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1375 s/iter. ETA=0:00:24\n",
      "\u001b[32m[09/07 09:45:40 d2.evaluation.evaluator]: \u001b[0mInference done 1050/1199. Dataloading: 0.0789 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1382 s/iter. ETA=0:00:20\n",
      "\u001b[32m[09/07 09:45:45 d2.evaluation.evaluator]: \u001b[0mInference done 1085/1199. Dataloading: 0.0788 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1384 s/iter. ETA=0:00:15\n",
      "\u001b[32m[09/07 09:45:51 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.0792 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1388 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 09:45:56 d2.evaluation.evaluator]: \u001b[0mInference done 1162/1199. Dataloading: 0.0791 s/iter. Inference: 0.0592 s/iter. Eval: 0.0005 s/iter. Total: 0.1388 s/iter. ETA=0:00:05\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:45.159960 (0.138325 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.059227 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:46:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:46:02 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:46:03 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:46:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:46:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:46:03 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:46:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:46:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:46:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0341 s/iter. Inference: 0.0506 s/iter. Eval: 0.0004 s/iter. Total: 0.0850 s/iter. ETA=0:01:41\n",
      "\u001b[32m[09/07 09:46:09 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0582 s/iter. Inference: 0.0574 s/iter. Eval: 0.0004 s/iter. Total: 0.1161 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 09:46:14 d2.evaluation.evaluator]: \u001b[0mInference done 86/1199. Dataloading: 0.0744 s/iter. Inference: 0.0576 s/iter. Eval: 0.0005 s/iter. Total: 0.1325 s/iter. ETA=0:02:27\n",
      "\u001b[32m[09/07 09:46:19 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0775 s/iter. Inference: 0.0578 s/iter. Eval: 0.0004 s/iter. Total: 0.1358 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/07 09:46:25 d2.evaluation.evaluator]: \u001b[0mInference done 163/1199. Dataloading: 0.0746 s/iter. Inference: 0.0571 s/iter. Eval: 0.0004 s/iter. Total: 0.1322 s/iter. ETA=0:02:16\n",
      "\u001b[32m[09/07 09:46:30 d2.evaluation.evaluator]: \u001b[0mInference done 203/1199. Dataloading: 0.0739 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1314 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 09:46:35 d2.evaluation.evaluator]: \u001b[0mInference done 239/1199. Dataloading: 0.0750 s/iter. Inference: 0.0571 s/iter. Eval: 0.0004 s/iter. Total: 0.1326 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:46:40 d2.evaluation.evaluator]: \u001b[0mInference done 272/1199. Dataloading: 0.0772 s/iter. Inference: 0.0577 s/iter. Eval: 0.0004 s/iter. Total: 0.1354 s/iter. ETA=0:02:05\n",
      "\u001b[32m[09/07 09:46:45 d2.evaluation.evaluator]: \u001b[0mInference done 296/1199. Dataloading: 0.0834 s/iter. Inference: 0.0579 s/iter. Eval: 0.0004 s/iter. Total: 0.1418 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 09:46:50 d2.evaluation.evaluator]: \u001b[0mInference done 334/1199. Dataloading: 0.0822 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1407 s/iter. ETA=0:02:01\n",
      "\u001b[32m[09/07 09:46:55 d2.evaluation.evaluator]: \u001b[0mInference done 371/1199. Dataloading: 0.0814 s/iter. Inference: 0.0583 s/iter. Eval: 0.0005 s/iter. Total: 0.1402 s/iter. ETA=0:01:56\n",
      "\u001b[32m[09/07 09:47:00 d2.evaluation.evaluator]: \u001b[0mInference done 408/1199. Dataloading: 0.0816 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1407 s/iter. ETA=0:01:51\n",
      "\u001b[32m[09/07 09:47:05 d2.evaluation.evaluator]: \u001b[0mInference done 450/1199. Dataloading: 0.0797 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1388 s/iter. ETA=0:01:43\n",
      "\u001b[32m[09/07 09:47:11 d2.evaluation.evaluator]: \u001b[0mInference done 488/1199. Dataloading: 0.0794 s/iter. Inference: 0.0586 s/iter. Eval: 0.0005 s/iter. Total: 0.1385 s/iter. ETA=0:01:38\n",
      "\u001b[32m[09/07 09:47:16 d2.evaluation.evaluator]: \u001b[0mInference done 528/1199. Dataloading: 0.0786 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1378 s/iter. ETA=0:01:32\n",
      "\u001b[32m[09/07 09:47:21 d2.evaluation.evaluator]: \u001b[0mInference done 568/1199. Dataloading: 0.0777 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1370 s/iter. ETA=0:01:26\n",
      "\u001b[32m[09/07 09:47:26 d2.evaluation.evaluator]: \u001b[0mInference done 607/1199. Dataloading: 0.0770 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1365 s/iter. ETA=0:01:20\n",
      "\u001b[32m[09/07 09:47:32 d2.evaluation.evaluator]: \u001b[0mInference done 645/1199. Dataloading: 0.0783 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1376 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 09:47:37 d2.evaluation.evaluator]: \u001b[0mInference done 681/1199. Dataloading: 0.0784 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1377 s/iter. ETA=0:01:11\n",
      "\u001b[32m[09/07 09:47:42 d2.evaluation.evaluator]: \u001b[0mInference done 722/1199. Dataloading: 0.0774 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1368 s/iter. ETA=0:01:05\n",
      "\u001b[32m[09/07 09:47:47 d2.evaluation.evaluator]: \u001b[0mInference done 752/1199. Dataloading: 0.0795 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1390 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 09:47:53 d2.evaluation.evaluator]: \u001b[0mInference done 791/1199. Dataloading: 0.0792 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1387 s/iter. ETA=0:00:56\n",
      "\u001b[32m[09/07 09:47:58 d2.evaluation.evaluator]: \u001b[0mInference done 835/1199. Dataloading: 0.0782 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1374 s/iter. ETA=0:00:50\n",
      "\u001b[32m[09/07 09:48:03 d2.evaluation.evaluator]: \u001b[0mInference done 873/1199. Dataloading: 0.0780 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1374 s/iter. ETA=0:00:44\n",
      "\u001b[32m[09/07 09:48:08 d2.evaluation.evaluator]: \u001b[0mInference done 910/1199. Dataloading: 0.0780 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1374 s/iter. ETA=0:00:39\n",
      "\u001b[32m[09/07 09:48:13 d2.evaluation.evaluator]: \u001b[0mInference done 954/1199. Dataloading: 0.0770 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1364 s/iter. ETA=0:00:33\n",
      "\u001b[32m[09/07 09:48:18 d2.evaluation.evaluator]: \u001b[0mInference done 984/1199. Dataloading: 0.0779 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:00:29\n",
      "\u001b[32m[09/07 09:48:24 d2.evaluation.evaluator]: \u001b[0mInference done 1019/1199. Dataloading: 0.0786 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1380 s/iter. ETA=0:00:24\n",
      "\u001b[32m[09/07 09:48:29 d2.evaluation.evaluator]: \u001b[0mInference done 1057/1199. Dataloading: 0.0784 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:00:19\n",
      "\u001b[32m[09/07 09:48:34 d2.evaluation.evaluator]: \u001b[0mInference done 1093/1199. Dataloading: 0.0784 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1380 s/iter. ETA=0:00:14\n",
      "\u001b[32m[09/07 09:48:39 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.0789 s/iter. Inference: 0.0592 s/iter. Eval: 0.0005 s/iter. Total: 0.1386 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 09:48:44 d2.evaluation.evaluator]: \u001b[0mInference done 1163/1199. Dataloading: 0.0787 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1385 s/iter. ETA=0:00:04\n",
      "\u001b[32m[09/07 09:48:48 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:44.780371 (0.138007 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:48:48 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.059165 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:48:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:48:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:48:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:48:48 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:48:49 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001b[32m[09/07 09:48:49 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:48:49 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:48:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:48:49 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:48:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:48:51 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:48:51 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:48:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:48:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:48:51 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:48:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:48:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:48:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0306 s/iter. Inference: 0.0507 s/iter. Eval: 0.0004 s/iter. Total: 0.0818 s/iter. ETA=0:01:37\n",
      "\u001b[32m[09/07 09:48:57 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0614 s/iter. Inference: 0.0539 s/iter. Eval: 0.0005 s/iter. Total: 0.1158 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 09:49:02 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0807 s/iter. Inference: 0.0537 s/iter. Eval: 0.0005 s/iter. Total: 0.1349 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/07 09:49:07 d2.evaluation.evaluator]: \u001b[0mInference done 120/1199. Dataloading: 0.0824 s/iter. Inference: 0.0549 s/iter. Eval: 0.0005 s/iter. Total: 0.1378 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/07 09:49:13 d2.evaluation.evaluator]: \u001b[0mInference done 159/1199. Dataloading: 0.0797 s/iter. Inference: 0.0576 s/iter. Eval: 0.0005 s/iter. Total: 0.1378 s/iter. ETA=0:02:23\n",
      "\u001b[32m[09/07 09:49:18 d2.evaluation.evaluator]: \u001b[0mInference done 197/1199. Dataloading: 0.0788 s/iter. Inference: 0.0579 s/iter. Eval: 0.0005 s/iter. Total: 0.1373 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/07 09:49:23 d2.evaluation.evaluator]: \u001b[0mInference done 233/1199. Dataloading: 0.0793 s/iter. Inference: 0.0581 s/iter. Eval: 0.0005 s/iter. Total: 0.1380 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 09:49:28 d2.evaluation.evaluator]: \u001b[0mInference done 267/1199. Dataloading: 0.0804 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 09:49:33 d2.evaluation.evaluator]: \u001b[0mInference done 288/1199. Dataloading: 0.0879 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1469 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 09:49:38 d2.evaluation.evaluator]: \u001b[0mInference done 325/1199. Dataloading: 0.0864 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1459 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:49:43 d2.evaluation.evaluator]: \u001b[0mInference done 362/1199. Dataloading: 0.0851 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1452 s/iter. ETA=0:02:01\n",
      "\u001b[32m[09/07 09:49:48 d2.evaluation.evaluator]: \u001b[0mInference done 393/1199. Dataloading: 0.0864 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1466 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/07 09:49:54 d2.evaluation.evaluator]: \u001b[0mInference done 433/1199. Dataloading: 0.0844 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1448 s/iter. ETA=0:01:50\n",
      "\u001b[32m[09/07 09:49:59 d2.evaluation.evaluator]: \u001b[0mInference done 473/1199. Dataloading: 0.0836 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1436 s/iter. ETA=0:01:44\n",
      "\u001b[32m[09/07 09:50:04 d2.evaluation.evaluator]: \u001b[0mInference done 516/1199. Dataloading: 0.0816 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1416 s/iter. ETA=0:01:36\n",
      "\u001b[32m[09/07 09:50:09 d2.evaluation.evaluator]: \u001b[0mInference done 554/1199. Dataloading: 0.0813 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1412 s/iter. ETA=0:01:31\n",
      "\u001b[32m[09/07 09:50:14 d2.evaluation.evaluator]: \u001b[0mInference done 594/1199. Dataloading: 0.0803 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1402 s/iter. ETA=0:01:24\n",
      "\u001b[32m[09/07 09:50:19 d2.evaluation.evaluator]: \u001b[0mInference done 634/1199. Dataloading: 0.0795 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:01:18\n",
      "\u001b[32m[09/07 09:50:24 d2.evaluation.evaluator]: \u001b[0mInference done 670/1199. Dataloading: 0.0794 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:01:13\n",
      "\u001b[32m[09/07 09:50:29 d2.evaluation.evaluator]: \u001b[0mInference done 704/1199. Dataloading: 0.0797 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:01:09\n",
      "\u001b[32m[09/07 09:50:34 d2.evaluation.evaluator]: \u001b[0mInference done 744/1199. Dataloading: 0.0791 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:01:03\n",
      "\u001b[32m[09/07 09:50:39 d2.evaluation.evaluator]: \u001b[0mInference done 770/1199. Dataloading: 0.0810 s/iter. Inference: 0.0592 s/iter. Eval: 0.0005 s/iter. Total: 0.1408 s/iter. ETA=0:01:00\n",
      "\u001b[32m[09/07 09:50:44 d2.evaluation.evaluator]: \u001b[0mInference done 813/1199. Dataloading: 0.0800 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:00:53\n",
      "\u001b[32m[09/07 09:50:50 d2.evaluation.evaluator]: \u001b[0mInference done 855/1199. Dataloading: 0.0789 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:00:47\n",
      "\u001b[32m[09/07 09:50:55 d2.evaluation.evaluator]: \u001b[0mInference done 893/1199. Dataloading: 0.0788 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:00:42\n",
      "\u001b[32m[09/07 09:51:00 d2.evaluation.evaluator]: \u001b[0mInference done 931/1199. Dataloading: 0.0789 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1386 s/iter. ETA=0:00:37\n",
      "\u001b[32m[09/07 09:51:05 d2.evaluation.evaluator]: \u001b[0mInference done 963/1199. Dataloading: 0.0797 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 09:51:10 d2.evaluation.evaluator]: \u001b[0mInference done 1002/1199. Dataloading: 0.0795 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1389 s/iter. ETA=0:00:27\n",
      "\u001b[32m[09/07 09:51:15 d2.evaluation.evaluator]: \u001b[0mInference done 1038/1199. Dataloading: 0.0799 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:00:22\n",
      "\u001b[32m[09/07 09:51:21 d2.evaluation.evaluator]: \u001b[0mInference done 1075/1199. Dataloading: 0.0800 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:00:17\n",
      "\u001b[32m[09/07 09:51:26 d2.evaluation.evaluator]: \u001b[0mInference done 1115/1199. Dataloading: 0.0793 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1388 s/iter. ETA=0:00:11\n",
      "\u001b[32m[09/07 09:51:31 d2.evaluation.evaluator]: \u001b[0mInference done 1145/1199. Dataloading: 0.0802 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:00:07\n",
      "\u001b[32m[09/07 09:51:36 d2.evaluation.evaluator]: \u001b[0mInference done 1186/1199. Dataloading: 0.0796 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:00:01\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:46.112723 (0.139123 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.058936 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:51:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:51:40 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:51:40 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:51:40 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:51:40 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:51:40 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:51:40 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:51:40 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:51:41 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0431 s/iter. Inference: 0.0569 s/iter. Eval: 0.0004 s/iter. Total: 0.1004 s/iter. ETA=0:01:59\n",
      "\u001b[32m[09/07 09:51:46 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0589 s/iter. Inference: 0.0559 s/iter. Eval: 0.0004 s/iter. Total: 0.1154 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 09:51:52 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0783 s/iter. Inference: 0.0550 s/iter. Eval: 0.0004 s/iter. Total: 0.1338 s/iter. ETA=0:02:29\n",
      "\u001b[32m[09/07 09:51:57 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0780 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1360 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/07 09:52:02 d2.evaluation.evaluator]: \u001b[0mInference done 160/1199. Dataloading: 0.0754 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1343 s/iter. ETA=0:02:19\n",
      "\u001b[32m[09/07 09:52:07 d2.evaluation.evaluator]: \u001b[0mInference done 199/1199. Dataloading: 0.0743 s/iter. Inference: 0.0582 s/iter. Eval: 0.0004 s/iter. Total: 0.1331 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 09:52:12 d2.evaluation.evaluator]: \u001b[0mInference done 229/1199. Dataloading: 0.0784 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1385 s/iter. ETA=0:02:14\n",
      "\u001b[32m[09/07 09:52:17 d2.evaluation.evaluator]: \u001b[0mInference done 266/1199. Dataloading: 0.0782 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1381 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 09:52:22 d2.evaluation.evaluator]: \u001b[0mInference done 286/1199. Dataloading: 0.0857 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1464 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 09:52:27 d2.evaluation.evaluator]: \u001b[0mInference done 322/1199. Dataloading: 0.0853 s/iter. Inference: 0.0599 s/iter. Eval: 0.0004 s/iter. Total: 0.1457 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:52:32 d2.evaluation.evaluator]: \u001b[0mInference done 360/1199. Dataloading: 0.0842 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1446 s/iter. ETA=0:02:01\n",
      "\u001b[32m[09/07 09:52:37 d2.evaluation.evaluator]: \u001b[0mInference done 392/1199. Dataloading: 0.0852 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1460 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/07 09:52:42 d2.evaluation.evaluator]: \u001b[0mInference done 433/1199. Dataloading: 0.0832 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1438 s/iter. ETA=0:01:50\n",
      "\u001b[32m[09/07 09:52:48 d2.evaluation.evaluator]: \u001b[0mInference done 473/1199. Dataloading: 0.0824 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1426 s/iter. ETA=0:01:43\n",
      "\u001b[32m[09/07 09:52:53 d2.evaluation.evaluator]: \u001b[0mInference done 510/1199. Dataloading: 0.0814 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1422 s/iter. ETA=0:01:37\n",
      "\u001b[32m[09/07 09:52:58 d2.evaluation.evaluator]: \u001b[0mInference done 546/1199. Dataloading: 0.0820 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1421 s/iter. ETA=0:01:32\n",
      "\u001b[32m[09/07 09:53:03 d2.evaluation.evaluator]: \u001b[0mInference done 586/1199. Dataloading: 0.0818 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1416 s/iter. ETA=0:01:26\n",
      "\u001b[32m[09/07 09:53:08 d2.evaluation.evaluator]: \u001b[0mInference done 626/1199. Dataloading: 0.0811 s/iter. Inference: 0.0592 s/iter. Eval: 0.0005 s/iter. Total: 0.1408 s/iter. ETA=0:01:20\n",
      "\u001b[32m[09/07 09:53:13 d2.evaluation.evaluator]: \u001b[0mInference done 659/1199. Dataloading: 0.0821 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1417 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 09:53:18 d2.evaluation.evaluator]: \u001b[0mInference done 696/1199. Dataloading: 0.0821 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1413 s/iter. ETA=0:01:11\n",
      "\u001b[32m[09/07 09:53:24 d2.evaluation.evaluator]: \u001b[0mInference done 736/1199. Dataloading: 0.0818 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1408 s/iter. ETA=0:01:05\n",
      "\u001b[32m[09/07 09:53:29 d2.evaluation.evaluator]: \u001b[0mInference done 764/1199. Dataloading: 0.0836 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1427 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 09:53:34 d2.evaluation.evaluator]: \u001b[0mInference done 806/1199. Dataloading: 0.0827 s/iter. Inference: 0.0583 s/iter. Eval: 0.0005 s/iter. Total: 0.1416 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/07 09:53:39 d2.evaluation.evaluator]: \u001b[0mInference done 850/1199. Dataloading: 0.0813 s/iter. Inference: 0.0583 s/iter. Eval: 0.0005 s/iter. Total: 0.1401 s/iter. ETA=0:00:48\n",
      "\u001b[32m[09/07 09:53:44 d2.evaluation.evaluator]: \u001b[0mInference done 887/1199. Dataloading: 0.0811 s/iter. Inference: 0.0583 s/iter. Eval: 0.0004 s/iter. Total: 0.1400 s/iter. ETA=0:00:43\n",
      "\u001b[32m[09/07 09:53:49 d2.evaluation.evaluator]: \u001b[0mInference done 924/1199. Dataloading: 0.0810 s/iter. Inference: 0.0583 s/iter. Eval: 0.0004 s/iter. Total: 0.1398 s/iter. ETA=0:00:38\n",
      "\u001b[32m[09/07 09:53:55 d2.evaluation.evaluator]: \u001b[0mInference done 963/1199. Dataloading: 0.0813 s/iter. Inference: 0.0582 s/iter. Eval: 0.0004 s/iter. Total: 0.1400 s/iter. ETA=0:00:33\n",
      "\u001b[32m[09/07 09:54:00 d2.evaluation.evaluator]: \u001b[0mInference done 1003/1199. Dataloading: 0.0809 s/iter. Inference: 0.0581 s/iter. Eval: 0.0004 s/iter. Total: 0.1396 s/iter. ETA=0:00:27\n",
      "\u001b[32m[09/07 09:54:05 d2.evaluation.evaluator]: \u001b[0mInference done 1038/1199. Dataloading: 0.0812 s/iter. Inference: 0.0581 s/iter. Eval: 0.0004 s/iter. Total: 0.1398 s/iter. ETA=0:00:22\n",
      "\u001b[32m[09/07 09:54:10 d2.evaluation.evaluator]: \u001b[0mInference done 1075/1199. Dataloading: 0.0812 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1398 s/iter. ETA=0:00:17\n",
      "\u001b[32m[09/07 09:54:15 d2.evaluation.evaluator]: \u001b[0mInference done 1114/1199. Dataloading: 0.0809 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1394 s/iter. ETA=0:00:11\n",
      "\u001b[32m[09/07 09:54:20 d2.evaluation.evaluator]: \u001b[0mInference done 1145/1199. Dataloading: 0.0815 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1400 s/iter. ETA=0:00:07\n",
      "\u001b[32m[09/07 09:54:25 d2.evaluation.evaluator]: \u001b[0mInference done 1185/1199. Dataloading: 0.0810 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1395 s/iter. ETA=0:00:01\n",
      "\u001b[32m[09/07 09:54:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:46.672108 (0.139591 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:54:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:09 (0.057946 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:54:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:54:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:54:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:54:28 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:54:28 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 09:54:28 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:54:28 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:54:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:54:28 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:54:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:54:30 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:54:30 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:54:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:54:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:54:30 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:54:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:54:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:54:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0434 s/iter. Inference: 0.0514 s/iter. Eval: 0.0005 s/iter. Total: 0.0952 s/iter. ETA=0:01:53\n",
      "\u001b[32m[09/07 09:54:37 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0607 s/iter. Inference: 0.0552 s/iter. Eval: 0.0004 s/iter. Total: 0.1164 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 09:54:42 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0783 s/iter. Inference: 0.0554 s/iter. Eval: 0.0004 s/iter. Total: 0.1342 s/iter. ETA=0:02:29\n",
      "\u001b[32m[09/07 09:54:47 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0786 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1361 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/07 09:54:52 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0748 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1333 s/iter. ETA=0:02:18\n",
      "\u001b[32m[09/07 09:54:57 d2.evaluation.evaluator]: \u001b[0mInference done 201/1199. Dataloading: 0.0740 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1325 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 09:55:02 d2.evaluation.evaluator]: \u001b[0mInference done 237/1199. Dataloading: 0.0753 s/iter. Inference: 0.0581 s/iter. Eval: 0.0004 s/iter. Total: 0.1339 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 09:55:07 d2.evaluation.evaluator]: \u001b[0mInference done 272/1199. Dataloading: 0.0771 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1362 s/iter. ETA=0:02:06\n",
      "\u001b[32m[09/07 09:55:12 d2.evaluation.evaluator]: \u001b[0mInference done 295/1199. Dataloading: 0.0833 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1427 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 09:55:17 d2.evaluation.evaluator]: \u001b[0mInference done 332/1199. Dataloading: 0.0820 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1420 s/iter. ETA=0:02:03\n",
      "\u001b[32m[09/07 09:55:22 d2.evaluation.evaluator]: \u001b[0mInference done 368/1199. Dataloading: 0.0815 s/iter. Inference: 0.0597 s/iter. Eval: 0.0004 s/iter. Total: 0.1417 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/07 09:55:28 d2.evaluation.evaluator]: \u001b[0mInference done 403/1199. Dataloading: 0.0817 s/iter. Inference: 0.0597 s/iter. Eval: 0.0004 s/iter. Total: 0.1419 s/iter. ETA=0:01:52\n",
      "\u001b[32m[09/07 09:55:33 d2.evaluation.evaluator]: \u001b[0mInference done 443/1199. Dataloading: 0.0800 s/iter. Inference: 0.0601 s/iter. Eval: 0.0004 s/iter. Total: 0.1406 s/iter. ETA=0:01:46\n",
      "\u001b[32m[09/07 09:55:38 d2.evaluation.evaluator]: \u001b[0mInference done 483/1199. Dataloading: 0.0790 s/iter. Inference: 0.0597 s/iter. Eval: 0.0004 s/iter. Total: 0.1393 s/iter. ETA=0:01:39\n",
      "\u001b[32m[09/07 09:55:43 d2.evaluation.evaluator]: \u001b[0mInference done 522/1199. Dataloading: 0.0785 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1385 s/iter. ETA=0:01:33\n",
      "\u001b[32m[09/07 09:55:48 d2.evaluation.evaluator]: \u001b[0mInference done 560/1199. Dataloading: 0.0782 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1381 s/iter. ETA=0:01:28\n",
      "\u001b[32m[09/07 09:55:53 d2.evaluation.evaluator]: \u001b[0mInference done 597/1199. Dataloading: 0.0776 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1380 s/iter. ETA=0:01:23\n",
      "\u001b[32m[09/07 09:55:58 d2.evaluation.evaluator]: \u001b[0mInference done 638/1199. Dataloading: 0.0771 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1372 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 09:56:03 d2.evaluation.evaluator]: \u001b[0mInference done 673/1199. Dataloading: 0.0775 s/iter. Inference: 0.0596 s/iter. Eval: 0.0004 s/iter. Total: 0.1376 s/iter. ETA=0:01:12\n",
      "\u001b[32m[09/07 09:56:08 d2.evaluation.evaluator]: \u001b[0mInference done 708/1199. Dataloading: 0.0779 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1379 s/iter. ETA=0:01:07\n",
      "\u001b[32m[09/07 09:56:14 d2.evaluation.evaluator]: \u001b[0mInference done 747/1199. Dataloading: 0.0784 s/iter. Inference: 0.0596 s/iter. Eval: 0.0004 s/iter. Total: 0.1385 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 09:56:19 d2.evaluation.evaluator]: \u001b[0mInference done 780/1199. Dataloading: 0.0791 s/iter. Inference: 0.0596 s/iter. Eval: 0.0004 s/iter. Total: 0.1392 s/iter. ETA=0:00:58\n",
      "\u001b[32m[09/07 09:56:24 d2.evaluation.evaluator]: \u001b[0mInference done 824/1199. Dataloading: 0.0782 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1381 s/iter. ETA=0:00:51\n",
      "\u001b[32m[09/07 09:56:29 d2.evaluation.evaluator]: \u001b[0mInference done 867/1199. Dataloading: 0.0772 s/iter. Inference: 0.0593 s/iter. Eval: 0.0004 s/iter. Total: 0.1370 s/iter. ETA=0:00:45\n",
      "\u001b[32m[09/07 09:56:34 d2.evaluation.evaluator]: \u001b[0mInference done 905/1199. Dataloading: 0.0770 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1368 s/iter. ETA=0:00:40\n",
      "\u001b[32m[09/07 09:56:39 d2.evaluation.evaluator]: \u001b[0mInference done 947/1199. Dataloading: 0.0764 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1361 s/iter. ETA=0:00:34\n",
      "\u001b[32m[09/07 09:56:44 d2.evaluation.evaluator]: \u001b[0mInference done 981/1199. Dataloading: 0.0772 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1367 s/iter. ETA=0:00:29\n",
      "\u001b[32m[09/07 09:56:50 d2.evaluation.evaluator]: \u001b[0mInference done 1019/1199. Dataloading: 0.0777 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1371 s/iter. ETA=0:00:24\n",
      "\u001b[32m[09/07 09:56:55 d2.evaluation.evaluator]: \u001b[0mInference done 1059/1199. Dataloading: 0.0773 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1368 s/iter. ETA=0:00:19\n",
      "\u001b[32m[09/07 09:57:01 d2.evaluation.evaluator]: \u001b[0mInference done 1095/1199. Dataloading: 0.0776 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1372 s/iter. ETA=0:00:14\n",
      "\u001b[32m[09/07 09:57:06 d2.evaluation.evaluator]: \u001b[0mInference done 1132/1199. Dataloading: 0.0777 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:00:09\n",
      "\u001b[32m[09/07 09:57:11 d2.evaluation.evaluator]: \u001b[0mInference done 1170/1199. Dataloading: 0.0776 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1371 s/iter. ETA=0:00:03\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:43.186512 (0.136672 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.058890 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 09:57:14 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 09:57:16 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 09:57:16 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 09:57:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 09:57:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 09:57:16 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 09:57:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 09:57:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 09:57:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0549 s/iter. Inference: 0.0498 s/iter. Eval: 0.0004 s/iter. Total: 0.1051 s/iter. ETA=0:02:04\n",
      "\u001b[32m[09/07 09:57:23 d2.evaluation.evaluator]: \u001b[0mInference done 51/1199. Dataloading: 0.0673 s/iter. Inference: 0.0561 s/iter. Eval: 0.0004 s/iter. Total: 0.1239 s/iter. ETA=0:02:22\n",
      "\u001b[32m[09/07 09:57:29 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0832 s/iter. Inference: 0.0560 s/iter. Eval: 0.0004 s/iter. Total: 0.1397 s/iter. ETA=0:02:35\n",
      "\u001b[32m[09/07 09:57:34 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0812 s/iter. Inference: 0.0578 s/iter. Eval: 0.0004 s/iter. Total: 0.1395 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/07 09:57:39 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0776 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1365 s/iter. ETA=0:02:21\n",
      "\u001b[32m[09/07 09:57:44 d2.evaluation.evaluator]: \u001b[0mInference done 201/1199. Dataloading: 0.0757 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1347 s/iter. ETA=0:02:14\n",
      "\u001b[32m[09/07 09:57:49 d2.evaluation.evaluator]: \u001b[0mInference done 239/1199. Dataloading: 0.0761 s/iter. Inference: 0.0582 s/iter. Eval: 0.0004 s/iter. Total: 0.1348 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 09:57:54 d2.evaluation.evaluator]: \u001b[0mInference done 272/1199. Dataloading: 0.0788 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 09:57:59 d2.evaluation.evaluator]: \u001b[0mInference done 295/1199. Dataloading: 0.0850 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1442 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 09:58:04 d2.evaluation.evaluator]: \u001b[0mInference done 333/1199. Dataloading: 0.0837 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1430 s/iter. ETA=0:02:03\n",
      "\u001b[32m[09/07 09:58:09 d2.evaluation.evaluator]: \u001b[0mInference done 371/1199. Dataloading: 0.0825 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1420 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/07 09:58:14 d2.evaluation.evaluator]: \u001b[0mInference done 406/1199. Dataloading: 0.0827 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1422 s/iter. ETA=0:01:52\n",
      "\u001b[32m[09/07 09:58:20 d2.evaluation.evaluator]: \u001b[0mInference done 448/1199. Dataloading: 0.0813 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1407 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 09:58:25 d2.evaluation.evaluator]: \u001b[0mInference done 487/1199. Dataloading: 0.0802 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1397 s/iter. ETA=0:01:39\n",
      "\u001b[32m[09/07 09:58:30 d2.evaluation.evaluator]: \u001b[0mInference done 526/1199. Dataloading: 0.0799 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1389 s/iter. ETA=0:01:33\n",
      "\u001b[32m[09/07 09:58:35 d2.evaluation.evaluator]: \u001b[0mInference done 566/1199. Dataloading: 0.0790 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1380 s/iter. ETA=0:01:27\n",
      "\u001b[32m[09/07 09:58:40 d2.evaluation.evaluator]: \u001b[0mInference done 604/1199. Dataloading: 0.0787 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:01:21\n",
      "\u001b[32m[09/07 09:58:45 d2.evaluation.evaluator]: \u001b[0mInference done 644/1199. Dataloading: 0.0781 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1372 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 09:58:51 d2.evaluation.evaluator]: \u001b[0mInference done 675/1199. Dataloading: 0.0800 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1390 s/iter. ETA=0:01:12\n",
      "\u001b[32m[09/07 09:58:56 d2.evaluation.evaluator]: \u001b[0mInference done 716/1199. Dataloading: 0.0790 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1381 s/iter. ETA=0:01:06\n",
      "\u001b[32m[09/07 09:59:01 d2.evaluation.evaluator]: \u001b[0mInference done 748/1199. Dataloading: 0.0798 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1389 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 09:59:06 d2.evaluation.evaluator]: \u001b[0mInference done 779/1199. Dataloading: 0.0807 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1399 s/iter. ETA=0:00:58\n",
      "\u001b[32m[09/07 09:59:11 d2.evaluation.evaluator]: \u001b[0mInference done 822/1199. Dataloading: 0.0798 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1387 s/iter. ETA=0:00:52\n",
      "\u001b[32m[09/07 09:59:16 d2.evaluation.evaluator]: \u001b[0mInference done 863/1199. Dataloading: 0.0791 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1380 s/iter. ETA=0:00:46\n",
      "\u001b[32m[09/07 09:59:21 d2.evaluation.evaluator]: \u001b[0mInference done 899/1199. Dataloading: 0.0792 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1381 s/iter. ETA=0:00:41\n",
      "\u001b[32m[09/07 09:59:26 d2.evaluation.evaluator]: \u001b[0mInference done 942/1199. Dataloading: 0.0785 s/iter. Inference: 0.0581 s/iter. Eval: 0.0004 s/iter. Total: 0.1371 s/iter. ETA=0:00:35\n",
      "\u001b[32m[09/07 09:59:31 d2.evaluation.evaluator]: \u001b[0mInference done 976/1199. Dataloading: 0.0789 s/iter. Inference: 0.0581 s/iter. Eval: 0.0004 s/iter. Total: 0.1375 s/iter. ETA=0:00:30\n",
      "\u001b[32m[09/07 09:59:36 d2.evaluation.evaluator]: \u001b[0mInference done 1015/1199. Dataloading: 0.0786 s/iter. Inference: 0.0581 s/iter. Eval: 0.0004 s/iter. Total: 0.1372 s/iter. ETA=0:00:25\n",
      "\u001b[32m[09/07 09:59:41 d2.evaluation.evaluator]: \u001b[0mInference done 1048/1199. Dataloading: 0.0793 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:00:20\n",
      "\u001b[32m[09/07 09:59:46 d2.evaluation.evaluator]: \u001b[0mInference done 1086/1199. Dataloading: 0.0792 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1377 s/iter. ETA=0:00:15\n",
      "\u001b[32m[09/07 09:59:52 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.0795 s/iter. Inference: 0.0579 s/iter. Eval: 0.0004 s/iter. Total: 0.1379 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 09:59:57 d2.evaluation.evaluator]: \u001b[0mInference done 1162/1199. Dataloading: 0.0796 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1380 s/iter. ETA=0:00:05\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:44.329033 (0.137629 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:09 (0.057960 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.09 seconds.\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:00:02 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:00:04 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:00:04 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:00:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:00:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:00:04 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:00:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:00:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:00:06 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0394 s/iter. Inference: 0.0622 s/iter. Eval: 0.0004 s/iter. Total: 0.1020 s/iter. ETA=0:02:01\n",
      "\u001b[32m[09/07 10:00:11 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0613 s/iter. Inference: 0.0563 s/iter. Eval: 0.0004 s/iter. Total: 0.1181 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 10:00:16 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0802 s/iter. Inference: 0.0549 s/iter. Eval: 0.0004 s/iter. Total: 0.1355 s/iter. ETA=0:02:31\n",
      "\u001b[32m[09/07 10:00:21 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0814 s/iter. Inference: 0.0554 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/07 10:00:26 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0783 s/iter. Inference: 0.0556 s/iter. Eval: 0.0004 s/iter. Total: 0.1345 s/iter. ETA=0:02:19\n",
      "\u001b[32m[09/07 10:00:31 d2.evaluation.evaluator]: \u001b[0mInference done 199/1199. Dataloading: 0.0759 s/iter. Inference: 0.0577 s/iter. Eval: 0.0004 s/iter. Total: 0.1341 s/iter. ETA=0:02:14\n",
      "\u001b[32m[09/07 10:00:36 d2.evaluation.evaluator]: \u001b[0mInference done 233/1199. Dataloading: 0.0774 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1363 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 10:00:41 d2.evaluation.evaluator]: \u001b[0mInference done 267/1199. Dataloading: 0.0788 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1380 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 10:00:46 d2.evaluation.evaluator]: \u001b[0mInference done 288/1199. Dataloading: 0.0858 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1455 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 10:00:51 d2.evaluation.evaluator]: \u001b[0mInference done 324/1199. Dataloading: 0.0849 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1450 s/iter. ETA=0:02:06\n",
      "\u001b[32m[09/07 10:00:56 d2.evaluation.evaluator]: \u001b[0mInference done 362/1199. Dataloading: 0.0840 s/iter. Inference: 0.0597 s/iter. Eval: 0.0004 s/iter. Total: 0.1442 s/iter. ETA=0:02:00\n",
      "\u001b[32m[09/07 10:01:01 d2.evaluation.evaluator]: \u001b[0mInference done 394/1199. Dataloading: 0.0847 s/iter. Inference: 0.0601 s/iter. Eval: 0.0004 s/iter. Total: 0.1453 s/iter. ETA=0:01:56\n",
      "\u001b[32m[09/07 10:01:07 d2.evaluation.evaluator]: \u001b[0mInference done 434/1199. Dataloading: 0.0830 s/iter. Inference: 0.0600 s/iter. Eval: 0.0004 s/iter. Total: 0.1435 s/iter. ETA=0:01:49\n",
      "\u001b[32m[09/07 10:01:12 d2.evaluation.evaluator]: \u001b[0mInference done 473/1199. Dataloading: 0.0823 s/iter. Inference: 0.0597 s/iter. Eval: 0.0004 s/iter. Total: 0.1426 s/iter. ETA=0:01:43\n",
      "\u001b[32m[09/07 10:01:17 d2.evaluation.evaluator]: \u001b[0mInference done 514/1199. Dataloading: 0.0810 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1409 s/iter. ETA=0:01:36\n",
      "\u001b[32m[09/07 10:01:22 d2.evaluation.evaluator]: \u001b[0mInference done 551/1199. Dataloading: 0.0804 s/iter. Inference: 0.0596 s/iter. Eval: 0.0004 s/iter. Total: 0.1406 s/iter. ETA=0:01:31\n",
      "\u001b[32m[09/07 10:01:27 d2.evaluation.evaluator]: \u001b[0mInference done 590/1199. Dataloading: 0.0796 s/iter. Inference: 0.0598 s/iter. Eval: 0.0004 s/iter. Total: 0.1400 s/iter. ETA=0:01:25\n",
      "\u001b[32m[09/07 10:01:32 d2.evaluation.evaluator]: \u001b[0mInference done 632/1199. Dataloading: 0.0789 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1389 s/iter. ETA=0:01:18\n",
      "\u001b[32m[09/07 10:01:37 d2.evaluation.evaluator]: \u001b[0mInference done 665/1199. Dataloading: 0.0799 s/iter. Inference: 0.0593 s/iter. Eval: 0.0004 s/iter. Total: 0.1397 s/iter. ETA=0:01:14\n",
      "\u001b[32m[09/07 10:01:42 d2.evaluation.evaluator]: \u001b[0mInference done 702/1199. Dataloading: 0.0798 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1396 s/iter. ETA=0:01:09\n",
      "\u001b[32m[09/07 10:01:47 d2.evaluation.evaluator]: \u001b[0mInference done 740/1199. Dataloading: 0.0796 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1395 s/iter. ETA=0:01:04\n",
      "\u001b[32m[09/07 10:01:53 d2.evaluation.evaluator]: \u001b[0mInference done 766/1199. Dataloading: 0.0815 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1415 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/07 10:01:58 d2.evaluation.evaluator]: \u001b[0mInference done 806/1199. Dataloading: 0.0807 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1407 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/07 10:02:03 d2.evaluation.evaluator]: \u001b[0mInference done 851/1199. Dataloading: 0.0795 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1392 s/iter. ETA=0:00:48\n",
      "\u001b[32m[09/07 10:02:08 d2.evaluation.evaluator]: \u001b[0mInference done 887/1199. Dataloading: 0.0795 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1392 s/iter. ETA=0:00:43\n",
      "\u001b[32m[09/07 10:02:13 d2.evaluation.evaluator]: \u001b[0mInference done 924/1199. Dataloading: 0.0795 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1392 s/iter. ETA=0:00:38\n",
      "\u001b[32m[09/07 10:02:19 d2.evaluation.evaluator]: \u001b[0mInference done 963/1199. Dataloading: 0.0798 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1394 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 10:02:24 d2.evaluation.evaluator]: \u001b[0mInference done 1002/1199. Dataloading: 0.0794 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1393 s/iter. ETA=0:00:27\n",
      "\u001b[32m[09/07 10:02:29 d2.evaluation.evaluator]: \u001b[0mInference done 1038/1199. Dataloading: 0.0796 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1396 s/iter. ETA=0:00:22\n",
      "\u001b[32m[09/07 10:02:34 d2.evaluation.evaluator]: \u001b[0mInference done 1075/1199. Dataloading: 0.0795 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1395 s/iter. ETA=0:00:17\n",
      "\u001b[32m[09/07 10:02:39 d2.evaluation.evaluator]: \u001b[0mInference done 1113/1199. Dataloading: 0.0793 s/iter. Inference: 0.0596 s/iter. Eval: 0.0004 s/iter. Total: 0.1394 s/iter. ETA=0:00:11\n",
      "\u001b[32m[09/07 10:02:44 d2.evaluation.evaluator]: \u001b[0mInference done 1146/1199. Dataloading: 0.0798 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1398 s/iter. ETA=0:00:07\n",
      "\u001b[32m[09/07 10:02:50 d2.evaluation.evaluator]: \u001b[0mInference done 1186/1199. Dataloading: 0.0795 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1394 s/iter. ETA=0:00:01\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:46.616474 (0.139545 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:11 (0.059486 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:02:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:02:54 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:02:54 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:02:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:02:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:02:54 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:02:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:02:54 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:02:55 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0427 s/iter. Inference: 0.0527 s/iter. Eval: 0.0004 s/iter. Total: 0.0958 s/iter. ETA=0:01:53\n",
      "\u001b[32m[09/07 10:03:01 d2.evaluation.evaluator]: \u001b[0mInference done 54/1199. Dataloading: 0.0638 s/iter. Inference: 0.0544 s/iter. Eval: 0.0004 s/iter. Total: 0.1187 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 10:03:06 d2.evaluation.evaluator]: \u001b[0mInference done 86/1199. Dataloading: 0.0781 s/iter. Inference: 0.0552 s/iter. Eval: 0.0004 s/iter. Total: 0.1338 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/07 10:03:11 d2.evaluation.evaluator]: \u001b[0mInference done 119/1199. Dataloading: 0.0816 s/iter. Inference: 0.0569 s/iter. Eval: 0.0004 s/iter. Total: 0.1390 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/07 10:03:16 d2.evaluation.evaluator]: \u001b[0mInference done 159/1199. Dataloading: 0.0779 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1354 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/07 10:03:21 d2.evaluation.evaluator]: \u001b[0mInference done 197/1199. Dataloading: 0.0771 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1351 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 10:03:26 d2.evaluation.evaluator]: \u001b[0mInference done 235/1199. Dataloading: 0.0758 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1348 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 10:03:31 d2.evaluation.evaluator]: \u001b[0mInference done 269/1199. Dataloading: 0.0773 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1366 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 10:03:36 d2.evaluation.evaluator]: \u001b[0mInference done 289/1199. Dataloading: 0.0848 s/iter. Inference: 0.0593 s/iter. Eval: 0.0004 s/iter. Total: 0.1446 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 10:03:41 d2.evaluation.evaluator]: \u001b[0mInference done 325/1199. Dataloading: 0.0839 s/iter. Inference: 0.0597 s/iter. Eval: 0.0004 s/iter. Total: 0.1441 s/iter. ETA=0:02:05\n",
      "\u001b[32m[09/07 10:03:46 d2.evaluation.evaluator]: \u001b[0mInference done 366/1199. Dataloading: 0.0818 s/iter. Inference: 0.0600 s/iter. Eval: 0.0004 s/iter. Total: 0.1423 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/07 10:03:51 d2.evaluation.evaluator]: \u001b[0mInference done 400/1199. Dataloading: 0.0823 s/iter. Inference: 0.0599 s/iter. Eval: 0.0004 s/iter. Total: 0.1427 s/iter. ETA=0:01:54\n",
      "\u001b[32m[09/07 10:03:56 d2.evaluation.evaluator]: \u001b[0mInference done 441/1199. Dataloading: 0.0808 s/iter. Inference: 0.0596 s/iter. Eval: 0.0004 s/iter. Total: 0.1410 s/iter. ETA=0:01:46\n",
      "\u001b[32m[09/07 10:04:01 d2.evaluation.evaluator]: \u001b[0mInference done 479/1199. Dataloading: 0.0803 s/iter. Inference: 0.0595 s/iter. Eval: 0.0004 s/iter. Total: 0.1403 s/iter. ETA=0:01:41\n",
      "\u001b[32m[09/07 10:04:06 d2.evaluation.evaluator]: \u001b[0mInference done 518/1199. Dataloading: 0.0795 s/iter. Inference: 0.0593 s/iter. Eval: 0.0004 s/iter. Total: 0.1394 s/iter. ETA=0:01:34\n",
      "\u001b[32m[09/07 10:04:11 d2.evaluation.evaluator]: \u001b[0mInference done 556/1199. Dataloading: 0.0796 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1390 s/iter. ETA=0:01:29\n",
      "\u001b[32m[09/07 10:04:16 d2.evaluation.evaluator]: \u001b[0mInference done 592/1199. Dataloading: 0.0794 s/iter. Inference: 0.0592 s/iter. Eval: 0.0004 s/iter. Total: 0.1390 s/iter. ETA=0:01:24\n",
      "\u001b[32m[09/07 10:04:21 d2.evaluation.evaluator]: \u001b[0mInference done 634/1199. Dataloading: 0.0782 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1378 s/iter. ETA=0:01:17\n",
      "\u001b[32m[09/07 10:04:26 d2.evaluation.evaluator]: \u001b[0mInference done 669/1199. Dataloading: 0.0787 s/iter. Inference: 0.0589 s/iter. Eval: 0.0004 s/iter. Total: 0.1381 s/iter. ETA=0:01:13\n",
      "\u001b[32m[09/07 10:04:32 d2.evaluation.evaluator]: \u001b[0mInference done 704/1199. Dataloading: 0.0792 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1384 s/iter. ETA=0:01:08\n",
      "\u001b[32m[09/07 10:04:37 d2.evaluation.evaluator]: \u001b[0mInference done 744/1199. Dataloading: 0.0788 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1379 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 10:04:42 d2.evaluation.evaluator]: \u001b[0mInference done 770/1199. Dataloading: 0.0806 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1398 s/iter. ETA=0:00:59\n",
      "\u001b[32m[09/07 10:04:47 d2.evaluation.evaluator]: \u001b[0mInference done 813/1199. Dataloading: 0.0793 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1386 s/iter. ETA=0:00:53\n",
      "\u001b[32m[09/07 10:04:52 d2.evaluation.evaluator]: \u001b[0mInference done 856/1199. Dataloading: 0.0784 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1376 s/iter. ETA=0:00:47\n",
      "\u001b[32m[09/07 10:04:57 d2.evaluation.evaluator]: \u001b[0mInference done 895/1199. Dataloading: 0.0782 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:00:41\n",
      "\u001b[32m[09/07 10:05:02 d2.evaluation.evaluator]: \u001b[0mInference done 935/1199. Dataloading: 0.0778 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1369 s/iter. ETA=0:00:36\n",
      "\u001b[32m[09/07 10:05:07 d2.evaluation.evaluator]: \u001b[0mInference done 971/1199. Dataloading: 0.0780 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1372 s/iter. ETA=0:00:31\n",
      "\u001b[32m[09/07 10:05:12 d2.evaluation.evaluator]: \u001b[0mInference done 1011/1199. Dataloading: 0.0778 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1368 s/iter. ETA=0:00:25\n",
      "\u001b[32m[09/07 10:05:18 d2.evaluation.evaluator]: \u001b[0mInference done 1046/1199. Dataloading: 0.0782 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:00:21\n",
      "\u001b[32m[09/07 10:05:23 d2.evaluation.evaluator]: \u001b[0mInference done 1084/1199. Dataloading: 0.0781 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1371 s/iter. ETA=0:00:15\n",
      "\u001b[32m[09/07 10:05:28 d2.evaluation.evaluator]: \u001b[0mInference done 1125/1199. Dataloading: 0.0776 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1366 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 10:05:33 d2.evaluation.evaluator]: \u001b[0mInference done 1155/1199. Dataloading: 0.0782 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1374 s/iter. ETA=0:00:06\n",
      "\u001b[32m[09/07 10:05:38 d2.evaluation.evaluator]: \u001b[0mInference done 1196/1199. Dataloading: 0.0779 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1370 s/iter. ETA=0:00:00\n",
      "\u001b[32m[09/07 10:05:38 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:43.710497 (0.137111 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:05:38 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:09 (0.058614 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:05:38 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:05:38 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:05:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:05:39 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:05:39 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 10:05:39 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:05:39 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:05:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:05:39 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:05:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:05:41 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:05:41 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:05:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:05:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:05:41 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:05:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:05:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:05:43 d2.evaluation.evaluator]: \u001b[0mInference done 17/1199. Dataloading: 0.0620 s/iter. Inference: 0.0563 s/iter. Eval: 0.0004 s/iter. Total: 0.1188 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/07 10:05:48 d2.evaluation.evaluator]: \u001b[0mInference done 61/1199. Dataloading: 0.0608 s/iter. Inference: 0.0537 s/iter. Eval: 0.0004 s/iter. Total: 0.1150 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 10:05:53 d2.evaluation.evaluator]: \u001b[0mInference done 92/1199. Dataloading: 0.0765 s/iter. Inference: 0.0547 s/iter. Eval: 0.0004 s/iter. Total: 0.1317 s/iter. ETA=0:02:25\n",
      "\u001b[32m[09/07 10:05:58 d2.evaluation.evaluator]: \u001b[0mInference done 126/1199. Dataloading: 0.0799 s/iter. Inference: 0.0558 s/iter. Eval: 0.0004 s/iter. Total: 0.1362 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/07 10:06:03 d2.evaluation.evaluator]: \u001b[0mInference done 166/1199. Dataloading: 0.0785 s/iter. Inference: 0.0545 s/iter. Eval: 0.0004 s/iter. Total: 0.1335 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/07 10:06:08 d2.evaluation.evaluator]: \u001b[0mInference done 205/1199. Dataloading: 0.0772 s/iter. Inference: 0.0548 s/iter. Eval: 0.0004 s/iter. Total: 0.1325 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 10:06:14 d2.evaluation.evaluator]: \u001b[0mInference done 241/1199. Dataloading: 0.0797 s/iter. Inference: 0.0552 s/iter. Eval: 0.0004 s/iter. Total: 0.1354 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 10:06:19 d2.evaluation.evaluator]: \u001b[0mInference done 274/1199. Dataloading: 0.0820 s/iter. Inference: 0.0558 s/iter. Eval: 0.0004 s/iter. Total: 0.1382 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 10:06:24 d2.evaluation.evaluator]: \u001b[0mInference done 298/1199. Dataloading: 0.0867 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1445 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 10:06:29 d2.evaluation.evaluator]: \u001b[0mInference done 338/1199. Dataloading: 0.0841 s/iter. Inference: 0.0578 s/iter. Eval: 0.0004 s/iter. Total: 0.1424 s/iter. ETA=0:02:02\n",
      "\u001b[32m[09/07 10:06:34 d2.evaluation.evaluator]: \u001b[0mInference done 374/1199. Dataloading: 0.0839 s/iter. Inference: 0.0579 s/iter. Eval: 0.0004 s/iter. Total: 0.1423 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/07 10:06:39 d2.evaluation.evaluator]: \u001b[0mInference done 408/1199. Dataloading: 0.0838 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1429 s/iter. ETA=0:01:53\n",
      "\u001b[32m[09/07 10:06:44 d2.evaluation.evaluator]: \u001b[0mInference done 450/1199. Dataloading: 0.0817 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1407 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 10:06:49 d2.evaluation.evaluator]: \u001b[0mInference done 489/1199. Dataloading: 0.0807 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1397 s/iter. ETA=0:01:39\n",
      "\u001b[32m[09/07 10:06:54 d2.evaluation.evaluator]: \u001b[0mInference done 527/1199. Dataloading: 0.0802 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1392 s/iter. ETA=0:01:33\n",
      "\u001b[32m[09/07 10:06:59 d2.evaluation.evaluator]: \u001b[0mInference done 568/1199. Dataloading: 0.0791 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1380 s/iter. ETA=0:01:27\n",
      "\u001b[32m[09/07 10:07:04 d2.evaluation.evaluator]: \u001b[0mInference done 606/1199. Dataloading: 0.0787 s/iter. Inference: 0.0584 s/iter. Eval: 0.0004 s/iter. Total: 0.1376 s/iter. ETA=0:01:21\n",
      "\u001b[32m[09/07 10:07:09 d2.evaluation.evaluator]: \u001b[0mInference done 643/1199. Dataloading: 0.0786 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1375 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 10:07:15 d2.evaluation.evaluator]: \u001b[0mInference done 675/1199. Dataloading: 0.0805 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1395 s/iter. ETA=0:01:13\n",
      "\u001b[32m[09/07 10:07:20 d2.evaluation.evaluator]: \u001b[0mInference done 718/1199. Dataloading: 0.0792 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1383 s/iter. ETA=0:01:06\n",
      "\u001b[32m[09/07 10:07:25 d2.evaluation.evaluator]: \u001b[0mInference done 748/1199. Dataloading: 0.0800 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1395 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 10:07:30 d2.evaluation.evaluator]: \u001b[0mInference done 775/1199. Dataloading: 0.0816 s/iter. Inference: 0.0590 s/iter. Eval: 0.0004 s/iter. Total: 0.1411 s/iter. ETA=0:00:59\n",
      "\u001b[32m[09/07 10:07:35 d2.evaluation.evaluator]: \u001b[0mInference done 813/1199. Dataloading: 0.0811 s/iter. Inference: 0.0591 s/iter. Eval: 0.0004 s/iter. Total: 0.1407 s/iter. ETA=0:00:54\n",
      "\u001b[32m[09/07 10:07:41 d2.evaluation.evaluator]: \u001b[0mInference done 850/1199. Dataloading: 0.0806 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1407 s/iter. ETA=0:00:49\n",
      "\u001b[32m[09/07 10:07:46 d2.evaluation.evaluator]: \u001b[0mInference done 884/1199. Dataloading: 0.0807 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1409 s/iter. ETA=0:00:44\n",
      "\u001b[32m[09/07 10:07:51 d2.evaluation.evaluator]: \u001b[0mInference done 919/1199. Dataloading: 0.0812 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1411 s/iter. ETA=0:00:39\n",
      "\u001b[32m[09/07 10:07:56 d2.evaluation.evaluator]: \u001b[0mInference done 959/1199. Dataloading: 0.0805 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1405 s/iter. ETA=0:00:33\n",
      "\u001b[32m[09/07 10:08:01 d2.evaluation.evaluator]: \u001b[0mInference done 989/1199. Dataloading: 0.0812 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1414 s/iter. ETA=0:00:29\n",
      "\u001b[32m[09/07 10:08:06 d2.evaluation.evaluator]: \u001b[0mInference done 1023/1199. Dataloading: 0.0817 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1417 s/iter. ETA=0:00:24\n",
      "\u001b[32m[09/07 10:08:11 d2.evaluation.evaluator]: \u001b[0mInference done 1059/1199. Dataloading: 0.0819 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1418 s/iter. ETA=0:00:19\n",
      "\u001b[32m[09/07 10:08:16 d2.evaluation.evaluator]: \u001b[0mInference done 1093/1199. Dataloading: 0.0819 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1420 s/iter. ETA=0:00:15\n",
      "\u001b[32m[09/07 10:08:21 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.0825 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1426 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 10:08:27 d2.evaluation.evaluator]: \u001b[0mInference done 1162/1199. Dataloading: 0.0824 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1425 s/iter. ETA=0:00:05\n",
      "\u001b[32m[09/07 10:08:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:49.389527 (0.141867 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:08:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:11 (0.059552 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:08:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:08:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:08:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:08:31 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:08:32 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001b[32m[09/07 10:08:32 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:08:32 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:08:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:08:32 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:08:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:08:33 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:08:33 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:08:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:08:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:08:34 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:08:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:08:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:08:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0362 s/iter. Inference: 0.0491 s/iter. Eval: 0.0004 s/iter. Total: 0.0857 s/iter. ETA=0:01:41\n",
      "\u001b[32m[09/07 10:08:40 d2.evaluation.evaluator]: \u001b[0mInference done 52/1199. Dataloading: 0.0623 s/iter. Inference: 0.0557 s/iter. Eval: 0.0004 s/iter. Total: 0.1185 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 10:08:46 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0810 s/iter. Inference: 0.0561 s/iter. Eval: 0.0004 s/iter. Total: 0.1377 s/iter. ETA=0:02:33\n",
      "\u001b[32m[09/07 10:08:51 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0798 s/iter. Inference: 0.0582 s/iter. Eval: 0.0004 s/iter. Total: 0.1385 s/iter. ETA=0:02:29\n",
      "\u001b[32m[09/07 10:08:56 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0780 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1357 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/07 10:09:01 d2.evaluation.evaluator]: \u001b[0mInference done 197/1199. Dataloading: 0.0786 s/iter. Inference: 0.0578 s/iter. Eval: 0.0004 s/iter. Total: 0.1369 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/07 10:09:06 d2.evaluation.evaluator]: \u001b[0mInference done 232/1199. Dataloading: 0.0794 s/iter. Inference: 0.0579 s/iter. Eval: 0.0005 s/iter. Total: 0.1379 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 10:09:11 d2.evaluation.evaluator]: \u001b[0mInference done 267/1199. Dataloading: 0.0803 s/iter. Inference: 0.0586 s/iter. Eval: 0.0005 s/iter. Total: 0.1394 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 10:09:16 d2.evaluation.evaluator]: \u001b[0mInference done 287/1199. Dataloading: 0.0876 s/iter. Inference: 0.0592 s/iter. Eval: 0.0005 s/iter. Total: 0.1473 s/iter. ETA=0:02:14\n",
      "\u001b[32m[09/07 10:09:21 d2.evaluation.evaluator]: \u001b[0mInference done 322/1199. Dataloading: 0.0870 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1469 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 10:09:26 d2.evaluation.evaluator]: \u001b[0mInference done 359/1199. Dataloading: 0.0861 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1457 s/iter. ETA=0:02:02\n",
      "\u001b[32m[09/07 10:09:31 d2.evaluation.evaluator]: \u001b[0mInference done 391/1199. Dataloading: 0.0875 s/iter. Inference: 0.0586 s/iter. Eval: 0.0005 s/iter. Total: 0.1466 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/07 10:09:36 d2.evaluation.evaluator]: \u001b[0mInference done 431/1199. Dataloading: 0.0861 s/iter. Inference: 0.0581 s/iter. Eval: 0.0005 s/iter. Total: 0.1447 s/iter. ETA=0:01:51\n",
      "\u001b[32m[09/07 10:09:41 d2.evaluation.evaluator]: \u001b[0mInference done 471/1199. Dataloading: 0.0847 s/iter. Inference: 0.0579 s/iter. Eval: 0.0005 s/iter. Total: 0.1431 s/iter. ETA=0:01:44\n",
      "\u001b[32m[09/07 10:09:47 d2.evaluation.evaluator]: \u001b[0mInference done 514/1199. Dataloading: 0.0831 s/iter. Inference: 0.0579 s/iter. Eval: 0.0005 s/iter. Total: 0.1415 s/iter. ETA=0:01:36\n",
      "\u001b[32m[09/07 10:09:52 d2.evaluation.evaluator]: \u001b[0mInference done 552/1199. Dataloading: 0.0824 s/iter. Inference: 0.0578 s/iter. Eval: 0.0005 s/iter. Total: 0.1408 s/iter. ETA=0:01:31\n",
      "\u001b[32m[09/07 10:09:57 d2.evaluation.evaluator]: \u001b[0mInference done 592/1199. Dataloading: 0.0816 s/iter. Inference: 0.0578 s/iter. Eval: 0.0005 s/iter. Total: 0.1399 s/iter. ETA=0:01:24\n",
      "\u001b[32m[09/07 10:10:02 d2.evaluation.evaluator]: \u001b[0mInference done 633/1199. Dataloading: 0.0806 s/iter. Inference: 0.0579 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:01:18\n",
      "\u001b[32m[09/07 10:10:07 d2.evaluation.evaluator]: \u001b[0mInference done 666/1199. Dataloading: 0.0812 s/iter. Inference: 0.0580 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:01:14\n",
      "\u001b[32m[09/07 10:10:12 d2.evaluation.evaluator]: \u001b[0mInference done 703/1199. Dataloading: 0.0814 s/iter. Inference: 0.0577 s/iter. Eval: 0.0005 s/iter. Total: 0.1396 s/iter. ETA=0:01:09\n",
      "\u001b[32m[09/07 10:10:17 d2.evaluation.evaluator]: \u001b[0mInference done 741/1199. Dataloading: 0.0809 s/iter. Inference: 0.0577 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:01:03\n",
      "\u001b[32m[09/07 10:10:22 d2.evaluation.evaluator]: \u001b[0mInference done 764/1199. Dataloading: 0.0832 s/iter. Inference: 0.0580 s/iter. Eval: 0.0005 s/iter. Total: 0.1417 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/07 10:10:27 d2.evaluation.evaluator]: \u001b[0mInference done 806/1199. Dataloading: 0.0824 s/iter. Inference: 0.0580 s/iter. Eval: 0.0005 s/iter. Total: 0.1409 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/07 10:10:33 d2.evaluation.evaluator]: \u001b[0mInference done 850/1199. Dataloading: 0.0813 s/iter. Inference: 0.0578 s/iter. Eval: 0.0005 s/iter. Total: 0.1396 s/iter. ETA=0:00:48\n",
      "\u001b[32m[09/07 10:10:38 d2.evaluation.evaluator]: \u001b[0mInference done 889/1199. Dataloading: 0.0809 s/iter. Inference: 0.0578 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:00:43\n",
      "\u001b[32m[09/07 10:10:43 d2.evaluation.evaluator]: \u001b[0mInference done 927/1199. Dataloading: 0.0806 s/iter. Inference: 0.0578 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:00:37\n",
      "\u001b[32m[09/07 10:10:48 d2.evaluation.evaluator]: \u001b[0mInference done 963/1199. Dataloading: 0.0811 s/iter. Inference: 0.0577 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 10:10:53 d2.evaluation.evaluator]: \u001b[0mInference done 1003/1199. Dataloading: 0.0806 s/iter. Inference: 0.0578 s/iter. Eval: 0.0005 s/iter. Total: 0.1389 s/iter. ETA=0:00:27\n",
      "\u001b[32m[09/07 10:10:58 d2.evaluation.evaluator]: \u001b[0mInference done 1038/1199. Dataloading: 0.0808 s/iter. Inference: 0.0577 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:00:22\n",
      "\u001b[32m[09/07 10:11:03 d2.evaluation.evaluator]: \u001b[0mInference done 1075/1199. Dataloading: 0.0810 s/iter. Inference: 0.0576 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:00:17\n",
      "\u001b[32m[09/07 10:11:09 d2.evaluation.evaluator]: \u001b[0mInference done 1113/1199. Dataloading: 0.0808 s/iter. Inference: 0.0577 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:00:11\n",
      "\u001b[32m[09/07 10:11:14 d2.evaluation.evaluator]: \u001b[0mInference done 1144/1199. Dataloading: 0.0812 s/iter. Inference: 0.0580 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:00:07\n",
      "\u001b[32m[09/07 10:11:19 d2.evaluation.evaluator]: \u001b[0mInference done 1184/1199. Dataloading: 0.0808 s/iter. Inference: 0.0580 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:00:02\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:46.381811 (0.139348 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:09 (0.058013 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:11:21 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:11:23 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:11:23 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:11:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:11:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:11:23 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:11:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:11:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:11:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0366 s/iter. Inference: 0.0498 s/iter. Eval: 0.0004 s/iter. Total: 0.0868 s/iter. ETA=0:01:43\n",
      "\u001b[32m[09/07 10:11:30 d2.evaluation.evaluator]: \u001b[0mInference done 50/1199. Dataloading: 0.0673 s/iter. Inference: 0.0566 s/iter. Eval: 0.0005 s/iter. Total: 0.1245 s/iter. ETA=0:02:23\n",
      "\u001b[32m[09/07 10:11:35 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0844 s/iter. Inference: 0.0544 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:02:35\n",
      "\u001b[32m[09/07 10:11:40 d2.evaluation.evaluator]: \u001b[0mInference done 120/1199. Dataloading: 0.0818 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1408 s/iter. ETA=0:02:31\n",
      "\u001b[32m[09/07 10:11:46 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0781 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1374 s/iter. ETA=0:02:22\n",
      "\u001b[32m[09/07 10:11:51 d2.evaluation.evaluator]: \u001b[0mInference done 199/1199. Dataloading: 0.0771 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1364 s/iter. ETA=0:02:16\n",
      "\u001b[32m[09/07 10:11:56 d2.evaluation.evaluator]: \u001b[0mInference done 233/1199. Dataloading: 0.0787 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1384 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 10:12:01 d2.evaluation.evaluator]: \u001b[0mInference done 268/1199. Dataloading: 0.0792 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 10:12:06 d2.evaluation.evaluator]: \u001b[0mInference done 290/1199. Dataloading: 0.0863 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1463 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 10:12:11 d2.evaluation.evaluator]: \u001b[0mInference done 325/1199. Dataloading: 0.0864 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1462 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 10:12:16 d2.evaluation.evaluator]: \u001b[0mInference done 364/1199. Dataloading: 0.0843 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1443 s/iter. ETA=0:02:00\n",
      "\u001b[32m[09/07 10:12:21 d2.evaluation.evaluator]: \u001b[0mInference done 396/1199. Dataloading: 0.0853 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1453 s/iter. ETA=0:01:56\n",
      "\u001b[32m[09/07 10:12:26 d2.evaluation.evaluator]: \u001b[0mInference done 437/1199. Dataloading: 0.0833 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1431 s/iter. ETA=0:01:49\n",
      "\u001b[32m[09/07 10:12:31 d2.evaluation.evaluator]: \u001b[0mInference done 476/1199. Dataloading: 0.0820 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1419 s/iter. ETA=0:01:42\n",
      "\u001b[32m[09/07 10:12:36 d2.evaluation.evaluator]: \u001b[0mInference done 517/1199. Dataloading: 0.0802 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1404 s/iter. ETA=0:01:35\n",
      "\u001b[32m[09/07 10:12:41 d2.evaluation.evaluator]: \u001b[0mInference done 552/1199. Dataloading: 0.0807 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1407 s/iter. ETA=0:01:31\n",
      "\u001b[32m[09/07 10:12:46 d2.evaluation.evaluator]: \u001b[0mInference done 592/1199. Dataloading: 0.0799 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1398 s/iter. ETA=0:01:24\n",
      "\u001b[32m[09/07 10:12:51 d2.evaluation.evaluator]: \u001b[0mInference done 633/1199. Dataloading: 0.0792 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1386 s/iter. ETA=0:01:18\n",
      "\u001b[32m[09/07 10:12:56 d2.evaluation.evaluator]: \u001b[0mInference done 667/1199. Dataloading: 0.0799 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:01:14\n",
      "\u001b[32m[09/07 10:13:01 d2.evaluation.evaluator]: \u001b[0mInference done 703/1199. Dataloading: 0.0799 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1394 s/iter. ETA=0:01:09\n",
      "\u001b[32m[09/07 10:13:06 d2.evaluation.evaluator]: \u001b[0mInference done 742/1199. Dataloading: 0.0793 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1388 s/iter. ETA=0:01:03\n",
      "\u001b[32m[09/07 10:13:12 d2.evaluation.evaluator]: \u001b[0mInference done 764/1199. Dataloading: 0.0821 s/iter. Inference: 0.0592 s/iter. Eval: 0.0005 s/iter. Total: 0.1418 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/07 10:13:17 d2.evaluation.evaluator]: \u001b[0mInference done 806/1199. Dataloading: 0.0812 s/iter. Inference: 0.0592 s/iter. Eval: 0.0005 s/iter. Total: 0.1410 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/07 10:13:22 d2.evaluation.evaluator]: \u001b[0mInference done 851/1199. Dataloading: 0.0797 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1394 s/iter. ETA=0:00:48\n",
      "\u001b[32m[09/07 10:13:27 d2.evaluation.evaluator]: \u001b[0mInference done 889/1199. Dataloading: 0.0798 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:00:43\n",
      "\u001b[32m[09/07 10:13:32 d2.evaluation.evaluator]: \u001b[0mInference done 926/1199. Dataloading: 0.0798 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:00:37\n",
      "\u001b[32m[09/07 10:13:38 d2.evaluation.evaluator]: \u001b[0mInference done 963/1199. Dataloading: 0.0803 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1396 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 10:13:43 d2.evaluation.evaluator]: \u001b[0mInference done 1002/1199. Dataloading: 0.0799 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:00:27\n",
      "\u001b[32m[09/07 10:13:48 d2.evaluation.evaluator]: \u001b[0mInference done 1036/1199. Dataloading: 0.0803 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1395 s/iter. ETA=0:00:22\n",
      "\u001b[32m[09/07 10:13:53 d2.evaluation.evaluator]: \u001b[0mInference done 1073/1199. Dataloading: 0.0803 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1395 s/iter. ETA=0:00:17\n",
      "\u001b[32m[09/07 10:13:58 d2.evaluation.evaluator]: \u001b[0mInference done 1112/1199. Dataloading: 0.0796 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:00:12\n",
      "\u001b[32m[09/07 10:14:03 d2.evaluation.evaluator]: \u001b[0mInference done 1143/1199. Dataloading: 0.0802 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1398 s/iter. ETA=0:00:07\n",
      "\u001b[32m[09/07 10:14:08 d2.evaluation.evaluator]: \u001b[0mInference done 1181/1199. Dataloading: 0.0799 s/iter. Inference: 0.0592 s/iter. Eval: 0.0005 s/iter. Total: 0.1396 s/iter. ETA=0:00:02\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:46.422917 (0.139383 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.059060 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.09 seconds.\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:14:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:14:13 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:14:13 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:14:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:14:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:14:13 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:14:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:14:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:14:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0371 s/iter. Inference: 0.0717 s/iter. Eval: 0.0006 s/iter. Total: 0.1094 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 10:14:20 d2.evaluation.evaluator]: \u001b[0mInference done 50/1199. Dataloading: 0.0705 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1298 s/iter. ETA=0:02:29\n",
      "\u001b[32m[09/07 10:14:25 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0836 s/iter. Inference: 0.0567 s/iter. Eval: 0.0005 s/iter. Total: 0.1409 s/iter. ETA=0:02:36\n",
      "\u001b[32m[09/07 10:14:31 d2.evaluation.evaluator]: \u001b[0mInference done 119/1199. Dataloading: 0.0861 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1463 s/iter. ETA=0:02:38\n",
      "\u001b[32m[09/07 10:14:36 d2.evaluation.evaluator]: \u001b[0mInference done 160/1199. Dataloading: 0.0806 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1403 s/iter. ETA=0:02:25\n",
      "\u001b[32m[09/07 10:14:41 d2.evaluation.evaluator]: \u001b[0mInference done 199/1199. Dataloading: 0.0789 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1384 s/iter. ETA=0:02:18\n",
      "\u001b[32m[09/07 10:14:46 d2.evaluation.evaluator]: \u001b[0mInference done 234/1199. Dataloading: 0.0791 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:02:14\n",
      "\u001b[32m[09/07 10:14:51 d2.evaluation.evaluator]: \u001b[0mInference done 267/1199. Dataloading: 0.0809 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1414 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 10:14:56 d2.evaluation.evaluator]: \u001b[0mInference done 289/1199. Dataloading: 0.0875 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1482 s/iter. ETA=0:02:14\n",
      "\u001b[32m[09/07 10:15:01 d2.evaluation.evaluator]: \u001b[0mInference done 326/1199. Dataloading: 0.0861 s/iter. Inference: 0.0601 s/iter. Eval: 0.0005 s/iter. Total: 0.1467 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 10:15:06 d2.evaluation.evaluator]: \u001b[0mInference done 366/1199. Dataloading: 0.0841 s/iter. Inference: 0.0604 s/iter. Eval: 0.0005 s/iter. Total: 0.1451 s/iter. ETA=0:02:00\n",
      "\u001b[32m[09/07 10:15:11 d2.evaluation.evaluator]: \u001b[0mInference done 403/1199. Dataloading: 0.0831 s/iter. Inference: 0.0606 s/iter. Eval: 0.0005 s/iter. Total: 0.1443 s/iter. ETA=0:01:54\n",
      "\u001b[32m[09/07 10:15:16 d2.evaluation.evaluator]: \u001b[0mInference done 444/1199. Dataloading: 0.0817 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1424 s/iter. ETA=0:01:47\n",
      "\u001b[32m[09/07 10:15:21 d2.evaluation.evaluator]: \u001b[0mInference done 483/1199. Dataloading: 0.0806 s/iter. Inference: 0.0601 s/iter. Eval: 0.0005 s/iter. Total: 0.1413 s/iter. ETA=0:01:41\n",
      "\u001b[32m[09/07 10:15:27 d2.evaluation.evaluator]: \u001b[0mInference done 522/1199. Dataloading: 0.0802 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1406 s/iter. ETA=0:01:35\n",
      "\u001b[32m[09/07 10:15:32 d2.evaluation.evaluator]: \u001b[0mInference done 560/1199. Dataloading: 0.0795 s/iter. Inference: 0.0601 s/iter. Eval: 0.0005 s/iter. Total: 0.1401 s/iter. ETA=0:01:29\n",
      "\u001b[32m[09/07 10:15:37 d2.evaluation.evaluator]: \u001b[0mInference done 598/1199. Dataloading: 0.0789 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1396 s/iter. ETA=0:01:23\n",
      "\u001b[32m[09/07 10:15:42 d2.evaluation.evaluator]: \u001b[0mInference done 638/1199. Dataloading: 0.0785 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1388 s/iter. ETA=0:01:17\n",
      "\u001b[32m[09/07 10:15:47 d2.evaluation.evaluator]: \u001b[0mInference done 673/1199. Dataloading: 0.0788 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:01:13\n",
      "\u001b[32m[09/07 10:15:52 d2.evaluation.evaluator]: \u001b[0mInference done 708/1199. Dataloading: 0.0790 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1394 s/iter. ETA=0:01:08\n",
      "\u001b[32m[09/07 10:15:57 d2.evaluation.evaluator]: \u001b[0mInference done 746/1199. Dataloading: 0.0785 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:01:03\n",
      "\u001b[32m[09/07 10:16:02 d2.evaluation.evaluator]: \u001b[0mInference done 772/1199. Dataloading: 0.0804 s/iter. Inference: 0.0603 s/iter. Eval: 0.0005 s/iter. Total: 0.1412 s/iter. ETA=0:01:00\n",
      "\u001b[32m[09/07 10:16:07 d2.evaluation.evaluator]: \u001b[0mInference done 813/1199. Dataloading: 0.0799 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1404 s/iter. ETA=0:00:54\n",
      "\u001b[32m[09/07 10:16:13 d2.evaluation.evaluator]: \u001b[0mInference done 856/1199. Dataloading: 0.0792 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1395 s/iter. ETA=0:00:47\n",
      "\u001b[32m[09/07 10:16:18 d2.evaluation.evaluator]: \u001b[0mInference done 895/1199. Dataloading: 0.0789 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:00:42\n",
      "\u001b[32m[09/07 10:16:23 d2.evaluation.evaluator]: \u001b[0mInference done 933/1199. Dataloading: 0.0788 s/iter. Inference: 0.0598 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:00:37\n",
      "\u001b[32m[09/07 10:16:28 d2.evaluation.evaluator]: \u001b[0mInference done 970/1199. Dataloading: 0.0786 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:00:31\n",
      "\u001b[32m[09/07 10:16:33 d2.evaluation.evaluator]: \u001b[0mInference done 1007/1199. Dataloading: 0.0786 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:00:26\n",
      "\u001b[32m[09/07 10:16:38 d2.evaluation.evaluator]: \u001b[0mInference done 1044/1199. Dataloading: 0.0789 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:00:21\n",
      "\u001b[32m[09/07 10:16:43 d2.evaluation.evaluator]: \u001b[0mInference done 1079/1199. Dataloading: 0.0791 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:00:16\n",
      "\u001b[32m[09/07 10:16:48 d2.evaluation.evaluator]: \u001b[0mInference done 1118/1199. Dataloading: 0.0786 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1388 s/iter. ETA=0:00:11\n",
      "\u001b[32m[09/07 10:16:54 d2.evaluation.evaluator]: \u001b[0mInference done 1148/1199. Dataloading: 0.0795 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:00:07\n",
      "\u001b[32m[09/07 10:16:59 d2.evaluation.evaluator]: \u001b[0mInference done 1190/1199. Dataloading: 0.0790 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:00:01\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:46.196525 (0.139193 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:11 (0.059481 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:17:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:17:02 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:17:02 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:17:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:17:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:17:02 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:17:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:17:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:17:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0517 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1117 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 10:17:09 d2.evaluation.evaluator]: \u001b[0mInference done 53/1199. Dataloading: 0.0626 s/iter. Inference: 0.0553 s/iter. Eval: 0.0005 s/iter. Total: 0.1185 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 10:17:14 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0782 s/iter. Inference: 0.0552 s/iter. Eval: 0.0004 s/iter. Total: 0.1340 s/iter. ETA=0:02:29\n",
      "\u001b[32m[09/07 10:17:19 d2.evaluation.evaluator]: \u001b[0mInference done 119/1199. Dataloading: 0.0810 s/iter. Inference: 0.0572 s/iter. Eval: 0.0004 s/iter. Total: 0.1387 s/iter. ETA=0:02:29\n",
      "\u001b[32m[09/07 10:17:24 d2.evaluation.evaluator]: \u001b[0mInference done 160/1199. Dataloading: 0.0752 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1346 s/iter. ETA=0:02:19\n",
      "\u001b[32m[09/07 10:17:29 d2.evaluation.evaluator]: \u001b[0mInference done 197/1199. Dataloading: 0.0754 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1349 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 10:17:34 d2.evaluation.evaluator]: \u001b[0mInference done 231/1199. Dataloading: 0.0771 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1373 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 10:17:39 d2.evaluation.evaluator]: \u001b[0mInference done 266/1199. Dataloading: 0.0786 s/iter. Inference: 0.0592 s/iter. Eval: 0.0005 s/iter. Total: 0.1383 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 10:17:45 d2.evaluation.evaluator]: \u001b[0mInference done 286/1199. Dataloading: 0.0868 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1466 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 10:17:50 d2.evaluation.evaluator]: \u001b[0mInference done 321/1199. Dataloading: 0.0863 s/iter. Inference: 0.0596 s/iter. Eval: 0.0005 s/iter. Total: 0.1464 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 10:17:55 d2.evaluation.evaluator]: \u001b[0mInference done 362/1199. Dataloading: 0.0840 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1441 s/iter. ETA=0:02:00\n",
      "\u001b[32m[09/07 10:18:00 d2.evaluation.evaluator]: \u001b[0mInference done 392/1199. Dataloading: 0.0854 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1459 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/07 10:18:05 d2.evaluation.evaluator]: \u001b[0mInference done 431/1199. Dataloading: 0.0837 s/iter. Inference: 0.0601 s/iter. Eval: 0.0005 s/iter. Total: 0.1443 s/iter. ETA=0:01:50\n",
      "\u001b[32m[09/07 10:18:10 d2.evaluation.evaluator]: \u001b[0mInference done 468/1199. Dataloading: 0.0825 s/iter. Inference: 0.0606 s/iter. Eval: 0.0005 s/iter. Total: 0.1436 s/iter. ETA=0:01:44\n",
      "\u001b[32m[09/07 10:18:15 d2.evaluation.evaluator]: \u001b[0mInference done 502/1199. Dataloading: 0.0823 s/iter. Inference: 0.0612 s/iter. Eval: 0.0005 s/iter. Total: 0.1440 s/iter. ETA=0:01:40\n",
      "\u001b[32m[09/07 10:18:20 d2.evaluation.evaluator]: \u001b[0mInference done 539/1199. Dataloading: 0.0819 s/iter. Inference: 0.0610 s/iter. Eval: 0.0005 s/iter. Total: 0.1435 s/iter. ETA=0:01:34\n",
      "\u001b[32m[09/07 10:18:25 d2.evaluation.evaluator]: \u001b[0mInference done 574/1199. Dataloading: 0.0819 s/iter. Inference: 0.0610 s/iter. Eval: 0.0005 s/iter. Total: 0.1435 s/iter. ETA=0:01:29\n",
      "\u001b[32m[09/07 10:18:30 d2.evaluation.evaluator]: \u001b[0mInference done 615/1199. Dataloading: 0.0807 s/iter. Inference: 0.0610 s/iter. Eval: 0.0005 s/iter. Total: 0.1422 s/iter. ETA=0:01:23\n",
      "\u001b[32m[09/07 10:18:36 d2.evaluation.evaluator]: \u001b[0mInference done 645/1199. Dataloading: 0.0827 s/iter. Inference: 0.0607 s/iter. Eval: 0.0005 s/iter. Total: 0.1440 s/iter. ETA=0:01:19\n",
      "\u001b[32m[09/07 10:18:41 d2.evaluation.evaluator]: \u001b[0mInference done 684/1199. Dataloading: 0.0822 s/iter. Inference: 0.0605 s/iter. Eval: 0.0005 s/iter. Total: 0.1433 s/iter. ETA=0:01:13\n",
      "\u001b[32m[09/07 10:18:46 d2.evaluation.evaluator]: \u001b[0mInference done 724/1199. Dataloading: 0.0817 s/iter. Inference: 0.0603 s/iter. Eval: 0.0005 s/iter. Total: 0.1425 s/iter. ETA=0:01:07\n",
      "\u001b[32m[09/07 10:18:51 d2.evaluation.evaluator]: \u001b[0mInference done 752/1199. Dataloading: 0.0836 s/iter. Inference: 0.0601 s/iter. Eval: 0.0005 s/iter. Total: 0.1443 s/iter. ETA=0:01:04\n",
      "\u001b[32m[09/07 10:18:56 d2.evaluation.evaluator]: \u001b[0mInference done 787/1199. Dataloading: 0.0837 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1443 s/iter. ETA=0:00:59\n",
      "\u001b[32m[09/07 10:19:01 d2.evaluation.evaluator]: \u001b[0mInference done 829/1199. Dataloading: 0.0826 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1431 s/iter. ETA=0:00:52\n",
      "\u001b[32m[09/07 10:19:06 d2.evaluation.evaluator]: \u001b[0mInference done 868/1199. Dataloading: 0.0818 s/iter. Inference: 0.0601 s/iter. Eval: 0.0005 s/iter. Total: 0.1425 s/iter. ETA=0:00:47\n",
      "\u001b[32m[09/07 10:19:11 d2.evaluation.evaluator]: \u001b[0mInference done 905/1199. Dataloading: 0.0815 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1422 s/iter. ETA=0:00:41\n",
      "\u001b[32m[09/07 10:19:16 d2.evaluation.evaluator]: \u001b[0mInference done 946/1199. Dataloading: 0.0808 s/iter. Inference: 0.0601 s/iter. Eval: 0.0005 s/iter. Total: 0.1414 s/iter. ETA=0:00:35\n",
      "\u001b[32m[09/07 10:19:21 d2.evaluation.evaluator]: \u001b[0mInference done 979/1199. Dataloading: 0.0811 s/iter. Inference: 0.0601 s/iter. Eval: 0.0005 s/iter. Total: 0.1418 s/iter. ETA=0:00:31\n",
      "\u001b[32m[09/07 10:19:27 d2.evaluation.evaluator]: \u001b[0mInference done 1018/1199. Dataloading: 0.0809 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1414 s/iter. ETA=0:00:25\n",
      "\u001b[32m[09/07 10:19:32 d2.evaluation.evaluator]: \u001b[0mInference done 1050/1199. Dataloading: 0.0815 s/iter. Inference: 0.0599 s/iter. Eval: 0.0005 s/iter. Total: 0.1420 s/iter. ETA=0:00:21\n",
      "\u001b[32m[09/07 10:19:37 d2.evaluation.evaluator]: \u001b[0mInference done 1085/1199. Dataloading: 0.0815 s/iter. Inference: 0.0602 s/iter. Eval: 0.0005 s/iter. Total: 0.1422 s/iter. ETA=0:00:16\n",
      "\u001b[32m[09/07 10:19:42 d2.evaluation.evaluator]: \u001b[0mInference done 1124/1199. Dataloading: 0.0810 s/iter. Inference: 0.0603 s/iter. Eval: 0.0005 s/iter. Total: 0.1418 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 10:19:47 d2.evaluation.evaluator]: \u001b[0mInference done 1155/1199. Dataloading: 0.0815 s/iter. Inference: 0.0603 s/iter. Eval: 0.0005 s/iter. Total: 0.1424 s/iter. ETA=0:00:06\n",
      "\u001b[32m[09/07 10:19:52 d2.evaluation.evaluator]: \u001b[0mInference done 1196/1199. Dataloading: 0.0813 s/iter. Inference: 0.0601 s/iter. Eval: 0.0005 s/iter. Total: 0.1419 s/iter. ETA=0:00:00\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:49.514863 (0.141972 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:11 (0.060078 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.39 seconds.\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:19:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:19:55 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:19:55 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:19:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:19:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:19:55 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:19:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:19:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:19:57 d2.evaluation.evaluator]: \u001b[0mInference done 14/1199. Dataloading: 0.0481 s/iter. Inference: 0.0603 s/iter. Eval: 0.0004 s/iter. Total: 0.1089 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/07 10:20:03 d2.evaluation.evaluator]: \u001b[0mInference done 57/1199. Dataloading: 0.0593 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1168 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 10:20:08 d2.evaluation.evaluator]: \u001b[0mInference done 86/1199. Dataloading: 0.0777 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1368 s/iter. ETA=0:02:32\n",
      "\u001b[32m[09/07 10:20:13 d2.evaluation.evaluator]: \u001b[0mInference done 123/1199. Dataloading: 0.0790 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1383 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/07 10:20:18 d2.evaluation.evaluator]: \u001b[0mInference done 163/1199. Dataloading: 0.0762 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1357 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/07 10:20:23 d2.evaluation.evaluator]: \u001b[0mInference done 203/1199. Dataloading: 0.0749 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1344 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 10:20:28 d2.evaluation.evaluator]: \u001b[0mInference done 235/1199. Dataloading: 0.0775 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1376 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 10:20:33 d2.evaluation.evaluator]: \u001b[0mInference done 267/1199. Dataloading: 0.0800 s/iter. Inference: 0.0600 s/iter. Eval: 0.0005 s/iter. Total: 0.1406 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 10:20:39 d2.evaluation.evaluator]: \u001b[0mInference done 288/1199. Dataloading: 0.0870 s/iter. Inference: 0.0610 s/iter. Eval: 0.0005 s/iter. Total: 0.1485 s/iter. ETA=0:02:15\n",
      "\u001b[32m[09/07 10:20:44 d2.evaluation.evaluator]: \u001b[0mInference done 325/1199. Dataloading: 0.0856 s/iter. Inference: 0.0612 s/iter. Eval: 0.0005 s/iter. Total: 0.1474 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 10:20:49 d2.evaluation.evaluator]: \u001b[0mInference done 362/1199. Dataloading: 0.0841 s/iter. Inference: 0.0615 s/iter. Eval: 0.0005 s/iter. Total: 0.1461 s/iter. ETA=0:02:02\n",
      "\u001b[32m[09/07 10:20:54 d2.evaluation.evaluator]: \u001b[0mInference done 394/1199. Dataloading: 0.0845 s/iter. Inference: 0.0619 s/iter. Eval: 0.0005 s/iter. Total: 0.1470 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/07 10:20:59 d2.evaluation.evaluator]: \u001b[0mInference done 431/1199. Dataloading: 0.0834 s/iter. Inference: 0.0622 s/iter. Eval: 0.0005 s/iter. Total: 0.1461 s/iter. ETA=0:01:52\n",
      "\u001b[32m[09/07 10:21:04 d2.evaluation.evaluator]: \u001b[0mInference done 471/1199. Dataloading: 0.0821 s/iter. Inference: 0.0621 s/iter. Eval: 0.0005 s/iter. Total: 0.1447 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 10:21:09 d2.evaluation.evaluator]: \u001b[0mInference done 511/1199. Dataloading: 0.0807 s/iter. Inference: 0.0620 s/iter. Eval: 0.0005 s/iter. Total: 0.1432 s/iter. ETA=0:01:38\n",
      "\u001b[32m[09/07 10:21:14 d2.evaluation.evaluator]: \u001b[0mInference done 548/1199. Dataloading: 0.0808 s/iter. Inference: 0.0617 s/iter. Eval: 0.0005 s/iter. Total: 0.1431 s/iter. ETA=0:01:33\n",
      "\u001b[32m[09/07 10:21:19 d2.evaluation.evaluator]: \u001b[0mInference done 587/1199. Dataloading: 0.0800 s/iter. Inference: 0.0616 s/iter. Eval: 0.0005 s/iter. Total: 0.1422 s/iter. ETA=0:01:27\n",
      "\u001b[32m[09/07 10:21:24 d2.evaluation.evaluator]: \u001b[0mInference done 627/1199. Dataloading: 0.0793 s/iter. Inference: 0.0613 s/iter. Eval: 0.0005 s/iter. Total: 0.1412 s/iter. ETA=0:01:20\n",
      "\u001b[32m[09/07 10:21:29 d2.evaluation.evaluator]: \u001b[0mInference done 659/1199. Dataloading: 0.0803 s/iter. Inference: 0.0612 s/iter. Eval: 0.0005 s/iter. Total: 0.1421 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 10:21:35 d2.evaluation.evaluator]: \u001b[0mInference done 695/1199. Dataloading: 0.0805 s/iter. Inference: 0.0611 s/iter. Eval: 0.0005 s/iter. Total: 0.1421 s/iter. ETA=0:01:11\n",
      "\u001b[32m[09/07 10:21:40 d2.evaluation.evaluator]: \u001b[0mInference done 735/1199. Dataloading: 0.0800 s/iter. Inference: 0.0608 s/iter. Eval: 0.0005 s/iter. Total: 0.1413 s/iter. ETA=0:01:05\n",
      "\u001b[32m[09/07 10:21:45 d2.evaluation.evaluator]: \u001b[0mInference done 762/1199. Dataloading: 0.0816 s/iter. Inference: 0.0607 s/iter. Eval: 0.0005 s/iter. Total: 0.1429 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 10:21:50 d2.evaluation.evaluator]: \u001b[0mInference done 802/1199. Dataloading: 0.0810 s/iter. Inference: 0.0606 s/iter. Eval: 0.0005 s/iter. Total: 0.1422 s/iter. ETA=0:00:56\n",
      "\u001b[32m[09/07 10:21:55 d2.evaluation.evaluator]: \u001b[0mInference done 843/1199. Dataloading: 0.0802 s/iter. Inference: 0.0605 s/iter. Eval: 0.0005 s/iter. Total: 0.1412 s/iter. ETA=0:00:50\n",
      "\u001b[32m[09/07 10:22:00 d2.evaluation.evaluator]: \u001b[0mInference done 880/1199. Dataloading: 0.0799 s/iter. Inference: 0.0606 s/iter. Eval: 0.0005 s/iter. Total: 0.1410 s/iter. ETA=0:00:44\n",
      "\u001b[32m[09/07 10:22:05 d2.evaluation.evaluator]: \u001b[0mInference done 915/1199. Dataloading: 0.0801 s/iter. Inference: 0.0606 s/iter. Eval: 0.0005 s/iter. Total: 0.1412 s/iter. ETA=0:00:40\n",
      "\u001b[32m[09/07 10:22:10 d2.evaluation.evaluator]: \u001b[0mInference done 959/1199. Dataloading: 0.0790 s/iter. Inference: 0.0606 s/iter. Eval: 0.0005 s/iter. Total: 0.1401 s/iter. ETA=0:00:33\n",
      "\u001b[32m[09/07 10:22:15 d2.evaluation.evaluator]: \u001b[0mInference done 989/1199. Dataloading: 0.0799 s/iter. Inference: 0.0607 s/iter. Eval: 0.0005 s/iter. Total: 0.1412 s/iter. ETA=0:00:29\n",
      "\u001b[32m[09/07 10:22:20 d2.evaluation.evaluator]: \u001b[0mInference done 1023/1199. Dataloading: 0.0803 s/iter. Inference: 0.0606 s/iter. Eval: 0.0005 s/iter. Total: 0.1415 s/iter. ETA=0:00:24\n",
      "\u001b[32m[09/07 10:22:26 d2.evaluation.evaluator]: \u001b[0mInference done 1060/1199. Dataloading: 0.0802 s/iter. Inference: 0.0605 s/iter. Eval: 0.0005 s/iter. Total: 0.1413 s/iter. ETA=0:00:19\n",
      "\u001b[32m[09/07 10:22:31 d2.evaluation.evaluator]: \u001b[0mInference done 1095/1199. Dataloading: 0.0804 s/iter. Inference: 0.0605 s/iter. Eval: 0.0005 s/iter. Total: 0.1414 s/iter. ETA=0:00:14\n",
      "\u001b[32m[09/07 10:22:36 d2.evaluation.evaluator]: \u001b[0mInference done 1129/1199. Dataloading: 0.0806 s/iter. Inference: 0.0605 s/iter. Eval: 0.0005 s/iter. Total: 0.1416 s/iter. ETA=0:00:09\n",
      "\u001b[32m[09/07 10:22:41 d2.evaluation.evaluator]: \u001b[0mInference done 1167/1199. Dataloading: 0.0803 s/iter. Inference: 0.0604 s/iter. Eval: 0.0005 s/iter. Total: 0.1413 s/iter. ETA=0:00:04\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:48.240140 (0.140905 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:12 (0.060402 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:22:45 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:22:47 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:22:47 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:22:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:22:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:22:47 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:22:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:22:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:22:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0380 s/iter. Inference: 0.0474 s/iter. Eval: 0.0004 s/iter. Total: 0.0859 s/iter. ETA=0:01:42\n",
      "\u001b[32m[09/07 10:22:53 d2.evaluation.evaluator]: \u001b[0mInference done 52/1199. Dataloading: 0.0622 s/iter. Inference: 0.0548 s/iter. Eval: 0.0005 s/iter. Total: 0.1175 s/iter. ETA=0:02:14\n",
      "\u001b[32m[09/07 10:22:59 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0813 s/iter. Inference: 0.0533 s/iter. Eval: 0.0005 s/iter. Total: 0.1351 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/07 10:23:04 d2.evaluation.evaluator]: \u001b[0mInference done 120/1199. Dataloading: 0.0813 s/iter. Inference: 0.0561 s/iter. Eval: 0.0005 s/iter. Total: 0.1379 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/07 10:23:09 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0770 s/iter. Inference: 0.0569 s/iter. Eval: 0.0005 s/iter. Total: 0.1344 s/iter. ETA=0:02:19\n",
      "\u001b[32m[09/07 10:23:14 d2.evaluation.evaluator]: \u001b[0mInference done 199/1199. Dataloading: 0.0756 s/iter. Inference: 0.0577 s/iter. Eval: 0.0005 s/iter. Total: 0.1339 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 10:23:19 d2.evaluation.evaluator]: \u001b[0mInference done 235/1199. Dataloading: 0.0763 s/iter. Inference: 0.0580 s/iter. Eval: 0.0005 s/iter. Total: 0.1349 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 10:23:24 d2.evaluation.evaluator]: \u001b[0mInference done 268/1199. Dataloading: 0.0782 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1372 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 10:23:29 d2.evaluation.evaluator]: \u001b[0mInference done 288/1199. Dataloading: 0.0859 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1454 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 10:23:34 d2.evaluation.evaluator]: \u001b[0mInference done 325/1199. Dataloading: 0.0851 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1444 s/iter. ETA=0:02:06\n",
      "\u001b[32m[09/07 10:23:39 d2.evaluation.evaluator]: \u001b[0mInference done 365/1199. Dataloading: 0.0834 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1424 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/07 10:23:44 d2.evaluation.evaluator]: \u001b[0mInference done 398/1199. Dataloading: 0.0844 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1433 s/iter. ETA=0:01:54\n",
      "\u001b[32m[09/07 10:23:49 d2.evaluation.evaluator]: \u001b[0mInference done 439/1199. Dataloading: 0.0828 s/iter. Inference: 0.0581 s/iter. Eval: 0.0005 s/iter. Total: 0.1414 s/iter. ETA=0:01:47\n",
      "\u001b[32m[09/07 10:23:54 d2.evaluation.evaluator]: \u001b[0mInference done 479/1199. Dataloading: 0.0817 s/iter. Inference: 0.0581 s/iter. Eval: 0.0005 s/iter. Total: 0.1404 s/iter. ETA=0:01:41\n",
      "\u001b[32m[09/07 10:23:59 d2.evaluation.evaluator]: \u001b[0mInference done 520/1199. Dataloading: 0.0803 s/iter. Inference: 0.0582 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:01:34\n",
      "\u001b[32m[09/07 10:24:05 d2.evaluation.evaluator]: \u001b[0mInference done 556/1199. Dataloading: 0.0802 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:01:29\n",
      "\u001b[32m[09/07 10:24:10 d2.evaluation.evaluator]: \u001b[0mInference done 593/1199. Dataloading: 0.0799 s/iter. Inference: 0.0586 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:01:24\n",
      "\u001b[32m[09/07 10:24:15 d2.evaluation.evaluator]: \u001b[0mInference done 632/1199. Dataloading: 0.0792 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1385 s/iter. ETA=0:01:18\n",
      "\u001b[32m[09/07 10:24:20 d2.evaluation.evaluator]: \u001b[0mInference done 665/1199. Dataloading: 0.0798 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1392 s/iter. ETA=0:01:14\n",
      "\u001b[32m[09/07 10:24:25 d2.evaluation.evaluator]: \u001b[0mInference done 703/1199. Dataloading: 0.0798 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:01:08\n",
      "\u001b[32m[09/07 10:24:30 d2.evaluation.evaluator]: \u001b[0mInference done 742/1199. Dataloading: 0.0791 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1386 s/iter. ETA=0:01:03\n",
      "\u001b[32m[09/07 10:24:35 d2.evaluation.evaluator]: \u001b[0mInference done 764/1199. Dataloading: 0.0813 s/iter. Inference: 0.0595 s/iter. Eval: 0.0005 s/iter. Total: 0.1413 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/07 10:24:40 d2.evaluation.evaluator]: \u001b[0mInference done 802/1199. Dataloading: 0.0809 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1409 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/07 10:24:45 d2.evaluation.evaluator]: \u001b[0mInference done 846/1199. Dataloading: 0.0801 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1396 s/iter. ETA=0:00:49\n",
      "\u001b[32m[09/07 10:24:50 d2.evaluation.evaluator]: \u001b[0mInference done 884/1199. Dataloading: 0.0798 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1394 s/iter. ETA=0:00:43\n",
      "\u001b[32m[09/07 10:24:55 d2.evaluation.evaluator]: \u001b[0mInference done 921/1199. Dataloading: 0.0797 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:00:38\n",
      "\u001b[32m[09/07 10:25:00 d2.evaluation.evaluator]: \u001b[0mInference done 962/1199. Dataloading: 0.0790 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1386 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 10:25:06 d2.evaluation.evaluator]: \u001b[0mInference done 994/1199. Dataloading: 0.0797 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1393 s/iter. ETA=0:00:28\n",
      "\u001b[32m[09/07 10:25:11 d2.evaluation.evaluator]: \u001b[0mInference done 1029/1199. Dataloading: 0.0800 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1395 s/iter. ETA=0:00:23\n",
      "\u001b[32m[09/07 10:25:16 d2.evaluation.evaluator]: \u001b[0mInference done 1063/1199. Dataloading: 0.0803 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1398 s/iter. ETA=0:00:19\n",
      "\u001b[32m[09/07 10:25:21 d2.evaluation.evaluator]: \u001b[0mInference done 1095/1199. Dataloading: 0.0808 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1405 s/iter. ETA=0:00:14\n",
      "\u001b[32m[09/07 10:25:26 d2.evaluation.evaluator]: \u001b[0mInference done 1130/1199. Dataloading: 0.0810 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1407 s/iter. ETA=0:00:09\n",
      "\u001b[32m[09/07 10:25:31 d2.evaluation.evaluator]: \u001b[0mInference done 1166/1199. Dataloading: 0.0808 s/iter. Inference: 0.0593 s/iter. Eval: 0.0005 s/iter. Total: 0.1407 s/iter. ETA=0:00:04\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:47.473876 (0.140263 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.059184 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:25:35 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:25:38 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:25:38 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:25:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:25:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:25:38 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:25:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:25:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:25:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0375 s/iter. Inference: 0.0568 s/iter. Eval: 0.0004 s/iter. Total: 0.0947 s/iter. ETA=0:01:52\n",
      "\u001b[32m[09/07 10:25:44 d2.evaluation.evaluator]: \u001b[0mInference done 52/1199. Dataloading: 0.0588 s/iter. Inference: 0.0602 s/iter. Eval: 0.0004 s/iter. Total: 0.1194 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/07 10:25:50 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0773 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1354 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/07 10:25:55 d2.evaluation.evaluator]: \u001b[0mInference done 122/1199. Dataloading: 0.0765 s/iter. Inference: 0.0586 s/iter. Eval: 0.0004 s/iter. Total: 0.1356 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/07 10:26:00 d2.evaluation.evaluator]: \u001b[0mInference done 161/1199. Dataloading: 0.0757 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1353 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/07 10:26:05 d2.evaluation.evaluator]: \u001b[0mInference done 201/1199. Dataloading: 0.0743 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1335 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 10:26:10 d2.evaluation.evaluator]: \u001b[0mInference done 237/1199. Dataloading: 0.0761 s/iter. Inference: 0.0588 s/iter. Eval: 0.0004 s/iter. Total: 0.1354 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/07 10:26:15 d2.evaluation.evaluator]: \u001b[0mInference done 270/1199. Dataloading: 0.0783 s/iter. Inference: 0.0594 s/iter. Eval: 0.0004 s/iter. Total: 0.1382 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 10:26:20 d2.evaluation.evaluator]: \u001b[0mInference done 292/1199. Dataloading: 0.0849 s/iter. Inference: 0.0599 s/iter. Eval: 0.0004 s/iter. Total: 0.1453 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 10:26:25 d2.evaluation.evaluator]: \u001b[0mInference done 326/1199. Dataloading: 0.0840 s/iter. Inference: 0.0610 s/iter. Eval: 0.0005 s/iter. Total: 0.1456 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/07 10:26:31 d2.evaluation.evaluator]: \u001b[0mInference done 366/1199. Dataloading: 0.0831 s/iter. Inference: 0.0605 s/iter. Eval: 0.0005 s/iter. Total: 0.1441 s/iter. ETA=0:02:00\n",
      "\u001b[32m[09/07 10:26:36 d2.evaluation.evaluator]: \u001b[0mInference done 402/1199. Dataloading: 0.0835 s/iter. Inference: 0.0597 s/iter. Eval: 0.0005 s/iter. Total: 0.1438 s/iter. ETA=0:01:54\n",
      "\u001b[32m[09/07 10:26:41 d2.evaluation.evaluator]: \u001b[0mInference done 444/1199. Dataloading: 0.0817 s/iter. Inference: 0.0594 s/iter. Eval: 0.0005 s/iter. Total: 0.1416 s/iter. ETA=0:01:46\n",
      "\u001b[32m[09/07 10:26:46 d2.evaluation.evaluator]: \u001b[0mInference done 484/1199. Dataloading: 0.0810 s/iter. Inference: 0.0591 s/iter. Eval: 0.0005 s/iter. Total: 0.1407 s/iter. ETA=0:01:40\n",
      "\u001b[32m[09/07 10:26:51 d2.evaluation.evaluator]: \u001b[0mInference done 524/1199. Dataloading: 0.0800 s/iter. Inference: 0.0590 s/iter. Eval: 0.0005 s/iter. Total: 0.1396 s/iter. ETA=0:01:34\n",
      "\u001b[32m[09/07 10:26:56 d2.evaluation.evaluator]: \u001b[0mInference done 562/1199. Dataloading: 0.0796 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:01:28\n",
      "\u001b[32m[09/07 10:27:01 d2.evaluation.evaluator]: \u001b[0mInference done 600/1199. Dataloading: 0.0797 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:01:23\n",
      "\u001b[32m[09/07 10:27:07 d2.evaluation.evaluator]: \u001b[0mInference done 641/1199. Dataloading: 0.0793 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1383 s/iter. ETA=0:01:17\n",
      "\u001b[32m[09/07 10:27:12 d2.evaluation.evaluator]: \u001b[0mInference done 675/1199. Dataloading: 0.0807 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1399 s/iter. ETA=0:01:13\n",
      "\u001b[32m[09/07 10:27:18 d2.evaluation.evaluator]: \u001b[0mInference done 718/1199. Dataloading: 0.0794 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:01:06\n",
      "\u001b[32m[09/07 10:27:23 d2.evaluation.evaluator]: \u001b[0mInference done 750/1199. Dataloading: 0.0801 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1395 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/07 10:27:28 d2.evaluation.evaluator]: \u001b[0mInference done 781/1199. Dataloading: 0.0809 s/iter. Inference: 0.0589 s/iter. Eval: 0.0005 s/iter. Total: 0.1404 s/iter. ETA=0:00:58\n",
      "\u001b[32m[09/07 10:27:33 d2.evaluation.evaluator]: \u001b[0mInference done 822/1199. Dataloading: 0.0803 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1395 s/iter. ETA=0:00:52\n",
      "\u001b[32m[09/07 10:27:38 d2.evaluation.evaluator]: \u001b[0mInference done 865/1199. Dataloading: 0.0797 s/iter. Inference: 0.0583 s/iter. Eval: 0.0005 s/iter. Total: 0.1386 s/iter. ETA=0:00:46\n",
      "\u001b[32m[09/07 10:27:43 d2.evaluation.evaluator]: \u001b[0mInference done 901/1199. Dataloading: 0.0797 s/iter. Inference: 0.0584 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:00:41\n",
      "\u001b[32m[09/07 10:27:48 d2.evaluation.evaluator]: \u001b[0mInference done 941/1199. Dataloading: 0.0791 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1381 s/iter. ETA=0:00:35\n",
      "\u001b[32m[09/07 10:27:53 d2.evaluation.evaluator]: \u001b[0mInference done 977/1199. Dataloading: 0.0792 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1383 s/iter. ETA=0:00:30\n",
      "\u001b[32m[09/07 10:27:58 d2.evaluation.evaluator]: \u001b[0mInference done 1017/1199. Dataloading: 0.0788 s/iter. Inference: 0.0585 s/iter. Eval: 0.0005 s/iter. Total: 0.1378 s/iter. ETA=0:00:25\n",
      "\u001b[32m[09/07 10:28:03 d2.evaluation.evaluator]: \u001b[0mInference done 1048/1199. Dataloading: 0.0795 s/iter. Inference: 0.0586 s/iter. Eval: 0.0005 s/iter. Total: 0.1386 s/iter. ETA=0:00:20\n",
      "\u001b[32m[09/07 10:28:08 d2.evaluation.evaluator]: \u001b[0mInference done 1085/1199. Dataloading: 0.0795 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:00:15\n",
      "\u001b[32m[09/07 10:28:15 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.0798 s/iter. Inference: 0.0588 s/iter. Eval: 0.0005 s/iter. Total: 0.1391 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 10:28:20 d2.evaluation.evaluator]: \u001b[0mInference done 1163/1199. Dataloading: 0.0797 s/iter. Inference: 0.0587 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:00:05\n",
      "\u001b[32m[09/07 10:28:24 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:45.755032 (0.138823 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:28:24 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.058670 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:28:24 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:28:24 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:28:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:28:24 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:28:25 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 10:28:25 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:28:25 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:28:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:28:25 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:28:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:28:27 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:28:27 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:28:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:28:27 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:28:27 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:28:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:28:27 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:28:28 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0388 s/iter. Inference: 0.0524 s/iter. Eval: 0.0004 s/iter. Total: 0.0917 s/iter. ETA=0:01:48\n",
      "\u001b[32m[09/07 10:28:33 d2.evaluation.evaluator]: \u001b[0mInference done 52/1199. Dataloading: 0.0627 s/iter. Inference: 0.0555 s/iter. Eval: 0.0005 s/iter. Total: 0.1188 s/iter. ETA=0:02:16\n",
      "\u001b[32m[09/07 10:28:39 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0815 s/iter. Inference: 0.0556 s/iter. Eval: 0.0004 s/iter. Total: 0.1376 s/iter. ETA=0:02:33\n",
      "\u001b[32m[09/07 10:28:44 d2.evaluation.evaluator]: \u001b[0mInference done 120/1199. Dataloading: 0.0840 s/iter. Inference: 0.0547 s/iter. Eval: 0.0004 s/iter. Total: 0.1392 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/07 10:28:49 d2.evaluation.evaluator]: \u001b[0mInference done 160/1199. Dataloading: 0.0803 s/iter. Inference: 0.0548 s/iter. Eval: 0.0004 s/iter. Total: 0.1357 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/07 10:28:54 d2.evaluation.evaluator]: \u001b[0mInference done 195/1199. Dataloading: 0.0800 s/iter. Inference: 0.0566 s/iter. Eval: 0.0004 s/iter. Total: 0.1372 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/07 10:28:59 d2.evaluation.evaluator]: \u001b[0mInference done 231/1199. Dataloading: 0.0803 s/iter. Inference: 0.0569 s/iter. Eval: 0.0004 s/iter. Total: 0.1377 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/07 10:29:04 d2.evaluation.evaluator]: \u001b[0mInference done 267/1199. Dataloading: 0.0810 s/iter. Inference: 0.0565 s/iter. Eval: 0.0004 s/iter. Total: 0.1381 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/07 10:29:09 d2.evaluation.evaluator]: \u001b[0mInference done 290/1199. Dataloading: 0.0874 s/iter. Inference: 0.0569 s/iter. Eval: 0.0004 s/iter. Total: 0.1448 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/07 10:29:14 d2.evaluation.evaluator]: \u001b[0mInference done 327/1199. Dataloading: 0.0866 s/iter. Inference: 0.0568 s/iter. Eval: 0.0004 s/iter. Total: 0.1439 s/iter. ETA=0:02:05\n",
      "\u001b[32m[09/07 10:29:19 d2.evaluation.evaluator]: \u001b[0mInference done 368/1199. Dataloading: 0.0847 s/iter. Inference: 0.0565 s/iter. Eval: 0.0004 s/iter. Total: 0.1417 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/07 10:29:24 d2.evaluation.evaluator]: \u001b[0mInference done 404/1199. Dataloading: 0.0845 s/iter. Inference: 0.0567 s/iter. Eval: 0.0004 s/iter. Total: 0.1417 s/iter. ETA=0:01:52\n",
      "\u001b[32m[09/07 10:29:29 d2.evaluation.evaluator]: \u001b[0mInference done 445/1199. Dataloading: 0.0824 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1399 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 10:29:34 d2.evaluation.evaluator]: \u001b[0mInference done 483/1199. Dataloading: 0.0818 s/iter. Inference: 0.0570 s/iter. Eval: 0.0004 s/iter. Total: 0.1393 s/iter. ETA=0:01:39\n",
      "\u001b[32m[09/07 10:29:40 d2.evaluation.evaluator]: \u001b[0mInference done 524/1199. Dataloading: 0.0810 s/iter. Inference: 0.0569 s/iter. Eval: 0.0004 s/iter. Total: 0.1385 s/iter. ETA=0:01:33\n",
      "\u001b[32m[09/07 10:29:45 d2.evaluation.evaluator]: \u001b[0mInference done 566/1199. Dataloading: 0.0793 s/iter. Inference: 0.0573 s/iter. Eval: 0.0004 s/iter. Total: 0.1371 s/iter. ETA=0:01:26\n",
      "\u001b[32m[09/07 10:29:50 d2.evaluation.evaluator]: \u001b[0mInference done 604/1199. Dataloading: 0.0790 s/iter. Inference: 0.0574 s/iter. Eval: 0.0004 s/iter. Total: 0.1369 s/iter. ETA=0:01:21\n",
      "\u001b[32m[09/07 10:29:55 d2.evaluation.evaluator]: \u001b[0mInference done 642/1199. Dataloading: 0.0787 s/iter. Inference: 0.0574 s/iter. Eval: 0.0004 s/iter. Total: 0.1366 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 10:30:01 d2.evaluation.evaluator]: \u001b[0mInference done 675/1199. Dataloading: 0.0804 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1386 s/iter. ETA=0:01:12\n",
      "\u001b[32m[09/07 10:30:06 d2.evaluation.evaluator]: \u001b[0mInference done 720/1199. Dataloading: 0.0791 s/iter. Inference: 0.0573 s/iter. Eval: 0.0004 s/iter. Total: 0.1370 s/iter. ETA=0:01:05\n",
      "\u001b[32m[09/07 10:30:11 d2.evaluation.evaluator]: \u001b[0mInference done 752/1199. Dataloading: 0.0806 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1387 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/07 10:30:16 d2.evaluation.evaluator]: \u001b[0mInference done 791/1199. Dataloading: 0.0802 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1383 s/iter. ETA=0:00:56\n",
      "\u001b[32m[09/07 10:30:21 d2.evaluation.evaluator]: \u001b[0mInference done 836/1199. Dataloading: 0.0788 s/iter. Inference: 0.0575 s/iter. Eval: 0.0004 s/iter. Total: 0.1368 s/iter. ETA=0:00:49\n",
      "\u001b[32m[09/07 10:30:26 d2.evaluation.evaluator]: \u001b[0mInference done 876/1199. Dataloading: 0.0782 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1363 s/iter. ETA=0:00:44\n",
      "\u001b[32m[09/07 10:30:31 d2.evaluation.evaluator]: \u001b[0mInference done 912/1199. Dataloading: 0.0782 s/iter. Inference: 0.0577 s/iter. Eval: 0.0004 s/iter. Total: 0.1365 s/iter. ETA=0:00:39\n",
      "\u001b[32m[09/07 10:30:37 d2.evaluation.evaluator]: \u001b[0mInference done 957/1199. Dataloading: 0.0773 s/iter. Inference: 0.0576 s/iter. Eval: 0.0004 s/iter. Total: 0.1354 s/iter. ETA=0:00:32\n",
      "\u001b[32m[09/07 10:30:42 d2.evaluation.evaluator]: \u001b[0mInference done 988/1199. Dataloading: 0.0781 s/iter. Inference: 0.0577 s/iter. Eval: 0.0004 s/iter. Total: 0.1363 s/iter. ETA=0:00:28\n",
      "\u001b[32m[09/07 10:30:47 d2.evaluation.evaluator]: \u001b[0mInference done 1019/1199. Dataloading: 0.0789 s/iter. Inference: 0.0580 s/iter. Eval: 0.0004 s/iter. Total: 0.1374 s/iter. ETA=0:00:24\n",
      "\u001b[32m[09/07 10:30:52 d2.evaluation.evaluator]: \u001b[0mInference done 1057/1199. Dataloading: 0.0785 s/iter. Inference: 0.0583 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:00:19\n",
      "\u001b[32m[09/07 10:30:57 d2.evaluation.evaluator]: \u001b[0mInference done 1089/1199. Dataloading: 0.0790 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1380 s/iter. ETA=0:00:15\n",
      "\u001b[32m[09/07 10:31:03 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.0793 s/iter. Inference: 0.0585 s/iter. Eval: 0.0004 s/iter. Total: 0.1383 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 10:31:08 d2.evaluation.evaluator]: \u001b[0mInference done 1162/1199. Dataloading: 0.0791 s/iter. Inference: 0.0587 s/iter. Eval: 0.0004 s/iter. Total: 0.1383 s/iter. ETA=0:00:05\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:45.001378 (0.138192 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.058877 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:31:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/07 10:31:15 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/07 10:31:15 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/07 10:31:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/07 10:31:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/07 10:31:15 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/07 10:31:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/07 10:31:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/07 10:31:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0408 s/iter. Inference: 0.0629 s/iter. Eval: 0.0004 s/iter. Total: 0.1041 s/iter. ETA=0:02:03\n",
      "\u001b[32m[09/07 10:31:22 d2.evaluation.evaluator]: \u001b[0mInference done 52/1199. Dataloading: 0.0555 s/iter. Inference: 0.0639 s/iter. Eval: 0.0005 s/iter. Total: 0.1200 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/07 10:31:27 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0777 s/iter. Inference: 0.0607 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:02:34\n",
      "\u001b[32m[09/07 10:31:32 d2.evaluation.evaluator]: \u001b[0mInference done 121/1199. Dataloading: 0.0785 s/iter. Inference: 0.0606 s/iter. Eval: 0.0005 s/iter. Total: 0.1397 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/07 10:31:38 d2.evaluation.evaluator]: \u001b[0mInference done 159/1199. Dataloading: 0.0764 s/iter. Inference: 0.0621 s/iter. Eval: 0.0005 s/iter. Total: 0.1390 s/iter. ETA=0:02:24\n",
      "\u001b[32m[09/07 10:31:43 d2.evaluation.evaluator]: \u001b[0mInference done 196/1199. Dataloading: 0.0763 s/iter. Inference: 0.0619 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:02:19\n",
      "\u001b[32m[09/07 10:31:48 d2.evaluation.evaluator]: \u001b[0mInference done 229/1199. Dataloading: 0.0781 s/iter. Inference: 0.0624 s/iter. Eval: 0.0005 s/iter. Total: 0.1411 s/iter. ETA=0:02:16\n",
      "\u001b[32m[09/07 10:31:53 d2.evaluation.evaluator]: \u001b[0mInference done 263/1199. Dataloading: 0.0785 s/iter. Inference: 0.0629 s/iter. Eval: 0.0005 s/iter. Total: 0.1420 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 10:31:58 d2.evaluation.evaluator]: \u001b[0mInference done 282/1199. Dataloading: 0.0872 s/iter. Inference: 0.0635 s/iter. Eval: 0.0005 s/iter. Total: 0.1513 s/iter. ETA=0:02:18\n",
      "\u001b[32m[09/07 10:32:03 d2.evaluation.evaluator]: \u001b[0mInference done 317/1199. Dataloading: 0.0864 s/iter. Inference: 0.0637 s/iter. Eval: 0.0005 s/iter. Total: 0.1507 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/07 10:32:08 d2.evaluation.evaluator]: \u001b[0mInference done 355/1199. Dataloading: 0.0848 s/iter. Inference: 0.0635 s/iter. Eval: 0.0005 s/iter. Total: 0.1488 s/iter. ETA=0:02:05\n",
      "\u001b[32m[09/07 10:32:13 d2.evaluation.evaluator]: \u001b[0mInference done 389/1199. Dataloading: 0.0851 s/iter. Inference: 0.0631 s/iter. Eval: 0.0005 s/iter. Total: 0.1487 s/iter. ETA=0:02:00\n",
      "\u001b[32m[09/07 10:32:18 d2.evaluation.evaluator]: \u001b[0mInference done 428/1199. Dataloading: 0.0835 s/iter. Inference: 0.0629 s/iter. Eval: 0.0005 s/iter. Total: 0.1470 s/iter. ETA=0:01:53\n",
      "\u001b[32m[09/07 10:32:23 d2.evaluation.evaluator]: \u001b[0mInference done 469/1199. Dataloading: 0.0818 s/iter. Inference: 0.0625 s/iter. Eval: 0.0005 s/iter. Total: 0.1449 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/07 10:32:28 d2.evaluation.evaluator]: \u001b[0mInference done 509/1199. Dataloading: 0.0808 s/iter. Inference: 0.0620 s/iter. Eval: 0.0005 s/iter. Total: 0.1434 s/iter. ETA=0:01:38\n",
      "\u001b[32m[09/07 10:32:34 d2.evaluation.evaluator]: \u001b[0mInference done 546/1199. Dataloading: 0.0806 s/iter. Inference: 0.0618 s/iter. Eval: 0.0005 s/iter. Total: 0.1430 s/iter. ETA=0:01:33\n",
      "\u001b[32m[09/07 10:32:39 d2.evaluation.evaluator]: \u001b[0mInference done 585/1199. Dataloading: 0.0795 s/iter. Inference: 0.0621 s/iter. Eval: 0.0005 s/iter. Total: 0.1422 s/iter. ETA=0:01:27\n",
      "\u001b[32m[09/07 10:32:44 d2.evaluation.evaluator]: \u001b[0mInference done 626/1199. Dataloading: 0.0790 s/iter. Inference: 0.0618 s/iter. Eval: 0.0005 s/iter. Total: 0.1413 s/iter. ETA=0:01:20\n",
      "\u001b[32m[09/07 10:32:49 d2.evaluation.evaluator]: \u001b[0mInference done 658/1199. Dataloading: 0.0798 s/iter. Inference: 0.0617 s/iter. Eval: 0.0005 s/iter. Total: 0.1421 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/07 10:32:54 d2.evaluation.evaluator]: \u001b[0mInference done 691/1199. Dataloading: 0.0803 s/iter. Inference: 0.0617 s/iter. Eval: 0.0005 s/iter. Total: 0.1426 s/iter. ETA=0:01:12\n",
      "\u001b[32m[09/07 10:32:59 d2.evaluation.evaluator]: \u001b[0mInference done 728/1199. Dataloading: 0.0797 s/iter. Inference: 0.0620 s/iter. Eval: 0.0005 s/iter. Total: 0.1423 s/iter. ETA=0:01:07\n",
      "\u001b[32m[09/07 10:33:04 d2.evaluation.evaluator]: \u001b[0mInference done 754/1199. Dataloading: 0.0814 s/iter. Inference: 0.0622 s/iter. Eval: 0.0005 s/iter. Total: 0.1442 s/iter. ETA=0:01:04\n",
      "\u001b[32m[09/07 10:33:09 d2.evaluation.evaluator]: \u001b[0mInference done 791/1199. Dataloading: 0.0811 s/iter. Inference: 0.0624 s/iter. Eval: 0.0005 s/iter. Total: 0.1441 s/iter. ETA=0:00:58\n",
      "\u001b[32m[09/07 10:33:14 d2.evaluation.evaluator]: \u001b[0mInference done 831/1199. Dataloading: 0.0801 s/iter. Inference: 0.0626 s/iter. Eval: 0.0005 s/iter. Total: 0.1433 s/iter. ETA=0:00:52\n",
      "\u001b[32m[09/07 10:33:20 d2.evaluation.evaluator]: \u001b[0mInference done 871/1199. Dataloading: 0.0794 s/iter. Inference: 0.0626 s/iter. Eval: 0.0005 s/iter. Total: 0.1426 s/iter. ETA=0:00:46\n",
      "\u001b[32m[09/07 10:33:25 d2.evaluation.evaluator]: \u001b[0mInference done 909/1199. Dataloading: 0.0793 s/iter. Inference: 0.0624 s/iter. Eval: 0.0005 s/iter. Total: 0.1423 s/iter. ETA=0:00:41\n",
      "\u001b[32m[09/07 10:33:30 d2.evaluation.evaluator]: \u001b[0mInference done 951/1199. Dataloading: 0.0783 s/iter. Inference: 0.0624 s/iter. Eval: 0.0005 s/iter. Total: 0.1413 s/iter. ETA=0:00:35\n",
      "\u001b[32m[09/07 10:33:35 d2.evaluation.evaluator]: \u001b[0mInference done 983/1199. Dataloading: 0.0789 s/iter. Inference: 0.0623 s/iter. Eval: 0.0005 s/iter. Total: 0.1418 s/iter. ETA=0:00:30\n",
      "\u001b[32m[09/07 10:33:41 d2.evaluation.evaluator]: \u001b[0mInference done 1019/1199. Dataloading: 0.0797 s/iter. Inference: 0.0623 s/iter. Eval: 0.0005 s/iter. Total: 0.1425 s/iter. ETA=0:00:25\n",
      "\u001b[32m[09/07 10:33:46 d2.evaluation.evaluator]: \u001b[0mInference done 1057/1199. Dataloading: 0.0793 s/iter. Inference: 0.0623 s/iter. Eval: 0.0005 s/iter. Total: 0.1422 s/iter. ETA=0:00:20\n",
      "\u001b[32m[09/07 10:33:51 d2.evaluation.evaluator]: \u001b[0mInference done 1089/1199. Dataloading: 0.0797 s/iter. Inference: 0.0624 s/iter. Eval: 0.0005 s/iter. Total: 0.1427 s/iter. ETA=0:00:15\n",
      "\u001b[32m[09/07 10:33:56 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.0801 s/iter. Inference: 0.0624 s/iter. Eval: 0.0005 s/iter. Total: 0.1431 s/iter. ETA=0:00:10\n",
      "\u001b[32m[09/07 10:34:02 d2.evaluation.evaluator]: \u001b[0mInference done 1161/1199. Dataloading: 0.0801 s/iter. Inference: 0.0625 s/iter. Eval: 0.0005 s/iter. Total: 0.1432 s/iter. ETA=0:00:05\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.evaluator]: \u001b[0mInference done 1199/1199. Dataloading: 0.0798 s/iter. Inference: 0.0625 s/iter. Eval: 0.0005 s/iter. Total: 0.1429 s/iter. ETA=0:00:00\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:50.647013 (0.142920 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:14 (0.062489 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.678\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 67.815 | 79.809 | 78.979 |  nan  |  nan  | 67.815 |\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/07 10:34:07 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 59.672 | sad        | 65.920 | surprised  | 63.154 |\n",
      "| happy      | 82.513 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 67.81473904192781, 'AP50': 79.80906209507968, 'AP75': 78.97863678133743, 'APs': nan, 'APm': nan, 'APl': 67.81473904192781, 'AP-angry': 59.671965348955865, 'AP-sad': 65.91976120528366, 'AP-surprised': 63.154363341332584, 'AP-happy': 82.51286627213912})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.config import CfgNode as CN\n",
    "import json\n",
    "import os\n",
    "\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_dir=None):\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_dir)\n",
    "    \n",
    "    def train(self):\n",
    "        self.results = {\"train\": [], \"test\": []}\n",
    "        super().train()\n",
    "        \n",
    "        # Add evaluation after each epoch\n",
    "        for epoch in range(0, self.cfg.SOLVER.MAX_ITER + 1, self.cfg.TEST.EVAL_PERIOD):\n",
    "            self.do_test(epoch)\n",
    "            if epoch % self.cfg.SOLVER.CHECKPOINT_PERIOD == 0:\n",
    "                self.checkpointer.save(f\"model_{epoch}\")\n",
    "\n",
    "    def do_test(self, epoch):\n",
    "        try:\n",
    "            # Perform evaluation\n",
    "            evaluator = self.build_evaluator(self.cfg, self.cfg.DATASETS.TEST[0], output_dir=self.cfg.OUTPUT_DIR)\n",
    "            val_loader = build_detection_test_loader(self.cfg, self.cfg.DATASETS.TEST[0])\n",
    "            results = inference_on_dataset(self.model, val_loader, evaluator)\n",
    "            self.results[\"test\"].append({\"epoch\": epoch, \"results\": results})\n",
    "            print(\"Evaluation results:\", results)\n",
    "    \n",
    "            # Save the results as JSON\n",
    "            self.save_results_as_json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation at epoch {epoch}: {e}\")\n",
    "    \n",
    "    def save_results_as_json(self):\n",
    "        try:\n",
    "            output_file_path = os.path.join(self.cfg.OUTPUT_DIR, \"train_test_results.json\")\n",
    "            print(f\"Saving results to: {output_file_path}\")\n",
    "            with open(output_file_path, 'w') as f:\n",
    "                json.dump(self.results, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results as JSON: {e}\")\n",
    "\n",
    "\n",
    "# Set up configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"face_data_set\",)\n",
    "cfg.DATASETS.TEST = (\"face_data_set_valid\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.MAX_ITER = 24020\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "cfg.INPUT.MAX_SIZE_TEST = 512\n",
    "cfg.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "cfg.INPUT.RANDOM_ROTATION = 30\n",
    "cfg.INPUT.CROP = CN({\"ENABLED\": True, \"TYPE\": \"relative_range\", \"SIZE\": [0.8, 0.8]})\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.OUTPUT_DIR = \"./models/faster_rcnn_R_50_FPN_3x\"\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "cfg.SOLVER.WARMUP_ITERS = 500\n",
    "\n",
    "setup_logger()\n",
    "trainer = CustomTrainer(cfg)\n",
    "trainer.resume_or_load(resume=True)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/p3/lib/python3.11/site-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': Instances(num_instances=1, image_height=700, image_width=1400, fields=[pred_boxes: Boxes(tensor([[ 755.2027,  120.8748, 1001.8848,  434.5255]], device='cuda:0')), scores: tensor([0.9788], device='cuda:0'), pred_classes: tensor([2], device='cuda:0')])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.structures import Boxes, Instances\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "# 설정을 로드\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.WEIGHTS = (\"models/faster_rcnn_R_50_FPN_3x//faster_rcnn_r_50_final_model.pth\")  # 훈련된 모델의 가중치 파일 경로\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # 예측에 사용할 임계값\n",
    "cfg.DATASETS.TEST = (\"face_data_set_valid\", )  # 테스트 데이터셋 이름\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "\n",
    "# 예측기를 설정\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# 예측할 이미지를 로드\n",
    "image_path = \"test.png\"  # 예측에 사용할 이미지 경로\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# 예측을 수행\n",
    "outputs = predictor(image)\n",
    "\n",
    "# 예측 결과를 확인\n",
    "print(outputs)\n",
    "\n",
    "# 결과 시각화\n",
    "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TEST[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "result_image = v.get_image()[:, :, ::-1]\n",
    "\n",
    "# 결과 이미지를 보기\n",
    "cv2.imshow(\"Predictions\", result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 결과 이미지를 저장\n",
    "cv2.imwrite(\"predicted_image.jpg\", result_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
