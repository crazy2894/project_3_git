{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:  /mnt/e/py_data/project_3_git\n"
     ]
    }
   ],
   "source": [
    "# 기본 작업 경로 설정\n",
    "import os\n",
    "notebook_path = os.path.abspath(\"project_3_git/readme.md\")\n",
    "notebook_dir = os.path.dirname(notebook_path)\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# 현재 작업 디렉토리 출력\n",
    "print(\"Current working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['공포, 헤드폰', '아찔한 분위기네요! 🎧 어떤 음악 듣고 계신가요? 궁금해요! 😊']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 데이터 가져오기\n",
    "import json\n",
    "\n",
    "# JSON 파일에서 딕셔너리 읽기\n",
    "with open('data/text_data/output_text.json', 'r') as file:\n",
    "    data_loaded = json.load(file)\n",
    "\n",
    "data_loaded['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# JSON 파일에서 딕셔너리 읽기\n",
    "with open('data/text_data/output_text.json', 'r') as file:\n",
    "    data_loaded = json.load(file)\n",
    "\n",
    "# 데이터 섞기\n",
    "items = list(data_loaded.items())\n",
    "random.shuffle(items)\n",
    "\n",
    "# 데이터를 섞은 후 딕셔너리로 변환\n",
    "data_loaded_shuffled = dict(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 테스트 데이터셋 나누기\n",
    "train_x = [data_loaded_shuffled[i][0] for i in data_loaded_shuffled][:5000]\n",
    "train_y = [data_loaded_shuffled[i][1] for i in data_loaded_shuffled][:5000]\n",
    "test_x = [data_loaded_shuffled[i][0] for i in data_loaded_shuffled][5000:]\n",
    "test_y = [data_loaded_shuffled[i][1] for i in data_loaded_shuffled][5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "\n",
    "name_folder = 'transfer_0'\n",
    "batch_size = 16\n",
    "train_epochs = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### early stopped 10\n",
    "### 모델 훈련시 적정 손실값 : 0.0001 ~ 0.0003\n",
    "https://huggingface.co/docs/transformers/model_doc/t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/p3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.17.1 at http://localhost:1238/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 서버가 http://127.0.0.1:1238 에서 실행중.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/p3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 9741.53 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████| 994/994 [00:00<00:00, 11094.57 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5321' max='31300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5321/31300 1:21:14 < 6:36:50, 1.09 it/s, Epoch 17/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.228090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.199032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.205700</td>\n",
       "      <td>0.189071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.185347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.181025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.179272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.178308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.178412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.179132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.179456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.180318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.180967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.182737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.184969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.187716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.189701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.193399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 't5/transfer_0/model'에 저장되었습니다.\n",
      "토크나이저가 't5/transfer_0/model/tokenizer'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('paust/pko-t5-base')\n",
    "\n",
    "# 데이터 전처리 함수 정의\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples['input_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(examples['target_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# 데이터셋 생성 및 토큰화\n",
    "dataset_train = Dataset.from_dict({'input_text': train_x,'target_text': train_y})\n",
    "dataset_test = Dataset.from_dict({'input_text': test_x,'target_text': test_y})\n",
    "\n",
    "tokenized_train_datasets = dataset_train.map(preprocess_function, batched=True)\n",
    "tokenized_test_datasets = dataset_test.map(preprocess_function, batched=True)\n",
    "\n",
    "# 학습인자\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size, # 학습 배치 사이즈\n",
    "    per_device_eval_batch_size=batch_size,  # 평가 배치 사이즈\n",
    "    output_dir=f't5/{name_folder}',         # 모델 및 체크포인트 저장 디렉토리\n",
    "    num_train_epochs=train_epochs,          # 학습 에폭 수\n",
    "    logging_dir=f't5/{name_folder}/logs',   # TensorBoard 로그가 저장될 디렉토리\n",
    "    logging_steps=100,                      # TensorBoard 로그를 기록할 간격 \n",
    "    report_to='tensorboard',                # TensorBoard로 로깅\n",
    "    load_best_model_at_end = True,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',                  # 에포크 마다 모델 저장\n",
    "    # resume_from_checkpoint=True           # 이어 학습\n",
    ")\n",
    "\n",
    "# Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset=tokenized_test_datasets,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train() # 이어 학습시 (resume_from_checkpoint=checkpoint_dir)\n",
    "\n",
    "# 모델과 토크나이저 저장\n",
    "model_save_path = f't5/{name_folder}/model'\n",
    "tokenizer_save_path = f't5/{name_folder}/model/tokenizer'\n",
    "\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "\n",
    "print(f\"모델이 '{model_save_path}'에 저장되었습니다.\")\n",
    "print(f\"토크나이저가 '{tokenizer_save_path}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위의 모델 이서서 계속 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 얼리 스탑 5321 step\n",
    "- 이후 이어서 진행 10643 step\n",
    "- 이후 이어서 진행 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후 진행 계획\n",
    "\n",
    "```py\n",
    "# 모델 구성 객체 로드 및 드롭아웃 비율 설정\n",
    "config = T5Config.from_pretrained('t5/transfer_1_continued/checkpoint-10642/')\n",
    "config.dropout_rate = 0.2  # 드롭아웃 비율을 20%로 설정 (기본값은 0.1)\n",
    "\n",
    "# 수정된 구성으로 모델 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5/transfer_1_continued/checkpoint-10642/', config=config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5/transfer_0/model/')\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5/transfer_0/model/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 입력: 야!!!!\n",
      "모델의 예측: 정말 화가 나신 것 같아요! 😠 어떤 일이 있으셨나요?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 테스트 입력\n",
    "test_input = \"야!!!!\"\n",
    "\n",
    "# 입력 토큰화\n",
    "input_ids = tokenizer.encode(test_input, return_tensors='pt')\n",
    "\n",
    "# GPU가 사용 가능한지 확인하고, 사용 가능한 경우 GPU로 이동\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 모델과 토크나이저를 GPU로 이동\n",
    "model.to(device)\n",
    "\n",
    "# 입력 데이터도 GPU로 이동\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# 예측을 GPU에서 수행\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# 예측 결과 디코딩\n",
    "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"테스트 입력: {test_input}\")\n",
    "print(f\"모델의 예측: {predicted_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
