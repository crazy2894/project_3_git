{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:  /mnt/e/py_data/project_3_git\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ ì‘ì—… ê²½ë¡œ ì„¤ì •\n",
    "import os\n",
    "notebook_path = os.path.abspath(\"project_3_git/readme.md\")\n",
    "notebook_dir = os.path.dirname(notebook_path)\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì¶œë ¥\n",
    "print(\"Current working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ê³µí¬, í—¤ë“œí°', 'ì•„ì°”í•œ ë¶„ìœ„ê¸°ë„¤ìš”! ğŸ§ ì–´ë–¤ ìŒì•… ë“£ê³  ê³„ì‹ ê°€ìš”? ê¶ê¸ˆí•´ìš”! ğŸ˜Š']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "import json\n",
    "\n",
    "# JSON íŒŒì¼ì—ì„œ ë”•ì…”ë„ˆë¦¬ ì½ê¸°\n",
    "with open('data/text_data/output_text.json', 'r') as file:\n",
    "    data_loaded = json.load(file)\n",
    "\n",
    "data_loaded['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# JSON íŒŒì¼ì—ì„œ ë”•ì…”ë„ˆë¦¬ ì½ê¸°\n",
    "with open('data/text_data/output_text.json', 'r') as file:\n",
    "    data_loaded = json.load(file)\n",
    "\n",
    "# ë°ì´í„° ì„ê¸°\n",
    "items = list(data_loaded.items())\n",
    "random.shuffle(items)\n",
    "\n",
    "# ë°ì´í„°ë¥¼ ì„ì€ í›„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "data_loaded_shuffled = dict(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°\n",
    "train_x = [data_loaded_shuffled[i][0] for i in data_loaded_shuffled][:5000]\n",
    "train_y = [data_loaded_shuffled[i][1] for i in data_loaded_shuffled][:5000]\n",
    "test_x = [data_loaded_shuffled[i][0] for i in data_loaded_shuffled][5000:]\n",
    "test_y = [data_loaded_shuffled[i][1] for i in data_loaded_shuffled][5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "name_folder = 'transfer_0'\n",
    "server_port = '4561'\n",
    "batch_size = 16\n",
    "train_epochs = 60\n",
    "\n",
    "\n",
    "\n",
    "# TensorBoard ì„œë²„ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)\n",
    "subprocess.Popen(['tensorboard', f'--logdir=t5/{name_folder}', f'--port={server_port}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### early stopped 10\n",
    "### ëª¨ë¸ í›ˆë ¨ì‹œ ì ì • ì†ì‹¤ê°’ : 0.0001 ~ 0.0003\n",
    "https://huggingface.co/docs/transformers/model_doc/t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/p3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.17.1 at http://localhost:1238/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard ì„œë²„ê°€ http://127.0.0.1:1238 ì—ì„œ ì‹¤í–‰ì¤‘.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/p3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 9741.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 994/994 [00:00<00:00, 11094.57 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5321' max='31300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5321/31300 1:21:14 < 6:36:50, 1.09 it/s, Epoch 17/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.228090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.199032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.205700</td>\n",
       "      <td>0.189071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.185347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.181025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.179272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.178308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.178412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.179132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.179456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.180318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.180967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.182737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.184969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.187716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.189701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.193399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ì´ 't5/transfer_0/model'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "í† í¬ë‚˜ì´ì €ê°€ 't5/transfer_0/model/tokenizer'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('paust/pko-t5-base')\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples['input_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(examples['target_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„± ë° í† í°í™”\n",
    "dataset_train = Dataset.from_dict({'input_text': train_x,'target_text': train_y})\n",
    "dataset_test = Dataset.from_dict({'input_text': test_x,'target_text': test_y})\n",
    "\n",
    "tokenized_train_datasets = dataset_train.map(preprocess_function, batched=True)\n",
    "tokenized_test_datasets = dataset_test.map(preprocess_function, batched=True)\n",
    "\n",
    "# í•™ìŠµì¸ì\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size, # í•™ìŠµ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    per_device_eval_batch_size=batch_size,  # í‰ê°€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    output_dir=f't5/{name_folder}',         # ëª¨ë¸ ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    num_train_epochs=train_epochs,          # í•™ìŠµ ì—í­ ìˆ˜\n",
    "    logging_dir=f't5/{name_folder}/logs',   # TensorBoard ë¡œê·¸ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬\n",
    "    logging_steps=100,                      # TensorBoard ë¡œê·¸ë¥¼ ê¸°ë¡í•  ê°„ê²© \n",
    "    report_to='tensorboard',                # TensorBoardë¡œ ë¡œê¹…\n",
    "    load_best_model_at_end = True,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',                  # ì—í¬í¬ ë§ˆë‹¤ ëª¨ë¸ ì €ì¥\n",
    "    # resume_from_checkpoint=True           # ì´ì–´ í•™ìŠµ\n",
    ")\n",
    "\n",
    "# Trainer ê°ì²´ ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset=tokenized_test_datasets,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train() # ì´ì–´ í•™ìŠµì‹œ (resume_from_checkpoint=checkpoint_dir)\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥\n",
    "model_save_path = f't5/{name_folder}/model'\n",
    "tokenizer_save_path = f't5/{name_folder}/model/tokenizer'\n",
    "\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "\n",
    "print(f\"ëª¨ë¸ì´ '{model_save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"í† í¬ë‚˜ì´ì €ê°€ '{tokenizer_save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìœ„ì˜ ëª¨ë¸ ì´ì„œì„œ ê³„ì† í›ˆë ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì–¼ë¦¬ ìŠ¤íƒ‘ 5321 step\n",
    "- ì´í›„ ì´ì–´ì„œ ì§„í–‰ 10643 step\n",
    "- ì´í›„ ì´ì–´ì„œ ì§„í–‰ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´í›„ ì§„í–‰ ê³„íš\n",
    "\n",
    "```py\n",
    "# ëª¨ë¸ êµ¬ì„± ê°ì²´ ë¡œë“œ ë° ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ì„¤ì •\n",
    "config = T5Config.from_pretrained('t5/transfer_1_continued/checkpoint-10642/')\n",
    "config.dropout_rate = 0.2  # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ì„ 20%ë¡œ ì„¤ì • (ê¸°ë³¸ê°’ì€ 0.1)\n",
    "\n",
    "# ìˆ˜ì •ëœ êµ¬ì„±ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5/transfer_1_continued/checkpoint-10642/', config=config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5/transfer_0/model/')\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5/transfer_0/model/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ì…ë ¥: ì•¼!!!!\n",
      "ëª¨ë¸ì˜ ì˜ˆì¸¡: ì •ë§ í™”ê°€ ë‚˜ì‹  ê²ƒ ê°™ì•„ìš”! ğŸ˜  ì–´ë–¤ ì¼ì´ ìˆìœ¼ì…¨ë‚˜ìš”?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì…ë ¥\n",
    "test_input = \"ì•¼!!!!\"\n",
    "\n",
    "# ì…ë ¥ í† í°í™”\n",
    "input_ids = tokenizer.encode(test_input, return_tensors='pt')\n",
    "\n",
    "# GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ê³ , ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° GPUë¡œ ì´ë™\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ GPUë¡œ ì´ë™\n",
    "model.to(device)\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„°ë„ GPUë¡œ ì´ë™\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# ì˜ˆì¸¡ì„ GPUì—ì„œ ìˆ˜í–‰\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ë””ì½”ë”©\n",
    "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì…ë ¥: {test_input}\")\n",
    "print(f\"ëª¨ë¸ì˜ ì˜ˆì¸¡: {predicted_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
